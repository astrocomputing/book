**Chapter 33: N-Body Simulations**

Having introduced the fundamental numerical methods used in astrophysical simulations, this chapter focuses specifically on one of the most widely used and foundational simulation techniques: **N-body simulations**. These simulations track the motion of a large number (`N`) of discrete particles interacting primarily through the force of gravity. N-body methods are the cornerstone for modeling systems where gravity is the dominant long-range force and where the detailed physics of gas dynamics or radiation are either negligible or can be treated separately, such as the evolution of dark matter structures in cosmology, the dynamics of stars within galaxies and star clusters, and the evolution of planetary systems. We will begin by distinguishing between collisionless and collisional N-body systems, defining their respective domains of applicability. We will then explore methods for setting up appropriate **initial conditions**, both for cosmological simulations based on primordial fluctuations and for idealized models of galaxies or star clusters. An overview of commonly used **N-body simulation codes** will be provided. We will illustrate how to perform simple N-body calculations or orbit integrations using Python tools like `galpy` or basic scripts. Methods for **analyzing the output** of N-body simulations, including reading snapshot data and calculating key properties like density profiles and velocity dispersions, will be discussed, along with the concept of halo finding. Finally, we will delve into the specific physical effects, such as two-body relaxation and mass segregation, that become important in **collisional** N-body systems like star clusters.

**33.1 Simulating Gravitational Dynamics: Collisionless vs. Collisional**

N-body simulations provide a powerful computational laboratory for studying the evolution of systems governed primarily by gravity. The core idea is to represent the mass distribution of the system using a finite number `N` of discrete particles and then track the trajectories of these particles over time by calculating the gravitational forces they exert on each other and integrating their equations of motion (Sec 31.3, 32.5). A crucial distinction arises based on the importance of close gravitational encounters between individual particles: **collisionless** versus **collisional** systems.

In **collisionless systems**, the gravitational potential is dominated by the smooth, large-scale mass distribution, and the effect of close, two-body encounters between individual particles on their orbits is negligible over the timescales of interest. The particles essentially move under the influence of the mean gravitational field generated by all other particles collectively. This regime applies well to systems with a very large number of particles spread over large volumes, such as dark matter halos (where dark matter particles are assumed to interact only gravitationally and are numerous), entire galaxies (where the billions of stars are typically far apart), and large galaxy clusters. N-body simulations of collisionless systems focus on modeling collective phenomena like relaxation (violent relaxation), phase mixing, the formation of bars and spiral arms in galactic disks, tidal disruption, and the hierarchical merging of dark matter halos.

The **collisional N-body** regime, in contrast, applies to systems where close gravitational encounters between individual particles *are* significant and cumulatively alter particle orbits over the system's lifetime. This occurs in systems that are relatively dense and have fewer particles compared to galaxies, most notably **globular star clusters**, **open star clusters**, and **planetary systems**. In these systems, two-body relaxation (the gradual change in particle velocities due to repeated weak encounters) becomes important, leading to phenomena like energy equipartition (tendency for massive stars to slow down and sink towards the center, while low-mass stars speed up and move outwards – **mass segregation**), the formation of dense cores (potentially leading to **core collapse**), and the gradual evaporation of stars from the cluster's tidal boundary. Simulating these collisional effects requires accurately calculating the forces between nearby particles, often necessitating direct summation methods (Sec 32.5).

The key factor distinguishing these regimes is the **relaxation timescale (t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99>)**, which estimates the time it takes for two-body encounters to significantly alter the velocities of stars. It typically scales as `t_relax ~ (N / ln N) * t_cross`, where `N` is the number of particles and `t_cross` is the crossing time (the typical time for a particle to cross the system). For galaxies (N ~ 10¹¹ stars), `t_relax` is much longer than the age of the Universe, justifying the collisionless approximation. For globular clusters (N ~ 10⁵ - 10⁶ stars), `t_relax` can be shorter than the age of the Universe, making collisional effects crucial. For open clusters (N ~ 10³ stars), `t_relax` is even shorter.

This distinction dictates the choice of simulation algorithm and code. Collisionless simulations can employ faster approximate gravity solvers like Tree codes or TreePM methods (Sec 32.5) that efficiently calculate long-range forces while softening short-range interactions (as close encounters are irrelevant). Codes like GADGET, AREPO, Enzo, and RAMSES are primarily designed for collisionless or hydrodynamical simulations incorporating collisionless components.

Collisional simulations, however, demand high accuracy in calculating forces between close pairs of particles. They typically use **direct summation** methods, often accelerated by specialized hardware (GPUs, GRAPE) or employing high-order integration schemes and sophisticated techniques (like regularization for close encounters or hierarchical approaches) to handle binary stars and multiple systems accurately. Codes like `NBODY6/7`, `PeTar`, or specialized planetary dynamics integrators fall into this category. These codes are computationally much more expensive per particle than typical collisionless codes.

It's important to choose the appropriate type of N-body simulation based on the system being studied. Applying a collisionless code (with force softening) to simulate a dense star cluster would artificially suppress crucial collisional effects like mass segregation and core collapse. Conversely, using a direct summation collisional code for a galaxy simulation would be computationally impossible due to the O(N²) scaling.

Furthermore, many astrophysical systems involve multiple components treated differently. Galaxy formation simulations often use collisionless N-body particles for dark matter and stars, coupled with hydrodynamics for the collisional gas component. Planetary system simulations might treat planets as N-body particles but model the gas disk using hydrodynamics. Understanding the collisionless vs. collisional nature of each component guides the choice of appropriate numerical methods.

This chapter primarily focuses on the methods and analysis relevant to **collisionless N-body simulations** as they are widely used in cosmology and galaxy dynamics, while also touching upon the specific physics relevant to the **collisional** regime in Section 33.6 and Application 33.B. Python tools are more commonly used for setup and analysis of collisionless simulations or for simpler orbit integrations within potentials, rather than for implementing the core collisional N-body algorithms themselves.

**33.2 Setting up Initial Conditions (ICs)**

The results of any N-body simulation are critically dependent on the **initial conditions (ICs)** provided at the starting time (t=0 or initial redshift z<0xE1><0xB5><0xA2><0xE1><0xB5><0x8A><0xE1><0xB5><0x97>). These ICs must accurately represent the physical state (positions, velocities, masses) of the system being modeled to ensure the subsequent simulated evolution is meaningful. Generating appropriate ICs is a non-trivial task, often requiring specialized tools or techniques depending on the type of simulation being performed (cosmological vs. isolated system).

For **cosmological N-body simulations**, the goal is to generate ICs that statistically reproduce the properties of the primordial density fluctuations observed in the Cosmic Microwave Background (CMB) at a specific early redshift (typically z=50-100, when fluctuations were still largely linear). This involves several steps:
1.  **Choose Cosmology:** Define the cosmological parameters (H₀, Ω<0xE1><0xB5><0x89>, Ω<0xE2><0x82><0x8B>, σ₈, n<0xE2><0x82><0x9B>, etc.) for the desired model (e.g., Planck 2018 ΛCDM).
2.  **Calculate Linear Power Spectrum:** Use a Boltzmann code like **CAMB** (Code for Anisotropies in the Microwave Background) or **CLASS** (Cosmic Linear Anisotropy Solving System) to compute the theoretical linear matter power spectrum P(k) at the initial redshift z<0xE1><0xB5><0xA2><0xE2><0x82><0x99>ᵢ<0xE1><0xB5><0x97>, corresponding to the chosen cosmology.
3.  **Generate Density Field:** Create a realization of a Gaussian random field in a cubic box (matching the intended simulation volume) whose power spectrum matches the theoretical P(k). This defines the initial density fluctuations δ(x).
4.  **Calculate Displacements and Velocities:** Using linear perturbation theory (e.g., the Zel'dovich approximation or second-order Lagrangian Perturbation Theory, 2LPT), calculate the initial displacement field and peculiar velocity field corresponding to the generated density field δ(x).
5.  **Place Particles:** Start with an initial particle distribution (often a regular grid or a low-discrepancy "glass" configuration) and apply the calculated displacements to move particles to their initial positions at z<0xE1><0xB5><0xA2><0xE2><0x82><0x99>ᵢ<0xE1><0xB5><0x97>. Assign the calculated peculiar velocities (plus the Hubble flow velocity) to the particles.
Specialized codes like **MUSIC** (MUlti-Scale Initial Conditions) or **MonofonIC** are commonly used to perform steps 3-5, taking the P(k) from CAMB/CLASS as input and generating particle initial condition files in formats compatible with N-body codes like GADGET or AREPO. Generating high-quality cosmological ICs is crucial for obtaining accurate simulation results for large-scale structure statistics.

For simulations of **isolated systems** like individual galaxies or star clusters, different techniques are used to generate ICs representing equilibrium or near-equilibrium configurations.
*   **Analytical Models:** For simple spherical systems like globular clusters or dark matter halos, analytical density profiles like the **Plummer sphere**, **Hernquist profile**, or **Navarro-Frenk-White (NFW) profile** are often used. Given the density profile ρ(r), the gravitational potential Φ(r) and the velocity distribution function f(E, L) (where E is energy, L is angular momentum) needed for an equilibrium system can often be derived (e.g., using Eddington's formula or solving the Jeans equations). Particle positions and velocities can then be sampled from these distribution functions to create an N-body realization that statistically matches the analytical model.
*   **Disk Galaxy Models:** Creating realistic equilibrium models for disk galaxies (containing bulge, stellar disk, gas disk, dark matter halo) is more complex. It often involves specifying density profiles for each component and numerically solving the combined Poisson equation and Jeans equations (or using Schwarzschild's orbit superposition method) to find a self-consistent velocity distribution that supports the structure. Particle positions and velocities are then sampled from the resulting distribution function. Libraries like **`galpy`** (introduced later) provide tools for setting up such multi-component potentials and generating approximate distribution functions, while specialized codes like **GALIC** or **MakeDisk** are dedicated to generating N-body ICs for disk galaxies.
*   **Merger Initial Conditions:** To simulate galaxy mergers, ICs typically consist of two separate equilibrium galaxy models (generated as above) placed at an initial separation with specified relative velocities defining their incoming orbit (often parabolic or hyperbolic).

Generating stable, equilibrium initial conditions for isolated systems is crucial to avoid artificial transients or rapid evolution at the start of the simulation due solely to poorly constructed ICs. The process often involves careful sampling from distribution functions to ensure the particle realization accurately represents the desired density and kinematic structure.

Python libraries can assist in generating ICs for idealized systems. For instance, `galpy` allows defining complex gravitational potentials for galaxy components and can sample positions/velocities from approximate distribution functions associated with these potentials. For simpler analytical profiles like Plummer or NFW, custom Python scripts using NumPy for random sampling based on the profile's density and velocity dispersion can be written.

```python
# --- Code Example 1: Sampling from a Plummer Sphere (Conceptual) ---
import numpy as np
import matplotlib.pyplot as plt
# Note: Real sampling requires transformation methods (e.g., inverse CDF)

print("Conceptual sampling from a Plummer sphere density profile:")

# Plummer profile: rho(r) = (3*M / (4*pi*a^3)) * (1 + r^2/a^2)^(-5/2)
# Cumulative Mass: M(<r) = M * (r^3) / (a^2 + r^2)^(3/2)

# Parameters
total_mass_M = 1.0e6 # Arbitrary units (e.g., Msun)
scale_radius_a = 1.0 # Arbitrary units (e.g., pc)
n_particles = 10000

print(f"\nPlummer Parameters: M={total_mass_M}, a={scale_radius_a}")
print(f"Generating {n_particles} particle positions...")

# --- Generate Radii according to Plummer profile ---
# This requires sampling from the mass distribution M(<r).
# The inverse transform sampling method can be used.
# Let X = M(<r) / M. We need r(X). X = q^3 / (1 + q^2)^(3/2) where q=r/a.
# Solving for q = r/a gives q = sqrt(X^(2/3) / (1 - X^(2/3)))
# We sample X uniformly from [0, 1] and calculate corresponding r.
np.random.seed(42)
X_uniform = np.random.uniform(0, 1, n_particles)
# Avoid X=1 where denominator is zero
X_uniform = np.clip(X_uniform, 0, 1 - 1e-9) 
q_sampled = np.sqrt(X_uniform**(2./3.) / (1. - X_uniform**(2./3.)))
r_sampled = q_sampled * scale_radius_a
print("Generated radii based on cumulative mass profile.")

# --- Generate Isotropic Positions ---
# Generate random points uniformly on a sphere (phi, cos(theta))
phi = np.random.uniform(0, 2 * np.pi, n_particles)
cos_theta = np.random.uniform(-1, 1, n_particles)
sin_theta = np.sqrt(1 - cos_theta**2)
# Convert to Cartesian coordinates (x, y, z)
x = r_sampled * sin_theta * np.cos(phi)
y = r_sampled * sin_theta * np.sin(phi)
z = r_sampled * cos_theta
positions = np.vstack((x, y, z)).T
print("Generated isotropic positions.")

# --- Visualize Projected Density ---
print("Generating projected density plot...")
fig, ax = plt.subplots(figsize=(6, 6))
# Plot projected positions (X vs Y)
ax.plot(positions[:, 0], positions[:, 1], '.', markersize=1, alpha=0.3)
# Create a 2D histogram to visualize density
bins = 50
hist, xedges, yedges = np.histogram2d(positions[:, 0], positions[:, 1], bins=bins, 
                                      range=[[-5*scale_radius_a, 5*scale_radius_a], 
                                             [-5*scale_radius_a, 5*scale_radius_a]])
ax.imshow(np.log10(hist.T + 1e-1), origin='lower', 
          extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], 
          cmap='viridis', alpha=0.7, aspect='auto') # Log scale density map overlay

ax.set_xlabel(f"X ({scale_radius_a} units)")
ax.set_ylabel(f"Y ({scale_radius_a} units)")
ax.set_title(f"Projected Positions Sampled from Plummer Profile")
ax.set_xlim(-5*scale_radius_a, 5*scale_radius_a)
ax.set_ylim(-5*scale_radius_a, 5*scale_radius_a)
ax.set_aspect('equal')
# plt.show()
print("Plot generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This code demonstrates how to generate particle positions following 
# a Plummer density profile using the inverse transform sampling method based on 
# the cumulative mass profile M(<r).
# 1. It defines the total mass `M` and scale radius `a` for the Plummer model.
# 2. It derives the formula `r(X)` where `X = M(<r)/M` is the cumulative mass fraction.
# 3. It generates `n_particles` random numbers `X_uniform` uniformly between 0 and 1.
# 4. It applies the inverse transform formula to get the sampled radii `r_sampled`.
# 5. It generates random isotropic directions (phi, theta) on a sphere.
# 6. It combines the sampled radii and directions to get 3D Cartesian positions.
# 7. It visualizes the result by plotting the projected X-Y positions and overlaying 
#    a 2D histogram (density map), which should show the characteristic centrally 
#    concentrated profile of the Plummer sphere. 
# Note: Generating corresponding equilibrium velocities requires sampling from the 
# distribution function f(E, L), which is more complex.
```

Whether generating cosmological ICs with dedicated packages or creating idealized systems using analytical profiles and sampling techniques (potentially aided by Python libraries like NumPy or `galpy`), the generation of accurate and appropriate initial conditions is a foundational step in the simulation lifecycle, determining the starting point from which the system's gravitational evolution will be numerically modeled. Errors or inconsistencies in the ICs can significantly impact the validity and interpretation of the simulation results.

**33.3 Introduction to N-body Codes**

Once initial conditions representing the system of interest have been generated (Sec 33.2), the next step is to evolve this system forward in time using an **N-body simulation code**. These codes are sophisticated software packages specifically designed to solve the gravitational N-body problem efficiently and accurately, typically leveraging advanced gravity solvers (Sec 32.5) and time integration schemes (Sec 32.3) and designed for execution on parallel computing architectures (Sec 32.6, Part VII). While writing a basic N-body solver is conceptually straightforward (as illustrated in Sec 31.1), achieving the performance, accuracy, and scalability required for state-of-the-art astrophysical simulations necessitates highly optimized, often publicly available community codes developed over many years.

A wide variety of N-body codes exist, differing in their primary application domain (collisionless vs. collisional), the specific numerical algorithms employed (e.g., TreePM vs. PM vs. AMR gravity solvers; Leapfrog vs. higher-order integrators), the physics included (gravity-only vs. coupled hydro/MHD), and their parallelization strategy (MPI, OpenMP, GPU). Some prominent examples include:

*   **GADGET (e.g., GADGET-2, GADGET-4):** Widely used for cosmological simulations (collisionless dark matter) and hydrodynamical simulations (using SPH). Employs a TreePM gravity solver and typically a Leapfrog integrator. Highly parallel using MPI. Outputs data in a specific binary or HDF5 format. (Springel 2005).
*   **AREPO:** Another popular code for cosmological hydrodynamics (used for Illustris/IllustrisTNG simulations). Uses a moving unstructured mesh for hydrodynamics (combining advantages of grid and particle methods) coupled with a TreePM gravity solver. Parallelized with MPI. Outputs typically in HDF5. (Springel 2010).
*   **ENZO:** A versatile, publicly available grid-based (AMR) code primarily for cosmology and galaxy formation. Includes modules for N-body gravity (using PM on coarse grids, direct summation or Tree on fine grids), hydrodynamics, MHD, radiative transfer, and chemistry. Parallelized with MPI. Outputs in HDF5 (often compatible with `yt`).
*   **RAMSES:** Another widely used public AMR code for cosmology and galaxy formation. Features N-body gravity (PM + multi-grid Poisson solver on refined levels) and hydrodynamics/MHD solvers. Parallelized with MPI. Outputs in a specific binary format or optionally Fortran unformatted.
*   **GIZMO:** A flexible code that includes various hydrodynamics methods (modern SPH variants, meshless finite mass/volume methods) coupled with gravity solvers similar to GADGET. Designed for galaxy formation, star formation, and other hydro problems. Parallelized with MPI. Outputs often in HDF5.
*   **CHANGA:** A scalable N-body+SPH code using a charm++ parallel framework, designed for very large cosmological simulations.
*   **NBODY6 / NBODY7 / PeTar:** Specialized codes designed for **collisional** N-body dynamics, primarily used for simulating star clusters (open and globular). They employ high-accuracy direct summation methods (often GPU-accelerated in modern versions) combined with sophisticated techniques for handling close encounters, binary star evolution, and stellar evolution integration. Parallelization strategies vary. Outputs are often specific ASCII or binary formats.
*   **REBOUND / MERCURIUS:** Codes specifically designed for planetary system dynamics (gravitational N-body focused on long-term integrations), often employing symplectic integrators for better energy/angular momentum conservation over many orbits.

These codes typically share common features despite their algorithmic differences. They read initial conditions from specific file formats (e.g., GADGET IC format, generic ASCII/binary lists, HDF5). They are controlled via text-based **parameter files** (Sec 31.5) that specify simulation options, physical parameters, output times/formats, etc. They evolve the system over discrete time steps, often using adaptive or shared time steps for different particles based on their local dynamical conditions. They produce output **snapshots** at specified intervals, saving the state (positions, velocities, IDs, masses, internal energy, etc.) of all particles/cells, usually in binary FITS or HDF5 formats. They also typically generate **log files** recording runtime information, performance statistics, and potential error messages.

Most modern astrophysical N-body codes are designed for **parallel execution** on HPC clusters using the **Message Passing Interface (MPI)** (Chapter 39) for communication between processes running on different nodes (distributed memory parallelism). Some might also incorporate **OpenMP** (Sec 38.6) for shared memory parallelism within a single multi-core node, or leverage **GPU acceleration** (Chapter 41) for computationally intensive parts like force calculation (especially direct summation or Tree codes). Understanding the parallel capabilities and scaling behavior of a code is crucial when planning large simulations.

Choosing the right code depends on the science problem (collisionless vs. collisional, need for hydro/MHD?), available computational resources, familiarity with the code's setup and analysis tools, and community support. Publicly available codes like GADGET, AREPO, ENZO, RAMSES, GIZMO, NBODY6/7 have large user bases and extensive documentation, making them popular choices.

Working with these codes typically involves:
1.  Obtaining the source code (often from a public repository like GitHub or a project website).
2.  Compiling the code on the target HPC system, configuring necessary options (e.g., enabling/disabling physics modules, setting compiler flags, linking libraries like MPI, HDF5, GSL).
3.  Generating initial conditions in the specific format required by the code.
4.  Creating the main parameter file specifying all runtime options.
5.  Writing a batch job submission script (e.g., for SLURM) that requests resources and launches the compiled executable using MPI (e.g., `mpirun -np N_cores ./CodeName Paramfile.txt`).
6.  Monitoring the job's execution and managing the output data.

While this book focuses on analysis using Python, understanding the existence, capabilities, and typical workflow of these underlying compiled simulation codes provides essential context for interpreting the simulation data products that Python tools like `yt` or `h5py` are used to analyze (Sec 33.5, Chapter 35). Direct interaction with these codes from Python is often limited to generating input files or analyzing output files, although some codes might offer limited Python scripting interfaces for specific tasks.

**33.4 Running a Simple N-body Simulation**

While large-scale, high-performance N-body simulations rely on complex codes written in compiled languages (Sec 33.3), understanding the core computational loop and illustrating basic gravitational dynamics can be achieved through simpler examples, some accessible directly within Python. This section demonstrates two approaches: using a specialized Python library (`galpy`) for orbit integration within fixed potentials (effectively a restricted N-body problem), and implementing a very basic direct summation N-body loop in pure Python/NumPy (highlighting its limitations).

**1. Orbit Integration with `galpy`:** The `galpy` library (`pip install galpy`) is a powerful Python package primarily designed for galactic dynamics calculations. It provides tools for defining complex gravitational potentials (representing components like disks, bulges, halos), creating `Orbit` objects representing stars or test particles, and efficiently integrating these orbits within the combined potential using high-accuracy ODE solvers (often wrapping C or Fortran libraries for speed). While not a full self-gravitating N-body simulation (the potential is fixed, not evolving due to the particles' own gravity), orbit integration is extremely useful for studying stellar streams, satellite galaxy disruption, orbital resonances, and general particle motion in galaxies.

The workflow typically involves:
*   Importing `galpy.potential` and `galpy.orbit`.
*   Defining one or more potential objects (e.g., `lp = potential.LogarithmicHaloPotential(...)`, `mp = potential.MiyamotoNagaiPotential(...)`). Combine potentials by putting them in a list.
*   Defining the initial conditions of an orbit as a list `[R, vR, vT, z, vz, phi]` (cylindrical coordinates and velocities) or `[ra, dec, dist, pmra, pmdec, vr]` (sky coordinates), typically using `astropy.units`.
*   Creating an `Orbit` object: `o = Orbit(vxvv=initial_conditions, ro=..., vo=...)` (scaling parameters `ro`, `vo` set physical units).
*   Defining a list of times `ts` (with units) at which to compute the orbit.
*   Integrating the orbit: `o.integrate(ts, potential_list)`.
*   Accessing and plotting the resulting trajectory (e.g., `o.x(ts)`, `o.y(ts)`, `o.vx(ts)`).

```python
# --- Code Example 1: Orbit Integration using galpy ---
# Note: Requires galpy installation: pip install galpy
# Requires astropy for units.
try:
    from galpy.potential import LogarithmicHaloPotential, MiyamotoNagaiPotential
    from galpy.orbit import Orbit
    from astropy import units as u
    import numpy as np
    import matplotlib.pyplot as plt
    galpy_installed = True
except ImportError:
    galpy_installed = False
    print("NOTE: galpy or astropy not installed. Skipping galpy example.")

print("Integrating an orbit in a combined potential using galpy:")

if galpy_installed:
    # Define Potentials (Example: Log Halo + Miyamoto-Nagai Disk)
    # Use galpy's internal units or scale later
    lp = LogarithmicHaloPotential(amp=1.0, q=0.9, normalize=1.) 
    mp = MiyamotoNagaiPotential(amp=0.6, a=0.5, b=0.1, normalize=1.)
    combined_potential = [lp, mp]
    print("\nDefined combined gravitational potential (LogHalo + M-N Disk).")

    # Define Initial Conditions for an orbit (example: inclined disk orbit)
    # [R,vR,vT, z,vz,phi] in galpy natural units (can be scaled)
    # Units are typically distance=ro, velocity=vo (set later if needed)
    initial_vxvv = [1.0, 0.1, 1.1, 0.1, 0.2, 0.0] # R, vR, vT, z, vz, phi
    orbit = Orbit(vxvv=initial_vxvv) 
    print(f"Initial orbit state: {initial_vxvv}")

    # Define Integration Time (e.g., 50 time units, ~few Gyr if scaled)
    time_units = 50.
    n_steps = 500
    times = np.linspace(0., time_units, n_steps) * u.Gyr # Attach units for plotting label
    # galpy integration itself often uses dimensionless time
    
    # Integrate the orbit
    print(f"\nIntegrating orbit for {time_units} time units...")
    orbit.integrate(times / u.Gyr * orbit._ro / orbit._vo, combined_potential) # Integrate using natural units time
    print("Integration complete.")

    # Plot the orbit (X-Y projection and R-Z projection)
    print("Generating orbit plots...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    orbit.plot(d1='x', d2='y', ax=ax1, xlabel="X (kpc)", ylabel="Y (kpc)", label=None) 
    ax1.set_title("Orbit Projection (X vs Y)")
    ax1.axis('equal')

    orbit.plot(d1='R', d2='z', ax=ax2, xlabel="R (kpc)", ylabel="Z (kpc)")
    ax2.set_title("Orbit Projection (R vs z)")

    fig.suptitle("Orbit Integrated in Combined Potential using galpy")
    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
    # plt.show()
    print("Plots generated.")
    plt.close(fig)

else:
    print("Skipping galpy example execution.")

print("-" * 20)

# Explanation: This code uses the `galpy` library for orbit integration.
# 1. It defines two potential components (`LogarithmicHaloPotential`, `MiyamotoNagaiPotential`) 
#    and combines them into a list.
# 2. It defines the initial 6D phase-space coordinates (`vxvv`) for an `Orbit` object.
# 3. It defines the time array `times` over which to integrate (using astropy units 
#    for the label, but converting to galpy's natural time units for integration).
# 4. `orbit.integrate(t, pot)` performs the high-accuracy ODE integration using the 
#    specified potential.
# 5. It uses `orbit.plot()` methods to easily visualize the integrated trajectory in 
#    different projections (X-Y plane and R-z plane), showing the path of the particle 
#    within the fixed potential. This demonstrates simulating the motion of test 
#    particles in a realistic galactic potential.
```

**2. Basic Direct Summation N-body:** To illustrate the core calculation involving mutual gravity, we can write a simple Python script that performs direct summation N-body integration for a very small number of particles (`N`). This involves a main time loop. Inside the loop, another nested loop calculates the pairwise forces between all particles (O(N²)). Velocities and positions are then updated using a simple scheme like Euler or Leapfrog. **This approach is purely illustrative and computationally infeasible for realistic N due to the N² scaling.** It also typically uses simple integrators and lacks sophisticated handling of close encounters or parallelization found in production codes.

```python
# --- Code Example 2: Simple Direct N-body Loop (ILLUSTRATIVE ONLY) ---
# Computationally very slow (O(N^2) per step), inaccurate integrator.
# NOT suitable for real simulations beyond N~few hundred at most.
import numpy as np
import matplotlib.pyplot as plt
import time

print("Illustrative direct N-body simulation loop (VERY SLOW for large N):")

# Parameters
N_particles = 50 # Keep N very small!
n_steps = 100
dt = 0.01 # Time step
G = 1.0
softening = 0.05 # Softening to avoid divergences

# Initial conditions (random sphere)
np.random.seed(1)
positions = np.random.randn(N_particles, 3) * 2.0 # 3D positions
velocities = np.random.randn(N_particles, 3) * 0.1
masses = np.ones(N_particles) * (1.0 / N_particles) # Total mass = 1

print(f"\nSimulating {N_particles} particles for {n_steps} steps (direct summation)...")
start_time_nb = time.time()

# --- Time Evolution Loop ---
for step in range(n_steps):
    # Calculate accelerations (O(N^2) direct summation)
    accel = np.zeros_like(positions)
    for i in range(N_particles):
        for j in range(N_particles):
            if i == j: continue
            dr = positions[j] - positions[i]
            dist_sq = np.sum(dr**2)
            inv_r3 = (dist_sq + softening**2)**(-1.5) # Softened gravity
            accel[i] += G * masses[j] * dr * inv_r3
            
    # Update velocities and positions (Forward Euler - inaccurate!)
    # Using Leapfrog (Kick-Drift-Kick) would be better
    velocities += accel * dt
    positions += velocities * dt
    
    # Optional: Print progress
    # if (step + 1) % 10 == 0: print(f"  Step {step+1}/{n_steps}")

end_time_nb = time.time()
print(f"Simulation finished. Time taken: {end_time_nb - start_time_nb:.2f}s")

# --- Visualize Initial and Final State (X-Y Projection) ---
print("Generating plot...")
fig, ax = plt.subplots(figsize=(6, 6))
# Need initial positions again - store them before loop or replot first step if stored
# Assuming we just plot final state here
ax.plot(positions[:, 0], positions[:, 1], '.', markersize=3)
ax.set_xlabel("X position"); ax.set_ylabel("Y position")
ax.set_title(f"Direct N-body Final State (N={N_particles}, {n_steps} steps)")
ax.set_aspect('equal')
ax.grid(True, alpha=0.4)
# plt.show()
print("Plot generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This code implements a bare-bones direct summation N-body simulation in Python.
# 1. It initializes positions, velocities, and masses for a small number of particles `N`.
# 2. It enters a time loop for `n_steps`.
# 3. **Crucially**, inside the time loop, it has nested loops (`for i... for j...`) 
#    that calculate the gravitational force between every pair of particles directly. 
#    This O(N^2) calculation dominates the runtime and becomes extremely slow as N increases.
# 4. It updates velocities and positions using a simple Forward Euler step (velocities 
#    are updated using acceleration at the start of the step, then positions are updated 
#    using the *new* velocity). This is numerically inaccurate for orbits; a Leapfrog 
#    scheme would be much better.
# 5. It plots the final projected particle positions, which should show some clustering or evolution.
# **This example primarily serves to illustrate the O(N^2) calculation bottleneck of 
# direct summation and the basic structure of an N-body time loop. It is NOT a 
# practical way to run simulations with more than a few hundred particles.**
```

These examples illustrate two extremes: `galpy` provides high-level, efficient orbit integration in *fixed* potentials, while the direct summation script shows the core (but inefficient) calculation for *mutual* gravity. Real N-body simulations use highly optimized implementations of methods like TreePM or P³M in compiled languages to handle millions or billions of interacting particles efficiently. Python's role is typically in setting up these simulations (generating ICs, writing parameter files) and analyzing their outputs.

**33.5 Analyzing N-body Output**

Once an N-body simulation (run using codes like GADGET, AREPO, Enzo, etc.) has completed, the crucial task is to analyze the generated output data to extract scientifically meaningful information. Simulation outputs typically consist of a series of **snapshots**: files written at specific intervals (time or redshift) that contain the state (positions, velocities, masses, particle IDs) of all particles at that moment. Analyzing these snapshots involves reading the data, calculating derived quantities, identifying structures like halos, and comparing statistical properties to theory or observations. Python, with libraries like `h5py`, `astropy`, `numpy`, `scipy`, and especially `yt`, provides powerful tools for this post-processing analysis.

**1. Reading Snapshot Data:** Large N-body simulations predominantly store snapshot data in binary formats, most commonly **HDF5** (Sec 2.1, 2.2) or specific binary formats like the GADGET format (which also has an HDF5 variant).
*   **HDF5:** As seen in Sec 2.2 and App 2.A, the `h5py` library allows direct access to the hierarchical structure. You open the file (`h5py.File(...)`), navigate groups (e.g., `f['PartType1']` for dark matter), and read datasets (e.g., `f['PartType1/Coordinates'][:]`) into NumPy arrays. You also need to read metadata (like box size, time, mass table) from attributes, typically stored in a 'Header' group.
*   **GADGET Format 1/2 (Binary):** Reading these requires understanding their specific block structure. Libraries like `yt` (Chapter 35), `pygadgetreader`, or custom Python scripts using `struct` or `numpy.fromfile` are needed to parse these formats correctly.
The **`yt`** analysis toolkit (Chapter 35) provides a high-level interface (`yt.load()`) that can automatically detect and parse various snapshot formats (including HDF5 and GADGET binary), loading the data and metadata into a unified `yt` dataset object, significantly simplifying the initial data loading step.

**2. Basic Particle Properties:** Once loaded (e.g., into NumPy arrays `pos`, `vel`, `mass`), you can directly analyze basic particle properties using standard NumPy/SciPy functions:
*   Calculate magnitudes of velocity vectors: `speeds = np.sqrt(np.sum(vel**2, axis=1))`
*   Calculate kinetic energy: `ke = 0.5 * mass * speeds**2` (if mass is an array or scalar)
*   Plot histograms of speed, mass, or position distributions (`plt.hist`).
*   Create scatter plots of velocity components or position projections (`plt.scatter`).

**3. Density Profiles:** A fundamental property of simulated structures (like dark matter halos) is their **density profile** ρ(r), describing how density varies with distance `r` from the center. To calculate this numerically from particle data:
*   Identify the center of the structure (e.g., center of mass, or potential minimum).
*   Define a series of radial bins (often logarithmically spaced).
*   Calculate the distance `r` of each particle from the center.
*   For each radial bin `[rᵢ, rᵢ₊₁]`, count the number of particles `Nᵢ` within that shell and sum their mass `Mᵢ`.
*   Calculate the volume of the spherical shell `Vᵢ = (4/3)π (rᵢ₊₁³ - rᵢ³)`.
*   The average density in the bin is ρᵢ = Mᵢ / Vᵢ.
*   Plot ρᵢ versus the bin center radius (often on log-log axes).
Libraries like `yt` provide functions to calculate profiles efficiently. Manual calculation involves NumPy indexing and looping/histogramming.

```python
# --- Code Example 1: Calculating Density Profile (Manual NumPy) ---
import numpy as np
import matplotlib.pyplot as plt

print("Calculating density profile from particle data (manual NumPy):")

# Simulate particle positions (e.g., within a sphere, denser near center)
np.random.seed(10)
n_parts = 50000
# Sample radius from r^2 distribution (for uniform density sphere) then modify
# Or sample from NFW/Plummer analytical profile for more realism
radius_sampled = np.random.power(a=3, size=n_parts) * 10.0 # Concentrated near center
# Generate isotropic directions
phi = np.random.uniform(0, 2*np.pi, n_parts)
costheta = np.random.uniform(-1, 1, n_parts)
sintheta = np.sqrt(1 - costheta**2)
x = radius_sampled * sintheta * np.cos(phi)
y = radius_sampled * sintheta * np.sin(phi)
z = radius_sampled * costheta
positions = np.vstack((x, y, z)).T
mass_per_particle = 1.0 # Assume equal mass particles
print(f"\nGenerated {n_parts} particle positions.")

# Assume center is at (0, 0, 0)
center = np.array([0., 0., 0.])

# Calculate radial distances
distances = np.sqrt(np.sum((positions - center)**2, axis=1))

# Define radial bins (logarithmic)
min_r = 0.1 
max_r = np.max(distances)
n_bins = 20
log_r_bins = np.logspace(np.log10(min_r), np.log10(max_r), n_bins + 1)
bin_centers_log = np.sqrt(log_r_bins[:-1] * log_r_bins[1:]) # Geometric mean center

# Calculate mass and density in bins
mass_in_bin = np.zeros(n_bins)
for i in range(n_bins):
    # Find particles within the radial shell
    in_bin_mask = (distances >= log_r_bins[i]) & (distances < log_r_bins[i+1])
    mass_in_bin[i] = np.sum(mass_per_particle * in_bin_mask) # Sum mass if variable

# Calculate bin volumes (spherical shells)
bin_volumes = (4./3.) * np.pi * (log_r_bins[1:]**3 - log_r_bins[:-1]**3)

# Calculate density (handle potential zero volume or mass)
density_profile = np.zeros_like(mass_in_bin)
valid = (bin_volumes > 0) & (mass_in_bin >= 0) # Should always be true here
density_profile[valid] = mass_in_bin[valid] / bin_volumes[valid]

print("Calculated density profile.")

# Plot density profile
print("Generating density profile plot...")
plt.figure(figsize=(7, 5))
plt.plot(bin_centers_log, density_profile, 'o-')
plt.xscale('log')
plt.yscale('log')
plt.xlabel("Radius (arbitrary units)")
plt.ylabel("Density (arbitrary units)")
plt.title("N-body Density Profile")
plt.grid(True, which='both', alpha=0.4)
# plt.show()
print("Plot generated.")
plt.close()
print("-" * 20)

# Explanation: This code calculates a radial density profile from particle positions.
# 1. It simulates 3D particle positions concentrated towards the center.
# 2. It calculates the distance of each particle from the assumed center (0,0,0).
# 3. It defines logarithmically spaced radial bins `log_r_bins`.
# 4. It loops through the bins:
#    - Selects particles within the current radial shell `in_bin_mask`.
#    - Sums the mass of particles in the bin (`mass_in_bin`).
# 5. It calculates the volume of each spherical shell `bin_volumes`.
# 6. It calculates the density `mass_in_bin / bin_volumes`.
# 7. It plots density vs. bin center radius on log-log axes, showing the profile.
# Note: Using libraries like `yt` automates and optimizes this process considerably.
```

**4. Velocity Dispersion Profiles:** Similarly, the **velocity dispersion** σ<0xE1><0xB5><0x9B>(r) (the standard deviation of particle velocities) as a function of radius provides information about the kinematic state and dynamical temperature of the system. Calculating this involves:
*   Defining radial bins.
*   For each bin, selecting the particles within that shell.
*   Calculating the mean velocity **v̄**<0xE1><0xB5><0xA2> of particles in the shell.
*   Calculating the velocity dispersion along each axis (σ<0xE1><0xB5><0x8B>², σ<0xE1><0xB5><0xA7>², σ<0xE1><0xB5><0xA3>²) as the variance of velocity components relative to the mean velocity (e.g., σ<0xE1><0xB5><0x8B>² = mean[(v<0xE1><0xB5><0x8B>ᵢ - v̄<0xE1><0xB5><0x8B>)²]).
*   Often, the 1D velocity dispersion is reported: σ₁<0xE1><0xB5><0x87> = sqrt[(σ<0xE1><0xB5><0x8B>² + σ<0xE1><0xB5><0xA7>² + σ<0xE1><0xB5><0xA3>²)/3].
*   Plot σ₁<0xE1><0xB5><0x87>(r) vs. radius. `scipy.stats.binned_statistic` or similar tools can aid this calculation.

**5. Structure/Halo Finding:** A crucial step in analyzing cosmological N-body simulations is identifying the gravitationally bound dark matter halos, which are the sites where galaxies are expected to form. Various **halo finding algorithms** exist:
*   **Friends-of-Friends (FoF):** Links particles together if their separation is less than a small fraction (`b`, the linking length, typically ~0.2) of the mean inter-particle separation. Groups of linked particles form FoF halos. Simple and fast but can sometimes link physically distinct structures that are close together (bridging).
*   **Spherical Overdensity (SO):** Identifies density peaks and grows spheres around them until the average density within the sphere drops below a certain threshold (e.g., 200 times the critical density of the Universe, Δ=200c, or 200 times the mean matter density, Δ=200m). Defines halos based on spherical boundaries and enclosed mass (M₂₀₀<0xE1><0xB5><0x84>, M₂₀₀<0xE1><0xB5><0x8B>). Often considered more physically meaningful than FoF halos, but assumes spherical symmetry.
*   Phase-Space Finders (e.g., **Rockstar**, **SUBFIND**): Operate in 6D phase space (position + velocity) to identify gravitationally bound (sub)structures even within larger halos, often providing more accurate identification of subhalos compared to purely spatial finders.
Running halo finders is often a computationally intensive post-processing step performed on HPC systems. The output is typically a **halo catalog** listing the position, mass, radius, velocity, and other properties (spin, concentration) for each identified halo and subhalo. `yt` includes interfaces to some halo finders or tools to work with their outputs.

**6. Statistical Analysis:** Beyond individual object properties, analyzing the statistical properties of the particle distribution or halo population is key. This includes calculating the **two-point correlation function** (measuring the excess probability of finding pairs of particles/halos at a given separation compared to random) or its Fourier transform, the **power spectrum**. These statistics are direct predictions of cosmological models and are compared quantitatively to observational measurements of galaxy clustering or weak lensing.

Analyzing N-body simulation output requires a combination of file reading (using `h5py`, `astropy.io.fits`, or ideally `yt`), numerical computation (using `numpy`, `scipy`) to calculate derived properties and profiles, specialized algorithms (like halo finders), and visualization techniques (`matplotlib`, `yt`). Python provides a powerful environment for orchestrating these analysis tasks.

**33.6 Collisionless vs. Collisional Effects**

As introduced in Section 33.1, the distinction between **collisionless** and **collisional** N-body dynamics is crucial, determined primarily by comparing the system's age with its **two-body relaxation timescale (t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99>)**. In collisionless systems (galaxies, dark matter halos), t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99> is much longer than the age of the Universe, meaning particle orbits are governed by the smooth mean gravitational potential, and close encounters have negligible cumulative effects. In collisional systems (star clusters, planetary systems), t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99> is comparable to or shorter than the system's age, meaning the cumulative effect of numerous weak two-body gravitational encounters significantly alters particle orbits and drives the system's long-term evolution. Simulating collisional systems requires accurately modeling these encounter effects.

**Two-body relaxation** is the fundamental process driving collisional dynamics. Repeated gravitational tugs between nearby stars cause their velocities to diffuse randomly, gradually driving the system towards a state of **energy equipartition**. In this state, lower-mass stars tend to gain kinetic energy and achieve higher average speeds, while higher-mass stars tend to lose kinetic energy and slow down. This has profound consequences for the structure of star clusters.

One major consequence is **mass segregation**. Due to energy equipartition, the more massive stars slow down and sink towards the gravitational potential well at the center of the cluster through dynamical friction. Conversely, lower-mass stars gain energy and tend to migrate outwards towards the cluster's periphery. Over time, this leads to a clear stratification, with the central regions becoming dominated by the most massive stars, while the outer regions are populated primarily by lower-mass stars. This process is readily observed in old globular clusters and dynamically evolved open clusters. Simulating mass segregation requires an N-body code that accurately resolves two-body encounters and includes a realistic spectrum of stellar masses.

The congregation of massive stars in the center can lead to further dramatic evolution. As the core becomes denser and dominated by heavy stars (which might include stellar remnants like white dwarfs, neutron stars, and black holes), the relaxation time there becomes even shorter. This can drive the central density extremely high, potentially leading to **core collapse**, where the central density formally diverges (though physical processes like binary star formation through three-body encounters can halt or reverse the collapse). Simulating core collapse accurately requires sophisticated collisional N-body codes that can handle extremely high densities and the dynamics of binary and multiple star systems formed dynamically.

Another effect driven by two-body relaxation is **evaporation**. Low-mass stars scattered onto high-energy orbits in the outer parts of the cluster can gain enough energy through encounters to exceed the cluster's escape velocity. These stars gradually "evaporate" from the cluster, leading to a slow decrease in the cluster's total mass and size over billions of years, particularly significant for lower-mass open clusters which can dissolve completely within a few Gyr. Modeling evaporation accurately requires simulating the cluster's interaction with the external Galactic tidal field as well as internal relaxation.

Collisional dynamics also play a crucial role in the formation and evolution of **binary stars** within dense clusters. Three-body encounters (two stars passing close to a third) can lead to the formation of new binaries or the modification (hardening or disruption) of existing ones. Interactions involving binaries can inject energy into the cluster (as the binary tightens), potentially halting or reversing core collapse ("binary burning"). Accurately simulating binary interactions within a large N-body context requires specialized algorithms within collisional codes.

In contrast, **collisionless dynamics**, relevant for galaxies, are governed by processes operating on larger scales and involving the collective gravitational field. **Violent relaxation** describes the rapid changes in the overall potential during mergers or collapses, which can efficiently redistribute energy and phase-mix stellar orbits, leading to a quasi-equilibrium state much faster than two-body relaxation. **Phase mixing** describes how stars on slightly different orbits gradually spread out in phase space due to differing orbital frequencies, leading to the smoothing out of initial clumps or streams. **Dynamical friction** causes massive objects (like satellite galaxies or black holes) moving through a sea of lighter particles (stars or dark matter) to lose orbital energy and spiral inwards due to the gravitational wake they induce. These collective, mean-field effects dominate galaxy evolution, while two-body relaxation is largely irrelevant.

Therefore, simulating collisionless systems (galaxies, dark matter halos) focuses on accurately calculating the large-scale potential and orbits within it, often using softened gravity and approximate solvers like TreePM. Simulating collisional systems (star clusters) requires accurately calculating short-range forces between individual stars using direct summation or specialized regularized techniques to capture the effects of two-body relaxation, mass segregation, core collapse, and binary dynamics. These are distinct computational problems requiring different numerical algorithms and simulation codes. Python tools are typically used for analysis of both types, but the core simulation engines differ significantly.

**Application 33.A: Simulating a Minor Merger using Orbit Integration**

**(Paragraph 1)** **Objective:** This application demonstrates a simplified approach to studying galaxy interactions, specifically the tidal disruption of a smaller satellite galaxy orbiting within the potential of a larger host galaxy. Instead of a full N-body simulation (which would track self-gravity of both galaxies), we use **orbit integration** of test particles within a *fixed* representation of the combined gravitational potential using the `galpy` library (Sec 33.4). This illustrates how orbit integration can capture key dynamical effects like tidal stripping and the formation of tidal tails, reinforcing concepts from Sec 33.1, 33.2, 33.4.

**(Paragraph 2)** **Astrophysical Context:** The hierarchical structure formation model predicts that large galaxies like the Milky Way grow, in part, by accreting smaller satellite galaxies. As a satellite orbits within the host's massive dark matter halo, it experiences strong tidal forces that can strip away its stars and gas, creating long, trailing and leading **tidal streams** or **tails**. Observing these streams provides crucial evidence for past accretion events and probes the gravitational potential of the host galaxy's halo. Simulating this disruption process helps interpret observed streams and understand the dynamics of satellite accretion.

**(Paragraph 3)** **Data Source:** Not applicable; this simulation uses idealized, analytical models for the gravitational potentials of the host and satellite galaxies. The inputs are the parameters defining these potentials (e.g., scale lengths, masses) and the initial 6D phase-space coordinates (position and velocity) of the satellite galaxy relative to the host center, defining its initial orbit.

**(Paragraph 4)** **Modules Used:** `galpy.potential` (to define analytical potentials for host and satellite), `galpy.orbit` (to represent and integrate orbits), `numpy` (for initial conditions and time arrays), `matplotlib.pyplot` (for visualization), `astropy.units` (recommended for defining parameters and interpreting results in physical units).

**(Paragraph 5)** **Technique Focus:** Setting up multi-component gravitational potentials in `galpy`. Representing both the satellite's center-of-mass orbit and the orbits of individual "test stars" initially bound to the satellite. Integrating these orbits simultaneously within the combined (host + satellite) potential over a long timescale using `orbit.integrate()`. Visualizing the resulting spatial distribution of the satellite's stars over time to show the development of tidal features. This leverages `galpy`'s orbit integration capabilities (Sec 33.4) as a proxy for visualizing effects seen in more complex N-body merger simulations.

**(Paragraph 6)** **Processing Step 1: Define Potentials:** Use `galpy.potential` to define realistic potentials for the host galaxy (e.g., `MWPotential2014` or a combination of `MiyamotoNagaiPotential` for disk, `HernquistPotential` for bulge, `NFWPotential` or `LogarithmicHaloPotential` for halo) and the satellite galaxy (e.g., a `PlummerPotential` or `HernquistPotential` representing its stars and dark matter). Ensure units are consistent (e.g., use `galpy`'s natural units or work with Astropy Quantities).

**(Paragraph 7)** **Processing Step 2: Define Initial Orbits:**
    *   Define the initial 6D phase-space coordinates (`vxvv` = [R, vR, vT, z, vz, phi] or equivalent) for the *center* of the satellite galaxy relative to the host center, placing it on an initial orbit (e.g., large apocenter, specific velocity). Create an `Orbit` object `o_sat` for this.
    *   Generate initial positions and velocities for numerous "test stars" *relative* to the satellite center, sampling them from a distribution representing the satellite's internal structure (e.g., sample from the satellite's potential model itself using `galpy`'s distribution function tools or simpler methods like Plummer sampling shown in Sec 33.2). Add the satellite's bulk velocity to these internal velocities. Create `Orbit` objects `o_star_i` for each test star, ensuring their initial coordinates are in the host galaxy's reference frame.

**(Paragraph 8)** **Processing Step 3: Integrate Orbits:** Define a time array `ts` spanning the desired simulation duration (e.g., 0 to 5 Gyr) with sufficient steps. Integrate the orbit of the satellite's center `o_sat` within the *host* potential only (if treating satellite as rigid for its own orbit). Integrate the orbits of *all* test stars `o_star_i` within the *combined potential* of the host *plus* the (potentially moving) satellite potential. (A simpler approach, illustrated below, treats the host potential as dominant and fixed, integrating test stars only within it, starting from initial positions offset from the satellite's initial position).

**(Paragraph 9)** **Processing Step 4: Analyze and Visualize:** Access the integrated positions (`o_star_i.x(ts)`, `o_star_i.y(ts)`, `o_star_i.z(ts)`) for all test stars at different time steps `t` in the `ts` array. Create plots showing the spatial distribution of these stars (e.g., X-Y projection) at selected snapshots (e.g., t=0, 1, 3, 5 Gyr). Color points perhaps by initial binding energy or radius within the satellite. Observe the formation of elongated tidal tails stretching along the satellite's path as stars are stripped by the host's tidal field. Plot the satellite center's orbit (`o_sat.plot()`) for reference.

**(Paragraph 10)** **Processing Step 5: Interpretation:** Analyze the morphology of the resulting stellar distribution. Identify the leading and trailing tidal tails. Observe how the bound core of the satellite potentially shrinks over time. Relate the extent and shape of the tails to the satellite's orbit and the host potential's properties. Recognize the limitations: this uses test particles in a mostly fixed potential, ignoring the satellite's self-gravity, internal evolution, and the dynamical friction affecting its orbit (unless explicitly included in the potential or integration). A full N-body simulation would capture these effects more accurately but is vastly more complex.

**Output, Testing, and Extension:** The primary output is a series of plots showing the spatial distribution of the satellite's test stars at different times, illustrating tidal stripping and tail formation, along with the satellite's orbital path. **Testing:** Verify the satellite orbit follows a plausible path. Check if stars are clearly stripped and form extended features over time. Ensure units are handled correctly if using `astropy.units`. **Extensions:** (1) Use more realistic galaxy potentials from `galpy.potential.mwpotentials`. (2) Vary the satellite's initial orbit (e.g., eccentricity, inclination) or mass/size and observe the impact on the disruption process and tail morphology. (3) Color the particles in the plots based on their velocity relative to the satellite center to distinguish bound stars from stripped stars. (4) Implement dynamical friction approximately by adding a drag term to the satellite's equation of motion during integration (requires custom integration or modifying potential). (5) Compare the visual results qualitatively to snapshots from actual N-body merger simulations found in the literature.

```python
# --- Code Example: Application 33.A ---
# Note: Requires galpy, astropy, matplotlib
try:
    # Using galpy potentials and orbit integration
    from galpy.potential import MiyamotoNagaiPotential, NFWPotential, PlummerPotential
    from galpy.orbit import Orbit
    from galpy.util import conversion # For unit conversions if needed
    from astropy import units as u
    import numpy as np
    import matplotlib.pyplot as plt
    galpy_installed = True
except ImportError:
    galpy_installed = False
    print("NOTE: galpy or astropy not installed. Skipping application.")

print("Simulating Satellite Tidal Disruption using galpy Orbit Integration:")

if galpy_installed:
    # --- Step 1: Define Potentials ---
    # Host Galaxy Potential (Simplified Milky Way: Disk + Halo)
    # Using galpy's natural units initially (can scale later)
    # Assume Vo=220 km/s, Ro=8 kpc for scaling
    vo_kms = 220.
    ro_kpc = 8.
    
    host_disk = MiyamotoNagaiPotential(amp=0.6 * (vo_kms**2), a=3.0/ro_kpc, b=0.28/ro_kpc)
    host_halo = NFWPotential(amp=0.4 * (vo_kms**2), a=16.0/ro_kpc) # Adjust amp based on total mass needed
    host_potential = [host_disk, host_halo]
    print("\nDefined Host Galaxy Potential (Disk + NFW Halo).")

    # Satellite Potential (for generating internal kinematics initially)
    # Assume satellite mass ~1e8 Msun, scale radius ~0.5 kpc
    # Convert satellite mass to galpy units (relative to M_vir ~ vo^2*ro/G)
    # M_vir_approx = vo_kms**2 * ro_kpc * u.kpc / const.G
    # sat_mass_msun = 1e8 * u.Msun
    # sat_amp_galpy = (sat_mass_msun / M_vir_approx.to(u.Msun)).value 
    sat_amp_galpy = 0.005 # Adjusted amplitude in galpy units
    sat_rcore_galpy = 0.5 / ro_kpc # Scale radius in galpy units
    sat_potential = PlummerPotential(amp=sat_amp_galpy, b=sat_rcore_galpy)
    print("Defined Satellite Potential (Plummer Sphere).")

    # --- Step 2: Define Initial Orbits ---
    # Satellite Center Initial Orbit (Example: start far out, falling in)
    # [R,vR,vT, z,vz,phi] in galpy units (R/ro, v/vo, v/vo, z/ro, v/vo, rad)
    R_init = 150.0 / ro_kpc # Start at 150 kpc
    z_init = 50.0 / ro_kpc
    vR_init = -100.0 / vo_kms
    vT_init = 50.0 / vo_kms # Some tangential velocity
    vz_init = -50.0 / vo_kms
    phi_init = 0.0
    o_sat = Orbit([R_init, vR_init, vT_init, z_init, vz_init, phi_init])
    print(f"\nSatellite initial state (R,vR,vT,z,vz,phi): {np.round(o_sat.vxvv, 2)}")

    # Generate Test Star Positions/Velocities *relative* to satellite center
    # Using simple sampling here - more sophisticated needed for true equilibrium
    n_stars = 200
    # Sample positions within satellite potential scale radius
    rel_pos = np.random.randn(n_stars, 3) * sat_rcore_galpy 
    # Sample velocities based on crude velocity dispersion estimate
    # sigma_approx = np.sqrt(G * M / (6*a)) # Virial theorem for Plummer
    # sigma_galpy = np.sqrt(const.G * sat_mass_msun / (6*sat_rcore_kpc*u.kpc)).to(u.km/u.s).value / vo_kms
    sigma_galpy = 0.1 # Assume internal dispersion ~ 0.1 * vo
    rel_vel = np.random.normal(0, sigma_galpy, size=(n_stars, 3))

    # Convert satellite initial state to Cartesian host frame
    x_sat_init = o_sat.x(); y_sat_init = o.y(); z_sat_init = o.z()
    vx_sat_init = o.vx(); vy_sat_init = o.vy(); vz_sat_init = o.vz()

    # Add relative positions/velocities to satellite center state in host frame
    # Ensure consistent coordinate system (galpy usually cylindrical for vxvv)
    # Simplification: Adding Cartesian relative values to host Cartesian center
    # This requires converting initial vxvv to Cartesian x,y,z,vx,vy,vz first
    # For simplicity, we'll just add offsets conceptually here for the plot setup
    # A proper implementation needs careful coord transforms or sampling in host frame.
    
    # For plotting: Initial positions of stars centered on satellite
    initial_star_pos_host_x = x_sat_init + rel_pos[:,0]
    initial_star_pos_host_y = y_sat_init + rel_pos[:,1]
    initial_star_pos_host_z = z_sat_init + rel_pos[:,2]
    # Initial velocities (ignoring complexity of adding velocity vectors properly)
    initial_star_vel_host_vx = vx_sat_init + rel_vel[:,0]
    initial_star_vel_host_vy = vy_sat_init + rel_vel[:,1]
    initial_star_vel_host_vz = vz_sat_init + rel_vel[:,2]
    
    # Create Orbit objects for stars (Need full 6D state vector per star)
    # This setup is complex - simplifying for illustration by just integrating 
    # the initial relative positions added to the *final* satellite positions later
    print(f"Generated {n_stars} test star relative positions/velocities.")

    # --- Step 3: Integrate Satellite Orbit ---
    # Integrate only the satellite center in the host potential first
    integration_time = 5.0 * u.Gyr # Integrate for 5 Gyr
    n_steps_orbit = 501
    times_gyr = np.linspace(0., integration_time.value, n_steps_orbit)
    # Convert time to galpy natural units for integration
    times_galpy = times_gyr * u.Gyr.to(u.s) * (vo_kms * 1000) / (ro_kpc * u.kpc.to(u.m))
    
    print(f"\nIntegrating satellite orbit for {integration_time}...")
    o_sat.integrate(times_galpy, host_potential)
    print("Satellite orbit integration complete.")

    # --- Step 4 & 5: Visualize (Simplified: Show satellite orbit + initial/final blob) ---
    # A full simulation integrates each star orbit. Here we simplify visualization.
    print("Generating visualization...")
    fig = plt.figure(figsize=(7, 7))
    ax = fig.add_subplot(111)
    
    # Plot satellite orbit path
    o_sat.plot(d1='x', d2='y', ax=ax, label='Satellite Orbit Path', color='gray', overplot=False)
    
    # Plot initial star distribution around initial satellite position
    ax.plot(initial_star_pos_host_x, initial_star_pos_host_y, 'bo', markersize=1, alpha=0.3, label='Initial Stars')
    
    # Plot final star distribution conceptually around final satellite position
    # (This is NOT a real simulation of stripping, just shows start/end)
    final_x_sat = o_sat.x(times_galpy[-1])
    final_y_sat = o.y(times_galpy[-1])
    # Use same relative positions for illustration (in reality they'd be stripped)
    final_star_pos_x = final_x_sat + rel_pos[:,0]
    final_star_pos_y = final_y_sat + rel_pos[:,1]
    ax.plot(final_star_pos_x, final_star_pos_y, 'rs', markersize=1, alpha=0.3, label='Final Stars (Conceptual)')
    
    # Scale axes based on orbit range (use galpy's units, convert labels)
    xlim = ax.get_xlim(); ylim = ax.get_ylim()
    max_range = max(abs(xlim[0]), abs(xlim[1]), abs(ylim[0]), abs(ylim[1]))
    ax.set_xlim(-max_range, max_range); ax.set_ylim(-max_range, max_range)
    ax.set_xlabel(f"X ({ro_kpc:.0f} kpc units)")
    ax.set_ylabel(f"Y ({ro_kpc:.0f} kpc units)")
    ax.set_title("Satellite Orbit and Conceptual Star Distribution")
    ax.legend()
    ax.set_aspect('equal')
    fig.tight_layout()
    # plt.show()
    print("Conceptual plot generated.")
    plt.close(fig)
    
else:
    print("Skipping galpy execution.")

print("-" * 20)
```

**(Paragraph 1)** **Objective:** This application conceptually explores the phenomenon of **mass segregation** in collisional N-body systems (Sec 33.6) like star clusters. Since simulating collisional dynamics accurately requires specialized codes (e.g., `NBODY6`, `PeTar`) usually written in Fortran/C++/CUDA and not easily run *within* Python, this application focuses on the **analysis** step: assuming we have output snapshots from such a simulation at early and late times, we use Python (`numpy`, `matplotlib`) to analyze the particle data and demonstrate how the radial distribution of massive stars changes relative to low-mass stars, illustrating the segregation effect. Reinforces Sec 33.5, 33.6.

**(Paragraph 2)** **Astrophysical Context:** Star clusters, particularly old, dense globular clusters, are ideal environments for studying collisional stellar dynamics. Over their long lifetimes (billions of years), two-body gravitational encounters gradually redistribute kinetic energy among stars. Energy equipartition leads more massive stars to lose energy and sink towards the cluster center, while lower-mass stars gain energy and move outwards. This **mass segregation** is a key observable prediction of collisional dynamics and affects the cluster's structure, appearance, and long-term evolution (e.g., core collapse, evaporation). Analyzing the spatial distribution of stars of different masses in observed or simulated clusters provides direct evidence for this process.

**(Paragraph 3)** **Data Source:** Output snapshots from a **collisional N-body simulation** of a star cluster (e.g., generated by `NBODY6` or similar codes). We need at least two snapshots: one at an early time (t ≈ 0, before significant segregation) and one at a late time (t >> t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99>, where t<0xE1><0xB5><0xA3><0xE1><0xB5><0x86><0xE1><0xB5><0x87><0xE1><0xB5><0x8A><0xE1><0xB5><0x99> is the initial half-mass relaxation time). Each snapshot must contain particle positions (x, y, z) and masses (m). The data format might be ASCII, simple binary, or potentially FITS/HDF5 depending on the code. We will simulate loading this data into NumPy arrays.

**(Paragraph 4)** **Modules Used:** `numpy` (for reading data conceptually, array manipulation, calculations), `matplotlib.pyplot` (for plotting). `astropy.io.fits` or `h5py` might be needed depending on the actual snapshot format.

**(Paragraph 5)** **Technique Focus:** Analyzing particle data from N-body snapshots (Sec 33.5). Calculating radial distances. Binning data radially. Calculating statistics (average mass) within bins. Comparing results between early and late time snapshots to demonstrate a dynamical effect (mass segregation, Sec 33.6). Visualizing the difference in radial distributions.

**(Paragraph 6)** **Processing Step 1: Load Snapshot Data (Simulated):** Simulate loading particle data for two time steps (t=0 and t=late) into NumPy arrays: `pos0`, `mass0` and `pos_late`, `mass_late`. Ensure `mass` array contains a realistic range of stellar masses (e.g., sampled from an IMF, with some massive stars). Assume the cluster center is at (0,0,0) or calculate it (e.g., center of mass) if necessary.

**(Paragraph 7)** **Processing Step 2: Calculate Radii:** For both snapshots, calculate the 3D radial distance of each particle from the cluster center: `r0 = np.sqrt(np.sum(pos0**2, axis=1))`, `r_late = np.sqrt(np.sum(pos_late**2, axis=1))`.

**(Paragraph 8)** **Processing Step 3: Define Radial Bins:** Choose a set of radial bins (e.g., logarithmically spaced) covering the extent of the cluster, e.g., `radial_bins = np.logspace(...)`.

**(Paragraph 9)** **Processing Step 4: Calculate Average Mass per Bin:** For each snapshot (t=0 and t=late):
    *   Loop through the radial bins `[bin_min, bin_max]`.
    *   Select particles falling within the current radial bin using boolean masks on `r0` or `r_late`.
    *   Calculate the *average mass* of the selected particles in that bin using `np.mean(mass[mask])`. Handle empty bins (assign NaN or zero).
    *   Store the average mass profile for both snapshots (`avg_mass0(r)`, `avg_mass_late(r)`).

**(Paragraph 10)** **Processing Step 5: Visualize and Interpret:** Plot both `avg_mass0` and `avg_mass_late` against the bin center radii on the same axes. Compare the two profiles. If mass segregation has occurred, the `avg_mass_late` profile should show significantly higher average masses in the inner bins and potentially lower average masses in the outer bins compared to the `avg_mass0` profile (which should be relatively flat if masses were initially randomly distributed with respect to radius). Add labels and title explaining the plot shows evidence for mass segregation.

**Output, Testing, and Extension:** The primary output is the plot comparing the average stellar mass per radial bin at the early and late simulation times. **Testing:** Verify that the average mass profile is roughly constant at t=0 if initial masses were uncorrelated with position. Confirm that the average mass *increases* towards the center at the late time. Check that the overall average mass remains approximately constant between the two snapshots (ignoring escapers). **Extensions:** (1) Instead of average mass, plot the cumulative radial distribution function for different *mass groups* (e.g., low-mass stars vs. high-mass stars) at both times. The high-mass distribution should become more centrally concentrated at late times. (2) Calculate the half-mass radius (radius containing half the total cluster mass) for different mass groups and see how it evolves. (3) If velocity data is available, calculate and compare the velocity dispersion profiles for low-mass and high-mass stars at the late time (expect σ<0xE1><0xB5><0x97><0xE1><0xB5><0x92><0xE1><0xB5><0x98> < σ<0xE1><0xB5><0x8B><0xE1><0xB5><0x92><0xE1><0xB5><0x98><0xE1><0xB5><0x98> due to energy equipartition). (4) Estimate the relaxation time for the simulated cluster and confirm the "late" snapshot is indeed many relaxation times old.

```python
# --- Code Example: Application 33.B ---
# Analyzes simulated N-body snapshot data to show mass segregation.
# Assumes snapshot data (pos, mass) at t=0 and t=late are loaded.

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats # For binned_statistic

print("Analyzing Mass Segregation in Simulated Star Cluster Snapshots:")

# Step 1: Simulate Loading Snapshot Data
np.random.seed(123)
n_total = 10000
# Simulate masses from a power law IMF (simplified: high mass = low number)
# Mass range 0.1 to 10 Msun, power index -2.35
mass_samples = (np.random.power(a=1.0/1.35, size=n_total) * 9.9 + 0.1) # Crude approximation
mass0 = mass_samples.copy()
mass_late = mass_samples.copy() # Assume mass loss is negligible for simplicity

# Simulate positions: Initially random, later segregated
# Initial: Uniform random sphere (for simplicity)
radius0 = np.random.uniform(0, 5.0, n_total)**(1./3.) * 5.0 # Approx uniform density
phi0 = np.random.uniform(0, 2*np.pi, n_total); costheta0 = np.random.uniform(-1, 1, n_total)
sintheta0 = np.sqrt(1 - costheta0**2)
pos0 = np.vstack((radius0*sintheta0*np.cos(phi0), radius0*sintheta0*np.sin(phi0), radius0*costheta0)).T

# Final: Make massive stars more centrally concentrated
# Introduce correlation between radius and mass for late snapshot
# Smaller radius for larger mass (crude segregation model)
radius_late = radius0 * (mass_late / np.mean(mass_late))**(-0.5) # Massive stars have smaller radii
radius_late = np.clip(radius_late, 0, np.max(radius0)*1.1) # Keep within bounds roughly
phi_late = np.random.uniform(0, 2*np.pi, n_total); costheta_late = np.random.uniform(-1, 1, n_total)
sintheta_late = np.sqrt(1 - costheta_late**2)
pos_late = np.vstack((radius_late*sintheta_late*np.cos(phi_late), 
                      radius_late*sintheta_late*np.sin(phi_late), 
                      radius_late*costheta_late)).T
print(f"\nGenerated/Loaded {n_total} particles for t=0 and t=late snapshots.")
print(f"  Mass range: {np.min(mass0):.2f} - {np.max(mass0):.2f}")

# Step 2: Calculate Radii
center = [0., 0., 0.]
r0 = np.sqrt(np.sum((pos0 - center)**2, axis=1))
r_late = np.sqrt(np.sum((pos_late - center)**2, axis=1))

# Step 3: Define Radial Bins (logarithmic)
n_bins = 15
max_r = max(np.max(r0), np.max(r_late))
min_r = max(0.01, min(np.min(r0[r0>0]), np.min(r_late[r_late>0])))
radial_bins = np.logspace(np.log10(min_r), np.log10(max_r), n_bins + 1)

# Step 4: Calculate Average Mass per Bin using scipy.stats.binned_statistic
print("\nCalculating average mass profile in radial bins...")
# Calculate for t=0
avg_mass0, _, _ = stats.binned_statistic(
    r0,             # Values to bin on (radius)
    mass0,          # Values to average (mass)
    statistic='mean', 
    bins=radial_bins
)
# Calculate for t=late
avg_mass_late, _, _ = stats.binned_statistic(
    r_late,         # Values to bin on (radius)
    mass_late,      # Values to average (mass)
    statistic='mean', 
    bins=radial_bins
)
# Get bin centers (geometric mean)
bin_centers = np.sqrt(radial_bins[:-1] * radial_bins[1:])
print("Calculation complete.")

# Step 5: Visualize and Interpret
print("Generating plot...")
fig, ax = plt.subplots(figsize=(8, 5))

ax.plot(bin_centers, avg_mass0, 'o--', label='t = 0 (Initial)', alpha=0.8)
ax.plot(bin_centers, avg_mass_late, 's-', label='t = Late (Segregated)', alpha=0.8)

ax.set_xscale('log')
# Optional: y-scale log or linear depending on segregation strength
# ax.set_yscale('log') 
ax.set_xlabel("Radius from Cluster Center (arbitrary units)")
ax.set_ylabel("Average Stellar Mass (arbitrary units)")
ax.set_title("Evidence for Mass Segregation in N-body Simulation")
ax.legend()
ax.grid(True, which='both', alpha=0.4)
fig.tight_layout()
# plt.show()
print("Plot generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This application analyzes simulated snapshot data to show mass segregation.
# 1. It simulates loading position and mass data for t=0 (`pos0`, `mass0`) and a later 
#    time (`pos_late`, `mass_late`). The 'late' positions are crudely constructed 
#    to place massive stars closer to the center, mimicking segregation.
# 2. It calculates the radial distance `r` for each star in both snapshots.
# 3. It defines logarithmically spaced radial bins.
# 4. It uses `scipy.stats.binned_statistic` to efficiently calculate the *average* mass 
#    of stars falling into each radial bin for both the t=0 and t=late snapshots.
# 5. It plots the average mass versus radius for both times. The t=0 profile should be 
#    relatively flat (as initial mass wasn't correlated with radius), while the t=late 
#    profile clearly shows a higher average mass near the center and lower average mass 
#    at larger radii, visually demonstrating mass segregation driven by collisional dynamics.
# This analysis workflow could be applied to output from real collisional N-body codes.
```

**Chapter 33 Summary**

This chapter focused on N-body simulations, a foundational technique in computational astrophysics for modeling systems dominated by gravity. It began by distinguishing between the **collisionless** regime, applicable to large systems like dark matter halos and galaxies where two-body encounters are negligible and evolution is governed by the mean gravitational field (modeled using codes like GADGET, AREPO with TreePM or PM solvers), and the **collisional** regime, relevant for dense star clusters and planetary systems where two-body relaxation significantly alters orbits over time, driving phenomena like mass segregation and core collapse (requiring specialized codes like NBODY6 using direct summation or regularized methods). The critical importance of generating appropriate **initial conditions (ICs)** was emphasized, outlining methods for creating cosmological ICs from linear perturbation theory (using tools like CAMB/CLASS and MUSIC/MonofonIC) and techniques for setting up idealized equilibrium models (like Plummer or NFW profiles, or multi-component disks) for isolated galaxies or clusters, often using analytical sampling or libraries like `galpy`.

An overview of prominent N-body simulation codes (GADGET, AREPO, ENZO, RAMSES, GIZMO, NBODY6/7, etc.) highlighted their different numerical approaches (TreePM, AMR, SPH coupling, direct summation) and target applications. While full N-body codes are typically complex and written in compiled languages, simple illustrations using Python were provided: **orbit integration** within fixed potentials using `galpy` to simulate test particle dynamics (like satellite disruption), and a basic (computationally inefficient) **direct summation** loop to demonstrate the core N² force calculation conceptually. Methods for **analyzing N-body snapshot outputs** (often FITS or HDF5) using Python were discussed, including reading particle data (positions, velocities, masses), calculating fundamental properties like **density profiles** and **velocity dispersion profiles** using NumPy and SciPy, and introducing the concept of **halo finding** algorithms (FoF, SO, phase-space) used to identify structures in cosmological simulations. Finally, the chapter elaborated on the specific physical **effects driven by collisional dynamics** (two-body relaxation, mass segregation, core collapse, evaporation, binary interactions) that distinguish star cluster evolution from the collisionless dynamics governing galaxies.

---

**References for Further Reading (APA Format, 7th Edition):**

1.  **Binney, J., & Tremaine, S. (2008).** *Galactic Dynamics* (2nd ed.). Princeton University Press.
    *(The definitive textbook on gravitational dynamics, providing thorough theoretical treatment of both collisionless (violent relaxation, phase mixing, dynamical friction) and collisional (two-body relaxation, Fokker-Planck equation) N-body systems, relevant to Sec 33.1, 33.6.)*

2.  **Springel, V. (2005).** The cosmological simulation code GADGET-2. *Monthly Notices of the Royal Astronomical Society*, *364*(4), 1105–1134. [https://doi.org/10.1111/j.1365-2966.2005.09655.x](https://doi.org/10.1111/j.1365-2966.2005.09655.x)
    *(Describes GADGET-2, a widely used code for cosmological N-body and SPH simulations, detailing its TreePM gravity solver and time integration, relevant context for Sec 33.3, 32.5.)*

3.  **Heggie, D., & Hut, P. (2003).** *The Gravitational Million-Body Problem: A Multidisciplinary Approach to Star Cluster Dynamics*. Cambridge University Press. [https://doi.org/10.1017/CBO9780511536262](https://doi.org/10.1017/CBO9780511536262)
    *(The standard text on collisional N-body dynamics, focusing on star cluster simulations, covering relaxation, mass segregation, core collapse, and numerical methods including direct summation, relevant to Sec 33.1, 33.6.)*

4.  **Bovy, J. (2015).** galpy: A Python Library for Galactic Dynamics. *The Astrophysical Journal Supplement Series*, *216*(2), 29. [https://doi.org/10.1088/0067-0049/216/2/29](https://doi.org/10.1088/0067-0049/216/2/29) (See also documentation: [https://docs.galpy.org/en/latest/](https://docs.galpy.org/en/latest/))
    *(Introduces the `galpy` library used in Sec 33.4 and Application 33.A for defining potentials and integrating orbits within them.)*

5.  **Knebe, A., Pearce, F. R., et al. (2011).** Halos gone MAD: the Halo Finder Comparison Project. *Monthly Notices of the Royal Astronomical Society*, *415*(3), 2293–2318. [https://doi.org/10.1111/j.1365-2966.2011.18846.x](https://doi.org/10.1111/j.1365-2966.2011.18846.x)
    *(An example of a project comparing different halo finding algorithms (FoF, SO, phase-space based) used in analyzing cosmological simulations, relevant to the halo finding concept mentioned in Sec 33.5.)*
