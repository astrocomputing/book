**Chapter 29: Building Simple LLM-Powered Astro Tools**

Having explored the potential applications and inherent limitations of Large Language Models (LLMs) for literature navigation, code assistance, and data interpretation, this chapter focuses on the practical steps involved in **building simple tools** that leverage these capabilities within an astrophysical context using Python. We move beyond conceptual discussions to demonstrate how to programmatically interact with LLM services, primarily focusing on using **LLM Application Programming Interfaces (APIs)**, such as the one provided by OpenAI for its GPT models. We will cover the basics of setting up API access, making requests, and handling responses. A significant focus will be placed on **prompt engineering** â€“ the art and science of crafting effective prompts to elicit desired and reliable outputs from LLMs for specific tasks like summarization, code generation, or information extraction relevant to astronomy. We will then introduce the powerful **Retrieval-Augmented Generation (RAG)** technique in more practical detail, outlining how combining LLM generation with information retrieved from a specific knowledge base (like local documentation or databases) can significantly improve factual accuracy and reduce hallucinations. This will be illustrated through two concrete examples: building a simple Python tool to answer questions about FITS header keywords by grounding the LLM in keyword descriptions, and creating a utility to automatically summarize recent arXiv astro-ph postings using API calls. Finally, practical considerations such as API costs, rate limits, and response latency will be discussed.

**29.1 Using LLM APIs (e.g., OpenAI API)**

The most common way to integrate the capabilities of powerful, state-of-the-art Large Language Models into custom applications or research workflows is through **Application Programming Interfaces (APIs)** offered by the model providers. Companies like OpenAI (GPT models), Anthropic (Claude models), Google (Gemini models), Cohere, and others provide web APIs that allow developers to send input prompts and receive generated text outputs from their hosted models without needing to manage the massive infrastructure required to run these models locally. This API-based access makes advanced AI capabilities widely accessible.

Interacting with these APIs from Python is typically facilitated by official **client libraries** provided by the vendor. For instance, OpenAI offers the `openai` library (`pip install openai`), Anthropic has the `anthropic` library, and Google provides client libraries within its Cloud ecosystem. These libraries abstract away the low-level details of handling HTTP requests, authentication headers, and JSON parsing, providing a more Pythonic interface to the underlying web service. Using these libraries is the standard practice for programmatic LLM interaction via APIs.

The first prerequisite for using a commercial LLM API is obtaining an **API key**. This usually involves creating an account on the provider's platform (e.g., OpenAI platform, Google Cloud Console) and generating a unique key associated with your account. This key serves both for **authentication** (proving you are authorized to use the service) and **billing** (tracking your usage, as most APIs charge based on the amount of text processed). API keys are highly sensitive credentials; they should be treated like passwords and **never** hardcoded directly into source code, especially if the code will be shared or committed to version control. Standard secure practices include storing the key in an environment variable (e.g., `OPENAI_API_KEY`) that the client library can automatically detect, using a dedicated secrets management tool, or loading it from a configuration file with restricted permissions.

Once the client library is installed and authentication is configured (often just by setting the environment variable), the typical workflow involves initializing a client object (e.g., `client = OpenAI()`). This object then provides methods for interacting with different API endpoints. For modern chat-based models like GPT-3.5, GPT-4, or Claude 3, the interaction usually centers around a "chat completion" endpoint.

Preparing the request for a chat completion API typically involves constructing a **list of messages** representing the conversation history. This list usually contains dictionaries, each specifying a `role` (`'system'`, `'user'`, or `'assistant'`) and the corresponding `content` (the text).
*   The `'system'` message (usually the first one) provides high-level instructions or sets the context for the AI's persona (e.g., "You are a helpful astrophysics research assistant.").
*   `'user'` messages contain the prompts, questions, or instructions from the human user.
*   `'assistant'` messages contain the previous responses generated by the LLM itself, providing conversational context for subsequent turns.
For a simple, single-turn query, the list might just contain a system message and a single user message with the prompt.

The API call itself is then made using a method like `client.chat.completions.create(...)` (in the `openai` v1.0+ library). Besides the target `model` identifier (e.g., `"gpt-4o"`, `"gpt-3.5-turbo"`, `"claude-3-opus-20240229"`) and the `messages` list, several key parameters control the generation process:
*   `max_tokens`: Limits the maximum number of tokens (subword units, roughly related to word count) in the generated response. Important for controlling output length and cost.
*   `temperature`: A value typically between 0 and 2 that controls the randomness of the output. Lower values (e.g., 0.0-0.3) make the output more deterministic and focused (good for factual tasks, code generation). Higher values (e.g., 0.7-1.0) increase randomness, leading to more diverse and "creative" responses but also potentially less coherent or factual ones.
*   `top_p` (nucleus sampling): An alternative parameter to temperature for controlling randomness, selecting from the smallest set of tokens whose cumulative probability exceeds `p`.
*   `n`: Number of different completions to generate for the same prompt.
*   `stop`: Sequences of characters where the API should stop generating further tokens.

The API call returns a response object, often structured like JSON. This object contains the generated text (usually within a nested structure like `response.choices[0].message.content` for OpenAI), along with metadata such as the reason generation stopped (e.g., 'stop' sequence encountered or 'length' limit reached) and crucial **usage information** detailing the number of tokens consumed by the prompt and the completion (`response.usage`). Monitoring token usage is essential for managing API costs.

Robust applications must incorporate **error handling** around API calls. Network issues, invalid API keys, malformed requests, server errors, content filtering flags, or exceeding rate limits (Sec 29.6) can all cause the API call to fail. The client libraries typically raise specific exceptions (e.g., `openai.AuthenticationError`, `openai.RateLimitError`, `openai.APIError`) that should be caught using `try...except` blocks to allow for retries, logging, or graceful failure reporting.

Using LLM APIs provides powerful "AI-as-a-service," enabling access to state-of-the-art models without local infrastructure burdens. The key steps involve secure key management, using the provider's client library, structuring prompts appropriately (often as message lists), configuring generation parameters, making the API call, parsing the response, and implementing robust error handling. This programmatic access is the foundation for building customized LLM-powered tools and workflows.

*(Code Example 1 from the previous response for Section 29.1 already demonstrates this workflow using the `openai` library and can be reused here.)*
```python
# --- Code Example 1: Basic OpenAI API Call (Conceptual - Requires API Key) ---
# Note: Requires openai library: pip install openai
# Requires setting OPENAI_API_KEY environment variable. DO NOT HARDCODE YOUR KEY.

import os
try:
    # Using the v1.0+ OpenAI library structure
    from openai import OpenAI, OpenAIError 
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping API example.")

print("Conceptual OpenAI API call using the 'openai' library:")

if openai_installed:
    # Step 1 & 3: API Key (Assumed set as environment variable)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Error: OPENAI_API_KEY environment variable not set.")
        # Exit or handle gracefully
        client = None 
    else:
         # Step 4: Initialize Client
         # The client automatically uses the OPENAI_API_KEY environment variable
         client = OpenAI()
         print("OpenAI client initialized (using API key from environment).")

    if client:
        # Step 5: Construct Request (Messages for Chat model)
        prompt_text = "Explain the concept of redshift in astrophysics in simple terms."
        messages = [
            {"role": "system", "content": "You are a helpful assistant explaining astrophysics."},
            {"role": "user", "content": prompt_text}
        ]
        print(f"\nSending prompt to model: '{prompt_text}'")

        # Step 6 & 7: Make API Call and Handle Response
        try:
            print("Making API call to OpenAI...")
            response = client.chat.completions.create(
                model="gpt-3.5-turbo", # Choose appropriate model
                messages=messages,
                max_tokens=150,      # Limit response length
                temperature=0.7,     # Controls randomness
                n=1                  # Number of choices/responses to generate
            )
            
            # Extract the response content
            generated_text = response.choices[0].message.content.strip()
            print("\nLLM Response:")
            print(generated_text)
            
            # Print usage information (tokens used - important for cost)
            usage = response.usage
            print(f"\nAPI Usage: Prompt Tokens={usage.prompt_tokens}, Completion Tokens={usage.completion_tokens}, Total Tokens={usage.total_tokens}")

        # Step 8: Error Handling
        except OpenAIError as e:
            print(f"\nAn OpenAI API error occurred: {e}")
        except Exception as e:
            print(f"\nAn unexpected error occurred: {e}")
    else:
         print("\nCannot proceed without OpenAI client initialization.")
else:
    print("\nSkipping execution as 'openai' library is not installed.")

print("-" * 20)
```

**29.2 Prompt Engineering Techniques**

The effectiveness of Large Language Models is not solely determined by the model itself but is critically dependent on the quality and structure of the **prompt** provided as input. **Prompt engineering** is the rapidly evolving practice of designing prompts that effectively guide an LLM to produce the desired output for a specific task. It combines elements of instruction design, providing context, structuring information, and sometimes including examples, transforming it into an essential skill for anyone interacting with LLMs programmatically or conversationally. Good prompts lead to more accurate, relevant, reliable, and correctly formatted responses.

At its core, a prompt should clearly communicate the **task** the LLM is expected to perform. Vague requests yield vague or unpredictable results. Instead of "Tell me about FITS files," a better prompt would be "Explain the purpose of the Primary HDU in a FITS file for someone familiar with basic astronomy data." Explicit instructions are key: "Summarize the following abstract in exactly two sentences," "Translate this Python code to IDL," "Generate a Python function that takes X and Y and returns Z," "Classify the following observing log entry into one of these categories: [Weather, Instrument, Target, Calibration]." Defining the expected **output format** is also crucial, especially for programmatic use: "Return the result as a JSON object with keys 'parameter' and 'value'," "List the steps as a numbered list," "Provide only the Python code, no explanation."

Providing sufficient **context** is vital for LLMs to generate relevant responses, particularly for tasks requiring specific knowledge not guaranteed to be perfectly captured in their general pre-training data. For question answering based on a document, the relevant text passages *must* be included in the prompt (as in RAG, Sec 29.3). For code generation or debugging, providing surrounding code snippets, variable definitions, or error messages gives the model essential context. When asking for summaries or interpretations, providing the source text or data summary within the prompt grounds the model's response. The more relevant context provided, the less the model has to rely on potentially faulty internal "memory."

One of the most powerful prompt engineering techniques is **few-shot prompting**. Instead of just describing the task, you provide the LLM with several **examples** (typically 1 to 5, hence "few-shot") of the desired input-output behavior directly within the prompt, *before* giving the actual input you want processed. This "in-context learning" allows the model to infer the pattern, format, and style required for the task much more effectively than instructions alone. For example, if extracting parameters from text, providing 2-3 examples of input text and the corresponding desired JSON output significantly increases the likelihood that the model will produce correctly formatted JSON for the final input text. Few-shot prompting is particularly effective for structured output generation and tasks requiring a specific style or format.

The **structure** of the prompt itself matters. Using clear separators (like triple backticks ``` for code or text blocks), headings (like `Context:`, `Question:`, `Example Input:`, `Example Output:`, `Your Task:`), bullet points, or numbered lists can help the LLM parse the prompt correctly and distinguish between instructions, examples, context, and the final query. A well-organized prompt reduces ambiguity and focuses the model on the desired task. Starting the prompt by clearly defining the **role** or **persona** the LLM should adopt (e.g., "You are an expert Python programmer assisting with Astropy code," "You are a science communicator explaining complex topics simply") can also help guide the tone and content of the response.

**Iterative refinement** is central to effective prompt engineering. It's rare to get the perfect prompt on the first try. The typical process involves:
1.  Start with a reasonably clear prompt for the task.
2.  Send it to the LLM and examine the output.
3.  Identify shortcomings (e.g., incorrect information, wrong format, missing details, wrong tone).
4.  Modify the prompt to address these issues: clarify instructions, add more context, provide better examples, explicitly tell the model what *not* to do, adjust formatting requests.
5.  Repeat steps 2-4 until the LLM consistently produces outputs that meet the requirements.
This iterative cycle of prompting, evaluating, and refining is key to developing robust prompts for specific applications.

For complex tasks, **breaking them down** into simpler sub-tasks, each with its own focused prompt, can be more effective than trying to achieve everything with one large, convoluted prompt. For example, instead of asking an LLM to read a paper, extract key results, summarize them, and format them into a report section all at once, one might use separate prompts for: (1) Identifying key sections, (2) Summarizing each relevant section, (3) Extracting specific numerical results, and (4) Synthesizing the extracted information into the final report format. Chaining these focused prompts (potentially managed by frameworks like LangChain) can often yield more reliable and controllable results.

Finally, adjusting API parameters like `temperature` complements prompt engineering. For tasks requiring factual accuracy or specific formats (code generation, data extraction, specific summarization), using a low temperature (e.g., 0.0-0.3) makes the output more deterministic and focused. For more creative tasks like brainstorming hypotheses or generating varied textual descriptions, a higher temperature (e.g., 0.7-1.0) might be desirable, but increases the risk of less predictable or factual output.

*(Code Example 1 from previous response for Section 29.2 already illustrates different prompt structures - basic, clearer instruction, few-shot, structured - and can be reused here.)*
```python
# --- Code Example 1: Illustrating Different Prompt Structures ---

# --- Basic Prompt (Less Effective) ---
prompt_basic = "Convert 10 parsecs to light years."

# --- Better Prompt (Clearer Instruction) ---
prompt_clear = "Using astropy.units, write a Python snippet to convert the value 10 parsecs into light years and print the result."

# --- Few-Shot Prompt (for structured output) ---
prompt_few_shot = """
Extract the numerical value and unit from the string. Return as JSON.

String: "The distance is 5.2 kpc"
Output: {"value": 5.2, "unit": "kpc"}

String: "Flux measured: 101.3 mJy"
Output: {"value": 101.3, "unit": "mJy"}

String: "Rotation period found to be 3.5 days"
Output: {"value": 3.5, "unit": "days"}

String: "Temperature is approx 5800 K"
Output: 
""" # LLM completes the JSON for the last example

# --- Structured Prompt with Context and Instructions ---
context_paper = "Paper X found H0 = 72 +/- 3 km/s/Mpc. Paper Y found H0 = 69 +/- 2 km/s/Mpc using a different method."
prompt_structured = f"""
You are an astrophysics assistant comparing results.
Context: {context_paper}
Task: Briefly state the H0 value and uncertainty reported by Paper X and Paper Y. Are the results consistent within their uncertainties? Explain briefly.
Output Format: Plain text paragraph.
"""

print("--- Example Prompts ---")
print("Basic:", prompt_basic)
print("\nClearer:", prompt_clear)
print("\nFew-Shot:", prompt_few_shot)
print("\nStructured:", prompt_structured)
print("-" * 20)
```

Mastering prompt engineering is less about knowing specific LLM internals and more about developing skills in clear communication, logical structuring of requests, providing good examples, and iteratively refining instructions based on observed model behavior. It is a crucial skill for effectively harnessing the capabilities of LLMs for specific scientific tasks within the astrocomputing workflow.

**29.3 Retrieval-Augmented Generation (RAG)**

A major limitation of standard Large Language Models is their reliance on the knowledge encoded (often implicitly and sometimes incorrectly) within their parameters during pre-training, coupled with their inability to access real-time or domain-specific information beyond that training data, leading to potential hallucinations and outdated responses (Sec 26.6). **Retrieval-Augmented Generation (RAG)** is a powerful architectural pattern designed specifically to address these limitations by dynamically grounding LLM responses in information retrieved from external, authoritative knowledge sources at the time of query processing. RAG significantly enhances the factual accuracy, relevance, and trustworthiness of LLM outputs, making it a crucial technique for building reliable LLM-powered tools for scientific applications.

The core idea behind RAG is simple yet effective: **retrieve relevant information first, then prompt the LLM to generate its response based on that retrieved information.** Instead of asking the LLM to answer a question based solely on its internal knowledge (which might be flawed or outdated), the RAG system first finds relevant text passages or data from a trusted external knowledge base and provides these passages as context within the prompt given to the LLM. The LLM is explicitly instructed to use this provided context to formulate its answer.

The typical RAG pipeline involves several key stages:
1.  **Indexing (Offline):** A corpus of relevant documents (the external knowledge base, e.g., instrument manuals, specific research papers, project documentation, database content) is processed. Documents are often split into smaller, manageable chunks (paragraphs, sections). **Embeddings** (dense vector representations capturing semantic meaning, Sec 26.2) are generated for each chunk using a suitable text embedding model (e.g., from `sentence-transformers`). These embeddings, along with the original text chunks and metadata, are stored in a **vector store** or **vector database** (like FAISS, ChromaDB, Pinecone, Weaviate), which allows for efficient similarity searching.
2.  **Retrieval (Online):** When a user submits a query, the query text is also embedded using the *same* embedding model. The system then queries the vector store to find the document chunks whose embeddings are most semantically similar (e.g., closest cosine similarity or Euclidean distance) to the query embedding. These top-ranked, relevant chunks are retrieved. Some systems might combine semantic retrieval with traditional keyword search for robustness.
3.  **Augmentation (Prompt Construction):** The retrieved document chunks are formatted and combined with the original user query into a single prompt for the generative LLM. Careful **prompt engineering** (Sec 29.2) is used here, typically instructing the LLM: "Based *only* on the following context documents, answer the user's question. If the answer is not found in the context, say 'I don't know based on the provided information.' Context: [Retrieved Chunk 1] [Retrieved Chunk 2] ... Question: [Original User Question]".
4.  **Generation:** This augmented prompt is sent to a powerful generative LLM (e.g., GPT-4, Claude, Llama-2-chat via API or local hosting). The LLM processes the combined input and generates a response that should be primarily based on the information present in the retrieved context chunks.
5.  **(Optional) Citation/Verification:** Advanced RAG systems may include mechanisms to link parts of the generated answer back to the specific source chunks used, allowing the user to easily verify the information's origin and accuracy.

The benefits of RAG are significant for scientific applications:
*   **Reduces Hallucinations:** By grounding the LLM in specific, retrieved text, RAG dramatically reduces the likelihood of the model generating factually incorrect or fabricated information. It answers based on provided evidence rather than just internal statistical patterns.
*   **Access to Up-to-Date/Domain-Specific Knowledge:** RAG allows LLMs to effectively utilize information *not* present in their original training data, as long as that information exists in the indexed knowledge base. By indexing recent papers, project documentation, or specific databases, the system can provide answers based on current or highly specialized information.
*   **Improved Transparency and Verifiability:** Knowing that the answer is based on specific retrieved documents (and potentially citing them) makes the LLM's output much more verifiable and trustworthy compared to opaque responses from a model relying solely on its internal knowledge.
*   **Customization:** RAG allows tailoring the knowledge base to specific needs â€“ using only approved instrument manuals, project-internal documents, or a curated set of research papers â€“ ensuring responses are relevant to a specific domain or task.

Building RAG systems involves integrating several components: document loaders, text splitters, embedding models, vector stores, retrievers, and LLMs. While this requires more setup than simple API calls, frameworks like **LangChain** and **LlamaIndex** provide high-level abstractions and tools in Python specifically designed to simplify the construction and orchestration of RAG pipelines. They offer integrations with various document loaders (PDF, web, Notion, etc.), text splitters, embedding models (OpenAI, Hugging Face), vector stores (FAISS, ChromaDB, etc.), and LLM providers, allowing developers to build sophisticated RAG applications more easily.

```python
# --- Code Example: Conceptual RAG Implementation Idea using LangChain ---
# Note: Highly conceptual structure. Requires multiple libraries and setup.
# pip install langchain langchain-openai langchain-community sentence-transformers faiss-cpu pypdf

print("Conceptual RAG Structure using LangChain:")

try:
    # --- 1. Indexing Stage (Done once or periodically) ---
    # from langchain_community.document_loaders import PyPDFLoader
    # from langchain.text_splitter import RecursiveCharacterTextSplitter
    # from langchain_community.vectorstores import FAISS
    # from langchain_openai import OpenAIEmbeddings # Or HuggingFaceEmbeddings

    # print("\n1. INDEXING (Conceptual):")
    # print("  - Load documents (e.g., loader = PyPDFLoader('manual.pdf'))")
    # # documents = loader.load()
    # print("  - Split documents into chunks (e.g., text_splitter = ...) ")
    # # chunks = text_splitter.split_documents(documents)
    # print("  - Create embeddings (e.g., embeddings = OpenAIEmbeddings())")
    # print("  - Build & save vector store (e.g., vector_store = FAISS.from_documents(chunks, embeddings))")
    # # vector_store.save_local("my_faiss_index") 
    print("  (Indexing involves loading, chunking, embedding, storing in vector DB)")

    # --- 2. Retrieval & Generation Stage (Done per query) ---
    # from langchain_openai import ChatOpenAI
    # from langchain.chains import RetrievalQA
    # from langchain.prompts import PromptTemplate
    # from langchain_community.vectorstores import FAISS # Load index
    # from langchain_openai import OpenAIEmbeddings # Use same embedding model

    print("\n2. RETRIEVAL & GENERATION (Conceptual):")
    # print("  - Load embedding model (embeddings = ...)")
    # print("  - Load saved vector store (vector_store = FAISS.load_local('my_faiss_index', embeddings, allow_dangerous_deserialization=True))") # Security flag often needed
    # print("  - Create retriever (retriever = vector_store.as_retriever())")
    # print("  - Define LLM (llm = ChatOpenAI(model_name='gpt-3.5-turbo'))")
    # print("  - Define Prompt Template (instructing use of context)")
    # template = "Answer based only on context:\nContext: {context}\nQuestion: {question}\nAnswer:"
    # prompt_template = PromptTemplate.from_template(template)
    # print("  - Create RetrievalQA chain")
    # qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type_kwargs={"prompt": prompt_template})
    
    question = "What is the UVIS pixel scale?"
    print(f"\n  User Question: '{question}'")
    print("  Invoking QA chain (conceptual)...")
    # result = qa_chain.invoke({"query": question}) # LangChain v0.1+ uses invoke
    # print(f"  Generated Answer: {result['result']}")
    print("  (RAG chain would retrieve relevant chunks, augment prompt, call LLM)")
    print("  (Result ideally grounded in retrieved handbook text)")

    langchain_components_available = True # Placeholder
except ImportError as e:
    print(f"\nRequired LangChain components not installed ({e}). Cannot fully illustrate.")
    langchain_components_available = False
except Exception as e:
     print(f"\nAn error occurred setting up conceptual RAG: {e}")
     langchain_components_available = False

if not langchain_components_available:
    print("\nSkipping RAG conceptual workflow execution.")

print("-" * 20)

# Explanation: This conceptual example outlines the RAG process using LangChain components.
# 1. Indexing (Offline): Conceptually describes loading documents (e.g., PDF manual), 
#    splitting into chunks, generating embeddings (using OpenAI or Hugging Face models), 
#    and storing them in a FAISS vector store, saving the index locally.
# 2. Retrieval & Generation (Per Query): 
#    a. Conceptually loads the embedding model and the saved FAISS index.
#    b. Creates a `retriever` from the vector store.
#    c. Defines the generative LLM (`ChatOpenAI`).
#    d. Creates a `PromptTemplate` instructing the LLM to use the provided context.
#    e. Creates a `RetrievalQA` chain, which orchestrates the process: when invoked 
#       with a `question`, it uses the `retriever` to find relevant document chunks, 
#       inserts them into the `prompt_template`, sends the augmented prompt to the 
#       `llm`, and returns the LLM's generated answer.
# This illustrates how frameworks abstract the complex retrieve-then-generate logic.
```

RAG represents a significant advancement in making LLMs more reliable and useful for knowledge-intensive tasks. By explicitly grounding responses in external, verifiable information sources, it addresses key limitations like outdated knowledge and hallucination, making LLMs safer and more applicable for scientific use cases like querying documentation, summarizing recent literature based on retrieved abstracts, or answering specific questions about project data described in internal reports. Building effective RAG systems requires careful design of the indexing, retrieval, and prompting stages.

**29.4 Example: Building a Chatbot for FITS Keywords**

Let's apply the principles discussed to build a concrete, albeit simple, tool: a Python function acting as a basic chatbot that explains standard FITS header keywords. This tool will leverage an LLM API (Sec 29.1), utilize prompt engineering (Sec 29.2), and incorporate a rudimentary form of Retrieval-Augmented Generation (RAG) (Sec 29.3) by using a predefined dictionary of keyword definitions as context to improve accuracy and grounding.

**Goal:** Create a Python function `explain_fits_keyword(keyword)` that takes a standard FITS keyword string (e.g., 'BITPIX', 'NAXIS', 'OBJECT') as input and returns a clear, concise explanation suitable for an astronomer, generated by an LLM but grounded by known definitions where available.

**Knowledge Base (Context Source):** We'll define a simple Python dictionary `fits_keyword_definitions` mapping common uppercase FITS keywords to their standard brief definitions or purpose. This serves as our local, trusted knowledge base for the RAG component. For keywords not found in this dictionary, the tool will rely on the LLM's general knowledge, clearly indicating this.

**LLM Interaction:** We will use the `openai` library to interact with a chat model like `gpt-3.5-turbo` or `gpt-4o`. The interaction will involve constructing a specific prompt for each query.

**Prompt Design:** The prompt needs to instruct the LLM on its role, the specific keyword to explain, and provide the retrieved definition (if found) as context. It should ask for a concise explanation relevant to astronomy. Example structure: "You are an expert FITS specialist. Explain the FITS keyword '{KEYWORD}'. [Optional: Standard definition: '{DEFINITION}'. Please elaborate on this in an astronomical context.] Keep the explanation brief (2-3 sentences)."

**Workflow Implementation:**
1.  Define the `fits_keyword_definitions` dictionary.
2.  Define the `explain_fits_keyword(keyword)` function.
3.  Inside the function, initialize the OpenAI client (checking for API key).
4.  Normalize the input `keyword` (e.g., to uppercase).
5.  Look up the normalized keyword in the `fits_keyword_definitions` dictionary (RAG retrieval step).
6.  Construct the prompt based on whether a definition was found, incorporating the definition as context if available.
7.  Prepare the message list for the chat API.
8.  Call `client.chat.completions.create()` with the messages, model name, and parameters (e.g., `max_tokens`, low `temperature` for factual explanation).
9.  Extract the generated explanation from the API response.
10. Include error handling for API calls.
11. Return the explanation (or an error message).

This approach combines the LLM's ability to generate fluent text with grounding from a predefined knowledge base for standard terms, aiming for more reliable explanations than relying solely on the LLM's internal memory. The tool clearly separates cases where it uses provided context versus general knowledge.

**(Code Example identical to Application 29.A from the previous response, which implemented this specific chatbot example, can be used here.)**
```python
# --- Code Example: FITS Keyword Explainer Chatbot Function ---
# Note: Requires openai library and API key environment variable.

import os
import time # For potential delays between calls
try:
    from openai import OpenAI, OpenAIError
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping chatbot example.")

# Step 2: Rudimentary Knowledge Base (Context Source)
fits_keyword_definitions = {
    'SIMPLE': 'Logical keyword. If T (True), file conforms to basic FITS standard.',
    'BITPIX': 'Integer keyword specifying data type of array pixels (e.g., 8, 16, 32, -32, -64).',
    'NAXIS': 'Integer keyword. Number of axes in the data array (0 for no data).',
    'NAXISn': 'Integer keyword. Length of data axis n (n=1 to NAXIS).',
    'EXTEND': 'Logical keyword in Primary HDU. If T, file may contain extensions.',
    'OBJECT': 'String keyword. Name/identifier of the observed object or target field.',
    'EXPTIME': 'Floating-point keyword. Exposure time, typically in seconds.',
    'DATE-OBS': 'String keyword. Date (or Date/Time) of the start of observation (UTC typically).',
    'INSTRUME': 'String keyword. Name of the instrument used.',
    'TELESCOP': 'String keyword. Name of the telescope used.',
    'BUNIT': 'String keyword. Physical units of the data array values (e.g., "adu", "Jy/pixel", "erg/s/cm2/A").',
    'BSCALE': 'Floating-point keyword. Multiplicative factor for scaling array values (Default: 1.0). True_Value = BZERO + BSCALE * Array_Value.',
    'BZERO': 'Floating-point keyword. Zero offset for scaling array values (Default: 0.0). True_Value = BZERO + BSCALE * Array_Value.',
    'CTYPEn': 'String keyword. Coordinate type and projection for axis n (e.g., "RA---TAN", "WAVE", "FREQ").',
    'CRVALn': 'Floating-point keyword. World coordinate value at the reference pixel for axis n.',
    'CRPIXn': 'Floating-point keyword. Pixel coordinate (1-based) of the reference pixel for axis n.',
    'CDELTn': 'Floating-point keyword. Coordinate increment per pixel at reference pixel for axis n (used if no CD/PC matrix).',
    'CUNITn': 'String keyword. Units of the world coordinate value for axis n (e.g., "deg", "Angstrom", "Hz").',
    'EQUINOX': 'Floating-point keyword. Equinox in years for celestial coordinate systems (e.g., 2000.0 for FK5 J2000).',
    'RADESYS': 'String keyword. Name of the celestial coordinate reference frame (e.g., "ICRS", "FK5").',
    'HISTORY': 'Keyword used for free-form text providing history/processing information.',
    'COMMENT': 'Keyword used for free-form explanatory comments.',
    'END': 'Mandatory keyword marking the physical end of a FITS header unit.'
    # Add more common keywords...
}

# Initialize OpenAI client (requires API key in environment)
client = None
if openai_installed:
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        client = OpenAI() # Automatically uses OPENAI_API_KEY from env
    else:
        print("Warning: OPENAI_API_KEY not set. Explainer will not function.")

# Step 3 & 4: Main Function with Prompting and API Call
def explain_fits_keyword_app(keyword):
    """Asks an LLM to explain a FITS keyword, providing known context (RAG)."""
    
    if not client:
        return "Error: OpenAI client not initialized (check API key)."
        
    keyword_upper = keyword.upper().replace('N', 'n') # Handle NAXISn, CTYPEn etc. case roughly
    # More robust handling would use regex or specific checks for keywords ending in n
    
    # Step 5: Retrieve known definition (RAG retrieval)
    known_definition = fits_keyword_definitions.get(keyword_upper, None)
    # Try base name if 'n' version not found
    if not known_definition and keyword_upper[-1].isdigit():
         base_keyword = keyword_upper[:-1] + 'n'
         known_definition = fits_keyword_definitions.get(base_keyword, None)
         
    # Step 6: Construct prompt
    prompt = "You are an expert astronomical data analyst explaining FITS keywords.\n\n"
    prompt += f"Please explain the meaning and typical usage of the FITS keyword '{keyword.upper()}' "
    prompt += "in astrophysics data analysis.\n"
    if known_definition:
        prompt += f"\nContext: The standard definition or purpose is often described as: '{known_definition}'\n"
        prompt += "\nPlease elaborate on this definition based on the provided context, giving practical examples if possible."
    else:
        prompt += "\nProvide a clear explanation suitable for someone familiar with astronomy but maybe not all FITS details."
        prompt += " If it's not a standard keyword, please indicate that."
    prompt += "\nKeep the explanation concise (approx. 2-4 sentences)."

    print(f"\n--- Sending Prompt for '{keyword.upper()}' ---")
    # print(prompt) # Uncomment to see the full prompt sent

    # Step 7: Prepare messages for chat API
    messages = [
        {"role": "system", "content": "You are an expert astronomical data analyst explaining FITS keywords concisely."},
        {"role": "user", "content": prompt}
    ]

    # Step 8: Call LLM API
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # A capable and cost-effective choice
            messages=messages,
            max_tokens=150, # Limit response length
            temperature=0.3, # Lower temperature for more factual explanation
            n=1
        )
        # Step 9: Extract explanation
        explanation = response.choices[0].message.content.strip()
        # Optional: Add token usage info if needed: usage = response.usage
        return explanation
        
    # Step 10: Error Handling
    except OpenAIError as e:
        print(f"OpenAI API error for keyword '{keyword}': {e}")
        return f"Error generating explanation (API Error): {e}"
    except Exception as e:
        print(f"Unexpected error for keyword '{keyword}': {e}")
        return f"Unexpected error: {e}"

# --- Example Usage ---
if openai_installed and client:
    keywords_to_explain = ['BITPIX', 'OBJECT', 'CRVAL1', 'HISTORY', 'MY_CUSTOM_KEY']
    for key in keywords_to_explain:
        explanation = explain_fits_keyword_app(key)
        print(f"\nExplanation for '{key.upper()}':")
        print(explanation)
        time.sleep(1) # Be polite to API endpoint
else:
    print("\nCannot run example usage as OpenAI library/key is missing.")
    
print("-" * 20)

```

**Application 29.B: arXiv Abstract Summarizer Tool**

**(Paragraph 1)** **Objective:** This application builds upon Application 26.A to create a more functional tool that retrieves recent arXiv preprints for a given query and uses an LLM API to generate a concise summary for each, demonstrating a practical integration of data retrieval and LLM-based processing. Reinforces Sec 29.1, 29.2, 26.4.

**(Paragraph 2)** **Astrophysical Context:** Staying current with the daily flood of arXiv preprints is essential but time-consuming. A tool that automatically fetches papers relevant to a researcher's interests and provides a one-or-two sentence summary of each abstract can significantly improve efficiency in literature monitoring, allowing researchers to quickly identify papers needing closer attention.

**(Paragraph 3)** **Data Source:** The arXiv API, accessed programmatically to search for papers based on user-defined queries (keywords, categories, authors, dates). The key data retrieved are the paper title and abstract text.

**(Paragraph 4)** **Modules Used:** `arxiv` (for querying arXiv API), `openai` (or similar LLM API client library), `datetime`, `time` (for delays), `os` (for API key).

**(Paragraph 5)** **Technique Focus:** Combining external API interaction (arXiv search) with LLM API interaction (summarization). Structuring the workflow within a reusable Python function. Implementing error handling for both arXiv queries and LLM API calls. Using prompt engineering to request concise, focused summaries of scientific abstracts. Managing API keys and potential rate limits.

**(Paragraph 6)** **Processing Step 1: Function Definition and API Setup:** Define a function `summarize_arxiv_papers(arxiv_query, max_papers=5)` that takes the query string and maximum results as input. Inside, initialize the `arxiv` client and the `openai` client (checking for API key).

**(Paragraph 7)** **Processing Step 2: Query arXiv:** Use `arxiv.Search` to perform the query based on `arxiv_query` and `max_papers`, sorting by submission date. Iterate through results, extracting `entry_id`, `title`, and `summary` (abstract) for each paper, storing them (e.g., in a list of dictionaries). Include `try...except` for the arXiv query.

**(Paragraph 8)** **Processing Step 3: Loop and Summarize via LLM API:** Iterate through the list of retrieved papers. For each paper:
    *   Construct a prompt specifically asking the LLM to summarize the provided abstract text in 1-2 concise sentences, focusing on the key result or contribution. Example: `"Summarize the main finding of this abstract in 1-2 sentences: [Abstract Text]"`.
    *   Prepare the message list for the chat API.
    *   Call `client.chat.completions.create()` with the prompt, specifying a suitable model (e.g., `gpt-3.5-turbo`), `max_tokens` (e.g., 60-100), and a low `temperature` (e.g., 0.3-0.5) for factual summarization.
    *   Extract the summary from the response.
    *   Store the summary along with the paper's ID and title.
    *   Include error handling for the API call.
    *   Add a small delay (`time.sleep`) between API calls to respect potential rate limits.

**(Paragraph 9)** **Processing Step 4: Format and Return Results:** Collate the results into a structured format (e.g., a list of dictionaries, where each dictionary contains 'id', 'title', 'summary'). Return this list.

**(Paragraph 10)** **Processing Step 5: Main Execution Block:** In the main part of the script, define an example arXiv query. Call the `summarize_arxiv_papers` function. Loop through the returned list and print the title and summary for each paper, along with a disclaimer about verification.

**Output, Testing, and Extension:** The output is a formatted list showing recent paper titles and their corresponding LLM-generated summaries for the specified arXiv query. **Testing:** Verify the arXiv query works correctly. Check the quality and conciseness of the generated summaries against the original abstracts. Test with different queries and numbers of results. Check error handling for invalid queries or API issues. **Extensions:** (1) Add command-line arguments (`argparse`) to allow users to specify the query and number of results. (2) Save the summaries to a file or database. (3) Implement filtering based on abstract content *before* summarization (e.g., only summarize papers mentioning specific methods or objects). (4) Compare summaries from different LLM models or prompts. (5) Integrate with email or a web service to provide daily/weekly summary reports.

```python
# --- Code Example: Application 29.B ---
# Note: Requires openai, arxiv libraries and API key environment variable.
# Performs actual network requests. Be mindful of API costs/rate limits.

import os
import time
import arxiv
from datetime import datetime, timedelta # For potential date filtering
try:
    from openai import OpenAI, OpenAIError
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping application.")

# Initialize OpenAI client (requires API key in environment)
client = None
if openai_installed:
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        client = OpenAI() 
    else:
        print("Warning: OPENAI_API_KEY not set. Summarizer will not function.")

# Main function combining arXiv query and LLM summarization
def summarize_arxiv_papers(arxiv_query, max_papers=3, model_name="gpt-3.5-turbo"):
    """Queries arXiv for recent papers and summarizes their abstracts using an LLM."""
    
    if not client:
        print("Error: OpenAI client not initialized.")
        return []
    if arxiv is None:
         print("Error: arxiv library not found.")
         return []
        
    print(f"\nSearching arXiv for '{arxiv_query}' (max {max_papers} recent papers)...")
    papers = []
    try:
        search = arxiv.Search(
            query=arxiv_query,
            max_results=max_papers,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        # Use a new client for each function call? Or reuse a global one?
        # Reusing global is fine if script isn't long-running server.
        arxiv_client = arxiv.Client(page_size=min(max_papers, 100), delay_seconds=2, num_retries=2) 
        
        results_retrieved = 0
        for result in arxiv_client.results(search):
            papers.append({
                'id': result.entry_id.split('/')[-1],
                'title': result.title.replace('\n',' ').strip(),
                'abstract': result.summary.replace('\n',' ').strip(),
                'published': result.published.date()
            })
            results_retrieved += 1
            if results_retrieved >= max_papers: break # Enforce max_results strictly
        print(f"Retrieved {len(papers)} abstracts.")
        
    except Exception as e:
        print(f"Error querying arXiv: {e}")
        return [] # Return empty list on error

    results_list = []
    if papers:
        print(f"\nGenerating summaries using LLM ({model_name})...")
        for i, paper in enumerate(papers):
            print(f"  Summarizing paper {i+1}/{len(papers)} ({paper['id']})...")
            
            # Step 3: Construct Summarization Prompt
            prompt = f"""Summarize the following astrophysical abstract in one or two 
            very concise sentences, focusing only on the primary result or conclusion.

            Abstract: "{paper['abstract']}"

            Concise Summary:
            """
            
            messages = [{"role": "user", "content": prompt}]

            # Step 4: Call LLM API
            try:
                response = client.chat.completions.create(
                    model=model_name, 
                    messages=messages,
                    max_tokens=100, # Adjust as needed
                    temperature=0.3 # Lower temp for factual summary
                )
                summary = response.choices[0].message.content.strip()
                results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': summary})
                
            except OpenAIError as e:
                print(f"    OpenAI API error for {paper['id']}: {e}")
                results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': f"(API Error)"})
            except Exception as e:
                 print(f"    Unexpected error during summarization for {paper['id']}: {e}")
                 results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': "(Summarization Error)"})
            
            # Step 8: Add delay between API calls
            time.sleep(1.0) # Be polite to API endpoint, avoid rate limits

    # Step 5: Return results
    return results_list

# --- Example Usage ---
if openai_installed and client and arxiv is not None:
    # Example query: recent extragalactic papers mentioning JWST
    query = '(cat:astro-ph.GA OR cat:astro-ph.CO) AND (abs:JWST OR ti:JWST)'
    num_summaries = 3
    
    summaries_list = summarize_arxiv_papers(query, max_papers=num_summaries)
    
    print("\n--- Recent arXiv Summaries ---")
    print("** Disclaimer: Summaries are AI-generated and require verification. **")
    if summaries_list:
        for item in summaries_list:
            print(f"\nID: {item['id']}")
            print(f"Title: {item['title']}")
            print(f"Summary: {item['summary']}")
    else:
        print("No summaries generated or retrieved.")
        
else:
    print("\nCannot run example usage as required libraries or API key are missing.")

print("-" * 20)
```

**Chapter 29 Summary**

This chapter provided practical guidance on building simple tools incorporating Large Language Models (LLMs) for astrophysical tasks, focusing primarily on using LLM APIs from Python. It detailed the typical workflow for interacting with APIs like OpenAI's, covering API key management (emphasizing security), using client libraries (e.g., `openai`), structuring requests (particularly the message list format for chat models), making API calls (`client.chat.completions.create`), configuring generation parameters (`model`, `max_tokens`, `temperature`), parsing responses, and implementing robust error handling. Significant emphasis was placed on **prompt engineering**, outlining techniques for crafting effective prompts through clear instructions, providing context, using few-shot examples to guide format and logic, structuring the prompt logically, potentially setting a persona, breaking down complex tasks, and iterating based on model outputs.

The chapter then explored **Retrieval-Augmented Generation (RAG)** as a key technique to improve the factual grounding and reliability of LLM responses. The RAG process â€“ indexing a knowledge base (e.g., documents, manuals), retrieving relevant chunks based on a user query (often using semantic search on embeddings via vector stores like FAISS or ChromaDB), augmenting the LLM prompt with this retrieved context, and generating an answer based on that context â€“ was explained conceptually, often facilitated by frameworks like LangChain or LlamaIndex. Two concrete application examples were developed: a **FITS keyword explainer** using an LLM API augmented by a predefined dictionary of keyword definitions (simple RAG), and an **arXiv abstract summarizer** combining arXiv API queries (via the `arxiv` library) with LLM API calls for automated summarization. Finally, crucial practical considerations for using LLM APIs were discussed, namely the need to monitor and manage **costs** (often based on token usage), respect provider **rate limits** (implementing delays or backoff strategies), and account for network and model **latency**.

---

**References for Further Reading (APA Format, 7th Edition):**

1.  **OpenAI. (n.d.).** *OpenAI API Documentation*. OpenAI. Retrieved January 16, 2024, from [https://platform.openai.com/docs](https://platform.openai.com/docs) (Includes Prompt Engineering Guide, API Reference, Cookbooks).
    *(Official documentation essential for using the OpenAI API (Sec 29.1), understanding prompt engineering best practices (Sec 29.2), and checking pricing/rate limits (Sec 29.6).)*

2.  **Lewis, P., et al. (2020).** Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. *(See reference in Chapter 26)*.
    *(The RAG paper provides the foundation for the techniques discussed in Sec 29.3 and applied conceptually in Application 29.A.)*

3.  **LangChain Developers. (n.d.).** *LangChain Documentation*. LangChain. Retrieved January 16, 2024, from [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction) (See also LlamaIndex Documentation: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/))
    *(Documentation for popular frameworks designed to build LLM applications, particularly RAG systems, providing tools for indexing, retrieval, prompting, and chaining relevant to Sec 29.3.)*

4.  **White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., ... & Schmidt, D. C. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with Large Language Models*. arXiv preprint arXiv:2302.11382. [https://arxiv.org/abs/2302.11382](https://arxiv.org/abs/2302.11382)
    *(Provides examples and categorizes various prompt engineering patterns and techniques, offering practical strategies relevant to Sec 29.2.)*

5.  **arXiv. (n.d.).** *arXiv API User Manual*. arXiv.org. Retrieved January 16, 2024, from [https://info.arxiv.org/help/api/index.html](https://info.arxiv.org/help/api/index.html) (See also `arxiv` Python library: [https://github.com/lukasschwab/arxiv.py](https://github.com/lukasschwab/arxiv.py))
    *(Documentation for the arXiv API used for programmatic data retrieval in Application 29.B. The linked Python library simplifies interaction.)*
