**Chapter 29: Building Simple LLM-Powered Astro Tools**

Having explored the potential applications and inherent limitations of Large Language Models (LLMs) for literature navigation, code assistance, and data interpretation, this chapter focuses on the practical steps involved in **building simple tools** that leverage these capabilities within an astrophysical context using Python. We move beyond conceptual discussions to demonstrate how to programmatically interact with LLM services, primarily focusing on using **LLM Application Programming Interfaces (APIs)**, such as the one provided by OpenAI for its GPT models. We will cover the basics of setting up API access, making requests, and handling responses. A significant focus will be placed on **prompt engineering** â€“ the art and science of crafting effective prompts to elicit desired and reliable outputs from LLMs for specific tasks like summarization, code generation, or information extraction relevant to astronomy. We will then introduce the powerful **Retrieval-Augmented Generation (RAG)** technique in more practical detail, outlining how combining LLM generation with information retrieved from a specific knowledge base (like local documentation or databases) can significantly improve factual accuracy and reduce hallucinations. This will be illustrated through two concrete examples: building a simple Python tool to answer questions about FITS header keywords by grounding the LLM in keyword descriptions, and creating a utility to automatically summarize recent arXiv astro-ph postings using API calls. Finally, practical considerations such as API costs, rate limits, and response latency will be discussed.

**29.1 Using LLM APIs (e.g., OpenAI API)**

The most common way to integrate the capabilities of powerful, state-of-the-art Large Language Models into custom applications or research workflows is through **Application Programming Interfaces (APIs)** offered by the model providers. Companies like OpenAI (GPT models), Anthropic (Claude models), Google (Gemini models), Cohere, and others provide web APIs that allow developers to send input prompts and receive generated text outputs from their hosted models without needing to manage the massive infrastructure required to run these models locally. This API-based access makes advanced AI capabilities widely accessible.

Interacting with these APIs from Python is typically facilitated by official **client libraries** provided by the vendor. For instance, OpenAI offers the `openai` library (`pip install openai`), Anthropic has the `anthropic` library, and Google provides client libraries within its Cloud ecosystem. These libraries abstract away the low-level details of handling HTTP requests, authentication headers, and JSON parsing, providing a more Pythonic interface to the underlying web service. Using these libraries is the standard practice for programmatic LLM interaction via APIs.

The first prerequisite for using a commercial LLM API is obtaining an **API key**. This usually involves creating an account on the provider's platform (e.g., OpenAI platform, Google Cloud Console) and generating a unique key associated with your account. This key serves both for **authentication** (proving you are authorized to use the service) and **billing** (tracking your usage, as most APIs charge based on the amount of text processed). API keys are highly sensitive credentials; they should be treated like passwords and **never** hardcoded directly into source code, especially if the code will be shared or committed to version control. Standard secure practices include storing the key in an environment variable (e.g., `OPENAI_API_KEY`) that the client library can automatically detect, using a dedicated secrets management tool, or loading it from a configuration file with restricted permissions.

Once the client library is installed and authentication is configured (often just by setting the environment variable), the typical workflow involves initializing a client object (e.g., `client = OpenAI()`). This object then provides methods for interacting with different API endpoints. For modern chat-based models like GPT-3.5, GPT-4, or Claude 3, the interaction usually centers around a "chat completion" endpoint.

Preparing the request for a chat completion API typically involves constructing a **list of messages** representing the conversation history. This list usually contains dictionaries, each specifying a `role` (`'system'`, `'user'`, or `'assistant'`) and the corresponding `content` (the text).
*   The `'system'` message (usually the first one) provides high-level instructions or sets the context for the AI's persona (e.g., "You are a helpful astrophysics research assistant.").
*   `'user'` messages contain the prompts, questions, or instructions from the human user.
*   `'assistant'` messages contain the previous responses generated by the LLM itself, providing conversational context for subsequent turns.
For a simple, single-turn query, the list might just contain a system message and a single user message with the prompt.

The API call itself is then made using a method like `client.chat.completions.create(...)` (in the `openai` v1.0+ library). Besides the target `model` identifier (e.g., `"gpt-4o"`, `"gpt-3.5-turbo"`, `"claude-3-opus-20240229"`) and the `messages` list, several key parameters control the generation process:
*   `max_tokens`: Limits the maximum number of tokens (subword units, roughly related to word count) in the generated response. Important for controlling output length and cost.
*   `temperature`: A value typically between 0 and 2 that controls the randomness of the output. Lower values (e.g., 0.0-0.3) make the output more deterministic and focused (good for factual tasks, code generation). Higher values (e.g., 0.7-1.0) increase randomness, leading to more diverse and "creative" responses but also potentially less coherent or factual ones.
*   `top_p` (nucleus sampling): An alternative parameter to temperature for controlling randomness, selecting from the smallest set of tokens whose cumulative probability exceeds `p`.
*   `n`: Number of different completions to generate for the same prompt.
*   `stop`: Sequences of characters where the API should stop generating further tokens.

The API call returns a response object, often structured like JSON. This object contains the generated text (usually within a nested structure like `response.choices[0].message.content` for OpenAI), along with metadata such as the reason generation stopped (e.g., 'stop' sequence encountered or 'length' limit reached) and crucial **usage information** detailing the number of tokens consumed by the prompt and the completion (`response.usage`). Monitoring token usage is essential for managing API costs.

Robust applications must incorporate **error handling** around API calls. Network issues, invalid API keys, malformed requests, server errors, content filtering flags, or exceeding rate limits (Sec 29.6) can all cause the API call to fail. The client libraries typically raise specific exceptions (e.g., `openai.AuthenticationError`, `openai.RateLimitError`, `openai.APIError`) that should be caught using `try...except` blocks to allow for retries, logging, or graceful failure reporting.

Using LLM APIs provides powerful "AI-as-a-service," enabling access to state-of-the-art models without local infrastructure burdens. The key steps involve secure key management, using the provider's client library, structuring prompts appropriately (often as message lists), configuring generation parameters, making the API call, parsing the response, and implementing robust error handling. This programmatic access is the foundation for building customized LLM-powered tools and workflows.

*(Code Example 1 from the previous response for Section 29.1 already demonstrates this workflow using the `openai` library and can be reused here.)*
```python
# --- Code Example 1: Basic OpenAI API Call (Conceptual - Requires API Key) ---
# Note: Requires openai library: pip install openai
# Requires setting OPENAI_API_KEY environment variable. DO NOT HARDCODE YOUR KEY.

import os
try:
    # Using the v1.0+ OpenAI library structure
    from openai import OpenAI, OpenAIError 
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping API example.")

print("Conceptual OpenAI API call using the 'openai' library:")

if openai_installed:
    # Step 1 & 3: API Key (Assumed set as environment variable)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Error: OPENAI_API_KEY environment variable not set.")
        # Exit or handle gracefully
        client = None 
    else:
         # Step 4: Initialize Client
         # The client automatically uses the OPENAI_API_KEY environment variable
         client = OpenAI()
         print("OpenAI client initialized (using API key from environment).")

    if client:
        # Step 5: Construct Request (Messages for Chat model)
        prompt_text = "Explain the concept of redshift in astrophysics in simple terms."
        messages = [
            {"role": "system", "content": "You are a helpful assistant explaining astrophysics."},
            {"role": "user", "content": prompt_text}
        ]
        print(f"\nSending prompt to model: '{prompt_text}'")

        # Step 6 & 7: Make API Call and Handle Response
        try:
            print("Making API call to OpenAI...")
            response = client.chat.completions.create(
                model="gpt-3.5-turbo", # Choose appropriate model
                messages=messages,
                max_tokens=150,      # Limit response length
                temperature=0.7,     # Controls randomness
                n=1                  # Number of choices/responses to generate
            )
            
            # Extract the response content
            generated_text = response.choices[0].message.content.strip()
            print("\nLLM Response:")
            print(generated_text)
            
            # Print usage information (tokens used - important for cost)
            usage = response.usage
            print(f"\nAPI Usage: Prompt Tokens={usage.prompt_tokens}, Completion Tokens={usage.completion_tokens}, Total Tokens={usage.total_tokens}")

        # Step 8: Error Handling
        except OpenAIError as e:
            print(f"\nAn OpenAI API error occurred: {e}")
        except Exception as e:
            print(f"\nAn unexpected error occurred: {e}")
    else:
         print("\nCannot proceed without OpenAI client initialization.")
else:
    print("\nSkipping execution as 'openai' library is not installed.")

print("-" * 20)
```

**29.2 Prompt Engineering Techniques**

The effectiveness of Large Language Models is not solely determined by the model itself but is critically dependent on the quality and structure of the **prompt** provided as input. **Prompt engineering** is the rapidly evolving practice of designing prompts that effectively guide an LLM to produce the desired output for a specific task. It combines elements of instruction design, providing context, structuring information, and sometimes including examples, transforming it into an essential skill for anyone interacting with LLMs programmatically or conversationally. Good prompts lead to more accurate, relevant, reliable, and correctly formatted responses.

At its core, a prompt should clearly communicate the **task** the LLM is expected to perform. Vague requests yield vague or unpredictable results. Instead of "Tell me about FITS files," a better prompt would be "Explain the purpose of the Primary HDU in a FITS file for someone familiar with basic astronomy data." Explicit instructions are key: "Summarize the following abstract in exactly two sentences," "Translate this Python code to IDL," "Generate a Python function that takes X and Y and returns Z," "Classify the following observing log entry into one of these categories: [Weather, Instrument, Target, Calibration]." Defining the expected **output format** is also crucial, especially for programmatic use: "Return the result as a JSON object with keys 'parameter' and 'value'," "List the steps as a numbered list," "Provide only the Python code, no explanation."

Providing sufficient **context** is vital for LLMs to generate relevant responses, particularly for tasks requiring specific knowledge not guaranteed to be perfectly captured in their general pre-training data. For question answering based on a document, the relevant text passages *must* be included in the prompt (as in RAG, Sec 29.3). For code generation or debugging, providing surrounding code snippets, variable definitions, or error messages gives the model essential context. When asking for summaries or interpretations, providing the source text or data summary within the prompt grounds the model's response. The more relevant context provided, the less the model has to rely on potentially faulty internal "memory."

One of the most powerful prompt engineering techniques is **few-shot prompting**. Instead of just describing the task, you provide the LLM with several **examples** (typically 1 to 5, hence "few-shot") of the desired input-output behavior directly within the prompt, *before* giving the actual input you want processed. This "in-context learning" allows the model to infer the pattern, format, and style required for the task much more effectively than instructions alone. For example, if extracting parameters from text, providing 2-3 examples of input text and the corresponding desired JSON output significantly increases the likelihood that the model will produce correctly formatted JSON for the final input text. Few-shot prompting is particularly effective for structured output generation and tasks requiring a specific style or format.

The **structure** of the prompt itself matters. Using clear separators (like triple backticks ``` for code or text blocks), headings (like `Context:`, `Question:`, `Example Input:`, `Example Output:`, `Your Task:`), bullet points, or numbered lists can help the LLM parse the prompt correctly and distinguish between instructions, examples, context, and the final query. A well-organized prompt reduces ambiguity and focuses the model on the desired task. Starting the prompt by clearly defining the **role** or **persona** the LLM should adopt (e.g., "You are an expert Python programmer assisting with Astropy code," "You are a science communicator explaining complex topics simply") can also help guide the tone and content of the response.

**Iterative refinement** is central to effective prompt engineering. It's rare to get the perfect prompt on the first try. The typical process involves:
1.  Start with a reasonably clear prompt for the task.
2.  Send it to the LLM and examine the output.
3.  Identify shortcomings (e.g., incorrect information, wrong format, missing details, wrong tone).
4.  Modify the prompt to address these issues: clarify instructions, add more context, provide better examples, explicitly tell the model what *not* to do, adjust formatting requests.
5.  Repeat steps 2-4 until the LLM consistently produces outputs that meet the requirements.
This iterative cycle of prompting, evaluating, and refining is key to developing robust prompts for specific applications.

For complex tasks, **breaking them down** into simpler sub-tasks, each with its own focused prompt, can be more effective than trying to achieve everything with one large, convoluted prompt. For example, instead of asking an LLM to read a paper, extract key results, summarize them, and format them into a report section all at once, one might use separate prompts for: (1) Identifying key sections, (2) Summarizing each relevant section, (3) Extracting specific numerical results, and (4) Synthesizing the extracted information into the final report format. Chaining these focused prompts (potentially managed by frameworks like LangChain) can often yield more reliable and controllable results.

Finally, adjusting API parameters like `temperature` complements prompt engineering. For tasks requiring factual accuracy or specific formats (code generation, data extraction, specific summarization), using a low temperature (e.g., 0.0-0.3) makes the output more deterministic and focused. For more creative tasks like brainstorming hypotheses or generating varied textual descriptions, a higher temperature (e.g., 0.7-1.0) might be desirable, but increases the risk of less predictable or factual output.

*(Code Example 1 from previous response for Section 29.2 already illustrates different prompt structures - basic, clearer instruction, few-shot, structured - and can be reused here.)*
```python
# --- Code Example 1: Illustrating Different Prompt Structures ---

# --- Basic Prompt (Less Effective) ---
prompt_basic = "Convert 10 parsecs to light years."

# --- Better Prompt (Clearer Instruction) ---
prompt_clear = "Using astropy.units, write a Python snippet to convert the value 10 parsecs into light years and print the result."

# --- Few-Shot Prompt (for structured output) ---
prompt_few_shot = """
Extract the numerical value and unit from the string. Return as JSON.

String: "The distance is 5.2 kpc"
Output: {"value": 5.2, "unit": "kpc"}

String: "Flux measured: 101.3 mJy"
Output: {"value": 101.3, "unit": "mJy"}

String: "Rotation period found to be 3.5 days"
Output: {"value": 3.5, "unit": "days"}

String: "Temperature is approx 5800 K"
Output: 
""" # LLM completes the JSON for the last example

# --- Structured Prompt with Context and Instructions ---
context_paper = "Paper X found H0 = 72 +/- 3 km/s/Mpc. Paper Y found H0 = 69 +/- 2 km/s/Mpc using a different method."
prompt_structured = f"""
You are an astrophysics assistant comparing results.
Context: {context_paper}
Task: Briefly state the H0 value and uncertainty reported by Paper X and Paper Y. Are the results consistent within their uncertainties? Explain briefly.
Output Format: Plain text paragraph.
"""

print("--- Example Prompts ---")
print("Basic:", prompt_basic)
print("\nClearer:", prompt_clear)
print("\nFew-Shot:", prompt_few_shot)
print("\nStructured:", prompt_structured)
print("-" * 20)
```

Mastering prompt engineering is less about knowing specific LLM internals and more about developing skills in clear communication, logical structuring of requests, providing good examples, and iteratively refining instructions based on observed model behavior. It is a crucial skill for effectively harnessing the capabilities of LLMs for specific scientific tasks within the astrocomputing workflow.

**29.3 Retrieval-Augmented Generation (RAG)**

A major limitation of standard Large Language Models is their reliance on the knowledge encoded (often implicitly and sometimes incorrectly) within their parameters during pre-training, coupled with their inability to access real-time or domain-specific information beyond that training data, leading to potential hallucinations and outdated responses (Sec 26.6). **Retrieval-Augmented Generation (RAG)** is a powerful architectural pattern designed specifically to address these limitations by dynamically grounding LLM responses in information retrieved from external, authoritative knowledge sources at the time of query processing. RAG significantly enhances the factual accuracy, relevance, and trustworthiness of LLM outputs, making it a crucial technique for building reliable LLM-powered tools for scientific applications.

The core idea behind RAG is simple yet effective: **retrieve relevant information first, then prompt the LLM to generate its response based on that retrieved information.** Instead of asking the LLM to answer a question based solely on its internal knowledge (which might be flawed or outdated), the RAG system first finds relevant text passages or data from a trusted external knowledge base and provides these passages as context within the prompt given to the LLM. The LLM is explicitly instructed to use this provided context to formulate its answer.

The typical RAG pipeline involves several key stages:
1.  **Indexing (Offline):** A corpus of relevant documents (the external knowledge base, e.g., instrument manuals, specific research papers, project documentation, database content) is processed. Documents are often split into smaller, manageable chunks (paragraphs, sections). **Embeddings** (dense vector representations capturing semantic meaning, Sec 26.2) are generated for each chunk using a suitable text embedding model (e.g., from `sentence-transformers`). These embeddings, along with the original text chunks and metadata, are stored in a **vector store** or **vector database** (like FAISS, ChromaDB, Pinecone, Weaviate), which allows for efficient similarity searching.
2.  **Retrieval (Online):** When a user submits a query, the query text is also embedded using the *same* embedding model. The system then queries the vector store to find the document chunks whose embeddings are most semantically similar (e.g., closest cosine similarity or Euclidean distance) to the query embedding. These top-ranked, relevant chunks are retrieved. Some systems might combine semantic retrieval with traditional keyword search for robustness.
3.  **Augmentation (Prompt Construction):** The retrieved document chunks are formatted and combined with the original user query into a single prompt for the generative LLM. Careful **prompt engineering** (Sec 29.2) is used here, typically instructing the LLM: "Based *only* on the following context documents, answer the user's question. If the answer is not found in the context, say 'I don't know based on the provided information.' Context: [Retrieved Chunk 1] [Retrieved Chunk 2] ... Question: [Original User Question]".
4.  **Generation:** This augmented prompt is sent to a powerful generative LLM (e.g., GPT-4, Claude, Llama-2-chat via API or local hosting). The LLM processes the combined input and generates a response that should be primarily based on the information present in the retrieved context chunks.
5.  **(Optional) Citation/Verification:** Advanced RAG systems may include mechanisms to link parts of the generated answer back to the specific source chunks used, allowing the user to easily verify the information's origin and accuracy.

The benefits of RAG are significant for scientific applications:
*   **Reduces Hallucinations:** By grounding the LLM in specific, retrieved text, RAG dramatically reduces the likelihood of the model generating factually incorrect or fabricated information. It answers based on provided evidence rather than just internal statistical patterns.
*   **Access to Up-to-Date/Domain-Specific Knowledge:** RAG allows LLMs to effectively utilize information *not* present in their original training data, as long as that information exists in the indexed knowledge base. By indexing recent papers, project documentation, or specific databases, the system can provide answers based on current or highly specialized information.
*   **Improved Transparency and Verifiability:** Knowing that the answer is based on specific retrieved documents (and potentially citing them) makes the LLM's output much more verifiable and trustworthy compared to opaque responses from a model relying solely on its internal knowledge.
*   **Customization:** RAG allows tailoring the knowledge base to specific needs â€“ using only approved instrument manuals, project-internal documents, or a curated set of research papers â€“ ensuring responses are relevant to a specific domain or task.

Building RAG systems involves integrating several components: document loaders, text splitters, embedding models, vector stores, retrievers, and LLMs. While this requires more setup than simple API calls, frameworks like **LangChain** and **LlamaIndex** provide high-level abstractions and tools in Python specifically designed to simplify the construction and orchestration of RAG pipelines. They offer integrations with various document loaders (PDF, web, Notion, etc.), text splitters, embedding models (OpenAI, Hugging Face), vector stores (FAISS, ChromaDB, etc.), and LLM providers, allowing developers to build sophisticated RAG applications more easily.

```python
# --- Code Example: Conceptual RAG Implementation Idea using LangChain ---
# Note: Highly conceptual structure. Requires multiple libraries and setup.
# pip install langchain langchain-openai langchain-community sentence-transformers faiss-cpu pypdf

print("Conceptual RAG Structure using LangChain:")

try:
    # --- 1. Indexing Stage (Done once or periodically) ---
    # from langchain_community.document_loaders import PyPDFLoader
    # from langchain.text_splitter import RecursiveCharacterTextSplitter
    # from langchain_community.vectorstores import FAISS
    # from langchain_openai import OpenAIEmbeddings # Or HuggingFaceEmbeddings

    # print("\n1. INDEXING (Conceptual):")
    # print("  - Load documents (e.g., loader = PyPDFLoader('manual.pdf'))")
    # # documents = loader.load()
    # print("  - Split documents into chunks (e.g., text_splitter = ...) ")
    # # chunks = text_splitter.split_documents(documents)
    # print("  - Create embeddings (e.g., embeddings = OpenAIEmbeddings())")
    # print("  - Build & save vector store (e.g., vector_store = FAISS.from_documents(chunks, embeddings))")
    # # vector_store.save_local("my_faiss_index") 
    print("  (Indexing involves loading, chunking, embedding, storing in vector DB)")

    # --- 2. Retrieval & Generation Stage (Done per query) ---
    # from langchain_openai import ChatOpenAI
    # from langchain.chains import RetrievalQA
    # from langchain.prompts import PromptTemplate
    # from langchain_community.vectorstores import FAISS # Load index
    # from langchain_openai import OpenAIEmbeddings # Use same embedding model

    print("\n2. RETRIEVAL & GENERATION (Conceptual):")
    # print("  - Load embedding model (embeddings = ...)")
    # print("  - Load saved vector store (vector_store = FAISS.load_local('my_faiss_index', embeddings, allow_dangerous_deserialization=True))") # Security flag often needed
    # print("  - Create retriever (retriever = vector_store.as_retriever())")
    # print("  - Define LLM (llm = ChatOpenAI(model_name='gpt-3.5-turbo'))")
    # print("  - Define Prompt Template (instructing use of context)")
    # template = "Answer based only on context:\nContext: {context}\nQuestion: {question}\nAnswer:"
    # prompt_template = PromptTemplate.from_template(template)
    # print("  - Create RetrievalQA chain")
    # qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type_kwargs={"prompt": prompt_template})
    
    question = "What is the UVIS pixel scale?"
    print(f"\n  User Question: '{question}'")
    print("  Invoking QA chain (conceptual)...")
    # result = qa_chain.invoke({"query": question}) # LangChain v0.1+ uses invoke
    # print(f"  Generated Answer: {result['result']}")
    print("  (RAG chain would retrieve relevant chunks, augment prompt, call LLM)")
    print("  (Result ideally grounded in retrieved handbook text)")

    langchain_components_available = True # Placeholder
except ImportError as e:
    print(f"\nRequired LangChain components not installed ({e}). Cannot fully illustrate.")
    langchain_components_available = False
except Exception as e:
     print(f"\nAn error occurred setting up conceptual RAG: {e}")
     langchain_components_available = False

if not langchain_components_available:
    print("\nSkipping RAG conceptual workflow execution.")

print("-" * 20)

# Explanation: This conceptual example outlines the RAG process using LangChain components.
# 1. Indexing (Offline): Conceptually describes loading documents (e.g., PDF manual), 
#    splitting into chunks, generating embeddings (using OpenAI or Hugging Face models), 
#    and storing them in a FAISS vector store, saving the index locally.
# 2. Retrieval & Generation (Per Query): 
#    a. Conceptually loads the embedding model and the saved FAISS index.
#    b. Creates a `retriever` from the vector store.
#    c. Defines the generative LLM (`ChatOpenAI`).
#    d. Creates a `PromptTemplate` instructing the LLM to use the provided context.
#    e. Creates a `RetrievalQA` chain, which orchestrates the process: when invoked 
#       with a `question`, it uses the `retriever` to find relevant document chunks, 
#       inserts them into the `prompt_template`, sends the augmented prompt to the 
#       `llm`, and returns the LLM's generated answer.
# This illustrates how frameworks abstract the complex retrieve-then-generate logic.
```

RAG represents a significant advancement in making LLMs more reliable and useful for knowledge-intensive tasks. By explicitly grounding responses in external, verifiable information sources, it addresses key limitations like outdated knowledge and hallucination, making LLMs safer and more applicable for scientific use cases like querying documentation, summarizing recent literature based on retrieved abstracts, or answering specific questions about project data described in internal reports. Building effective RAG systems requires careful design of the indexing, retrieval, and prompting stages.


You are absolutely right! Section 29.4 was presented as an "Example" section in the previous prompts, but it should be formatted consistently as "Application 29.A" following the detailed structure used for all other applications. My apologies for that inconsistency.

Let's reformat Section 29.4 (FITS Keyword Explainer) into Application 29.A with the standard 10-paragraph structure and detailed breakdown.

---

**Application 29.A: Building a Chatbot for FITS Keywords**

**(Paragraph 1)** **Objective:** This application demonstrates building a simple, practical LLM-powered tool using Python, specifically a function that acts as a basic chatbot to explain standard FITS header keywords. It leverages programmatic interaction with an LLM API (Sec 29.1), applies prompt engineering techniques (Sec 29.2), and incorporates a rudimentary form of Retrieval-Augmented Generation (RAG) (Sec 29.3) by providing known keyword definitions as context to enhance the accuracy and reliability of the generated explanations.

**(Paragraph 2)** **Astrophysical Context:** FITS files, the standard for astronomical data, rely heavily on header keywords (like `NAXIS`, `BITPIX`, `EXPTIME`, `CTYPE1`, `BUNIT`) to describe the data's structure, calibration, and observational context (Sec 1.3, 1.5). While experienced astronomers become familiar with common keywords, researchers new to FITS, or encountering less common keywords specific to certain instruments or data products, often need to look up their meaning. A tool that can provide quick, context-aware explanations of these keywords directly within a Python environment would be a valuable utility, saving time spent searching through documentation or the FITS standard itself.

**(Paragraph 3)** **Data Source:** The primary "data source" is the knowledge embedded within the LLM about FITS keywords (learned during pre-training) augmented by a curated, local **knowledge base**. For this application, the knowledge base is a predefined Python dictionary (`fits_keyword_definitions`) mapping common, standard FITS keywords (in uppercase) to their brief, official, or generally accepted definitions. This dictionary provides the grounding "truth" for the RAG aspect. The input from the user is the `keyword` string they want explained.

**(Paragraph 4)** **Modules Used:**
*   `openai`: The Python client library for interacting with OpenAI's LLM APIs (like GPT-3.5 or GPT-4). Requires installation (`pip install openai`) and API key setup. (Or equivalent library for another LLM provider).
*   `os`: To access the API key securely from an environment variable.
*   `time`: Optionally, to add delays between consecutive API calls if building a tool that might be called repeatedly.
*   (No astronomical data libraries like `astropy.io.fits` are strictly needed for the core *explanation* function itself, only for potentially *getting* the keyword from a file in a larger workflow).

**(Paragraph 5)** **Technique Focus:** This application integrates several key concepts from the chapter:
    *   **API Interaction (Sec 29.1):** Using the `openai` library to initialize a client, construct a request (`messages` list), call the chat completion endpoint (`client.chat.completions.create`), and handle the response.
    *   **Prompt Engineering (Sec 29.2):** Designing a structured prompt that clearly defines the LLM's role ("expert astronomical data analyst"), states the task (explain keyword X), provides context (the retrieved definition, if available), and specifies desired output constraints (concise, relevant to astronomers).
    *   **Retrieval-Augmented Generation (RAG - Simple Form) (Sec 29.3):** Implementing the "Retrieve" step by looking up the keyword in the local `fits_keyword_definitions` dictionary. Implementing the "Augment" step by incorporating the retrieved definition into the prompt sent to the LLM. Implementing the "Generate" step via the API call. This grounds the explanation for known keywords.
    *   **Error Handling:** Including `try...except` blocks for potential API errors or issues during keyword lookup/prompting.

**(Paragraph 6)** **Processing Step 1: Define Knowledge Base:** Create the `fits_keyword_definitions` Python dictionary containing key-value pairs where keys are common uppercase FITS keywords (potentially including base forms like `NAXISn`) and values are their concise definitions. This should be populated with reliable information from the FITS standard documents or trusted resources.

**(Paragraph 7)** **Processing Step 2: Define Explainer Function:** Create the main function `explain_fits_keyword_app(keyword)`. Inside this function:
    *   Check for and initialize the LLM API client (e.g., `client = OpenAI()`, assuming the API key is in the environment). Handle cases where the client cannot be initialized.
    *   Normalize the input `keyword` (e.g., convert to uppercase, handle potential 'n' suffixes like NAXISn).
    *   Perform the retrieval step: Use `.get()` on the `fits_keyword_definitions` dictionary to find a `known_definition` for the normalized keyword, defaulting to `None` if not found.

**(Paragraph 8)** **Processing Step 3: Construct Prompt:** Build the prompt string dynamically. Start with setting the persona ("You are an expert..."). State the task clearly ("Explain the FITS keyword 'X'...). If a `known_definition` was retrieved, include it explicitly labeled as context ("Context: Standard definition is '...'") and ask the LLM to elaborate on it within an astronomical context. If no definition was found, instruct the LLM to provide a general explanation suitable for astronomers and perhaps indicate if it's not a standard keyword. Add constraints on conciseness (e.g., "2-4 sentences").

**(Paragraph 9)** **Processing Step 4: Call LLM API and Handle Response:** Prepare the `messages` list for the chat API, including the system message and the constructed user prompt. Call `client.chat.completions.create(...)`, specifying the desired `model` (e.g., 'gpt-3.5-turbo'), the `messages`, a reasonable `max_tokens` limit, and a low `temperature` (e.g., 0.3) to encourage factual, less creative explanations. Wrap the call in a `try...except OpenAIError` block. Extract the generated text content from the response (e.g., `response.choices[0].message.content.strip()`).

**(Paragraph 10)** **Processing Step 5: Return Result:** Return the extracted explanation string. If an error occurred during the API call or setup, return an informative error message string instead. The calling script can then print or use this returned explanation.

**Output, Testing, and Extension:** The primary output is the textual explanation of the requested FITS keyword, generated by the LLM potentially using provided context. **Testing:** Test with various standard keywords included in the knowledge base (e.g., 'SIMPLE', 'BITPIX', 'EXPTIME', 'BUNIT', 'NAXIS1', 'HISTORY') and verify the explanation is accurate and elaborates reasonably on the definition. Test with keywords *not* in the knowledge base ('MYCUSTOMKEY', 'OBSNOTES') to see how the LLM handles unknown terms (it should ideally state it's non-standard or provide a plausible guess based on general knowledge). Test case sensitivity handling. **Extensions:** (1) Expand the `fits_keyword_definitions` dictionary significantly to cover more standard and common instrument-specific keywords. (2) Load keyword definitions from an external file (e.g., CSV or JSON) instead of hardcoding in the script. (3) Implement more sophisticated RAG: If a keyword isn't in the dictionary, perform a quick web search or query a FITS standard documentation source to retrieve context dynamically before prompting the LLM. (4) Add options to control the verbosity or target audience (e.g., beginner vs. expert) of the explanation via prompt modification. (5) Integrate this function into a larger script that reads a FITS header and allows the user to interactively ask for explanations of specific keywords found in that header.

```python
# --- Code Example: Application 29.A ---
# (Identical code to Application 29.A provided in the previous response, 
# implementing the FITS Keyword Explainer function `explain_fits_keyword_app`)

# Note: Requires openai library and API key environment variable.

import os
import time # For potential delays between calls
try:
    from openai import OpenAI, OpenAIError
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping chatbot example.")

# Step 1 & 2 (partially): Knowledge Base defined within function scope below implicitly
# Step 2: Rudimentary Knowledge Base (Context Source)
# (Defined again here for clarity within this application block)
fits_keyword_definitions = {
    'SIMPLE': 'Logical keyword. If T (True), file conforms to basic FITS standard.',
    'BITPIX': 'Integer keyword specifying data type of array pixels (e.g., 8, 16, 32, -32, -64).',
    'NAXIS': 'Integer keyword. Number of axes in the data array (0 for no data).',
    'NAXISn': 'Integer keyword. Length of data axis n (n=1 to NAXIS).',
    'EXTEND': 'Logical keyword in Primary HDU. If T, file may contain extensions.',
    'OBJECT': 'String keyword. Name/identifier of the observed object or target field.',
    'EXPTIME': 'Floating-point keyword. Exposure time, typically in seconds.',
    'DATE-OBS': 'String keyword. Date (or Date/Time) of the start of observation (UTC typically).',
    'INSTRUME': 'String keyword. Name of the instrument used.',
    'TELESCOP': 'String keyword. Name of the telescope used.',
    'BUNIT': 'String keyword. Physical units of the data array values (e.g., "adu", "Jy/pixel", "erg/s/cm2/A").',
    'BSCALE': 'Floating-point keyword. Multiplicative factor for scaling array values (Default: 1.0). True_Value = BZERO + BSCALE * Array_Value.',
    'BZERO': 'Floating-point keyword. Zero offset for scaling array values (Default: 0.0). True_Value = BZERO + BSCALE * Array_Value.',
    'CTYPEn': 'String keyword. Coordinate type and projection for axis n (e.g., "RA---TAN", "WAVE", "FREQ").',
    'CRVALn': 'Floating-point keyword. World coordinate value at the reference pixel for axis n.',
    'CRPIXn': 'Floating-point keyword. Pixel coordinate (1-based) of the reference pixel for axis n.',
    'CDELTn': 'Floating-point keyword. Coordinate increment per pixel at reference pixel for axis n (used if no CD/PC matrix).',
    'CUNITn': 'String keyword. Units of the world coordinate value for axis n (e.g., "deg", "Angstrom", "Hz").',
    'EQUINOX': 'Floating-point keyword. Equinox in years for celestial coordinate systems (e.g., 2000.0 for FK5 J2000).',
    'RADESYS': 'String keyword. Name of the celestial coordinate reference frame (e.g., "ICRS", "FK5").',
    'HISTORY': 'Keyword used for free-form text providing history/processing information.',
    'COMMENT': 'Keyword used for free-form explanatory comments.',
    'END': 'Mandatory keyword marking the physical end of a FITS header unit.'
}

# Initialize OpenAI client (requires API key in environment)
client = None
if openai_installed:
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        client = OpenAI() # Automatically uses OPENAI_API_KEY from env
    else:
        print("Warning: OPENAI_API_KEY not set. Explainer will not function.")

# Step 2-5: Main Function definition
def explain_fits_keyword_app(keyword):
    """Asks an LLM to explain a FITS keyword, providing known context (RAG)."""
    
    if not client:
        return "Error: OpenAI client not initialized (check API key)."
        
    keyword_upper = keyword.upper()
    # Handle NAXISn, CTYPEn etc. by checking if last char is digit
    # More robust matching might be needed for all cases (e.g., CDi_j)
    base_keyword = keyword_upper
    if keyword_upper[-1].isdigit():
         base_keyword_n = keyword_upper[:-1] + 'n'
         if base_keyword_n in fits_keyword_definitions:
              base_keyword = base_keyword_n # Use generic 'NAXISn' definition if specific NAXIS1 not found
              
    # Retrieve known definition (RAG retrieval)
    known_definition = fits_keyword_definitions.get(keyword_upper, None)
    if not known_definition and base_keyword != keyword_upper:
         known_definition = fits_keyword_definitions.get(base_keyword, None)

    # Construct prompt
    prompt = f"You are an expert astronomical data analyst explaining FITS keywords concisely.\n\n"
    prompt += f"Please explain the meaning and typical usage of the FITS keyword '{keyword.upper()}' in astronomy.\n"
    if known_definition:
        prompt += f"\nContext: The standard definition or purpose is often described as: '{known_definition}'\n"
        prompt += "\nPlease elaborate briefly on this definition based ONLY on the provided context and general FITS knowledge relevant to astronomers."
    else:
        prompt += "\nProvide a clear explanation suitable for someone familiar with astronomy but maybe not all FITS details."
        prompt += " If this is likely not a standard FITS keyword, please indicate that."
    prompt += "\nKeep the explanation concise (approx 2-4 sentences)."

    print(f"\n--- Sending Prompt for '{keyword.upper()}' ---")
    # print(f"DEBUG: Using definition: {known_definition}") # For debugging RAG part
    # print(prompt) # Uncomment to see the full prompt sent

    messages = [
        {"role": "system", "content": "You are an expert astronomical data analyst providing concise FITS keyword explanations."},
        {"role": "user", "content": prompt}
    ]

    # Call LLM API
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", 
            messages=messages,
            max_tokens=150, 
            temperature=0.2, # Low temperature for factual explanation
            n=1
        )
        explanation = response.choices[0].message.content.strip()
        return explanation
        
    # Error Handling
    except OpenAIError as e:
        print(f"OpenAI API error for keyword '{keyword}': {e}")
        return f"Error generating explanation (API Error): {e}"
    except Exception as e:
        print(f"Unexpected error for keyword '{keyword}': {e}")
        return f"Unexpected error: {e}"

# --- Example Usage ---
if openai_installed and client:
    keywords_to_test = ['SIMPLE', 'BUNIT', 'NAXIS3', 'HISTORY', 'FILTER_A']
    for key in keywords_to_test:
        explanation_result = explain_fits_keyword_app(key)
        print(f"\nExplanation for '{key.upper()}':")
        print(explanation_result)
        time.sleep(0.5) # Be slightly polite to API endpoint
else:
    print("\nCannot run example usage as OpenAI library/key is missing.")
    
print("-" * 20)
```

**Application 29.B: arXiv Abstract Summarizer Tool**

**(Paragraph 1)** **Objective:** This application builds a functional Python tool that combines external data retrieval from the arXiv preprint server with LLM processing via an API to provide automated summaries of recent research abstracts for a user-specified query. It demonstrates a complete, practical workflow involving API interaction (arXiv and LLM), prompt engineering for summarization, and handling lists of documents. Reinforces Sec 29.1, 29.2, 26.4.

**(Paragraph 2)** **Astrophysical Context:** As highlighted previously (App 26.A), keeping up with the daily postings on arXiv (`astro-ph`) is a common challenge. A tool that can automatically fetch the latest papers matching a researcher's interests (defined by keywords or categories) and provide a very short summary of each abstract can significantly aid in quickly scanning new literature and identifying papers that require more detailed reading. This moves beyond manual browsing to automated information filtering.

**(Paragraph 3)** **Data Source:** The arXiv API (programmatic interface to the arXiv preprint server). The API allows searching for papers based on query strings that can include keywords, author names, subject categories (like `cat:astro-ph.GA`), dates, etc. The primary data retrieved are the metadata (entry ID, title, authors, publication date) and the abstract text for each matching paper.

**(Paragraph 4)** **Modules Used:**
*   `arxiv`: A Python wrapper library (`pip install arxiv`) that simplifies querying the arXiv API. Alternatively, `requests` and `xml.etree.ElementTree` or `feedparser` could be used to interact with the raw API feed.
*   `openai` (or similar): Python client library for the chosen LLM API provider (e.g., OpenAI). Requires API key setup.
*   `datetime`, `timedelta`: For potentially specifying date ranges in arXiv queries.
*   `time`: To introduce delays between LLM API calls.
*   `os`: For API key access.

**(Paragraph 5)** **Technique Focus:** Orchestrating a multi-step workflow involving two different external APIs within a Python function. (1) Using the `arxiv` library to perform targeted searches based on a query string and retrieve structured metadata and abstract text. (2) Implementing a loop to process multiple retrieved abstracts. (3) Inside the loop, performing **prompt engineering** specifically for abstractive summarization, requesting concise output focused on key findings. (4) Calling an LLM API (e.g., `openai`) for each abstract using the summarization prompt. (5) Handling potential errors gracefully during both arXiv query and LLM API calls. (6) Collating and returning the results (e.g., title + summary pairs).

**(Paragraph 6)** **Processing Step 1: Define Function and Initialize Clients:** Create a Python function, e.g., `summarize_recent_arxiv(arxiv_query, max_papers=5, summary_model="gpt-3.5-turbo")`. Inside, perform necessary imports and initialize the `arxiv.Client` and the LLM API client (e.g., `openai.Client`), including checks for API key availability.

**(Paragraph 7)** **Processing Step 2: Query arXiv:** Construct an `arxiv.Search` object using the input `arxiv_query`, desired `max_papers`, and sorting by `SubmittedDate`. Execute the search using `client.results(search)`. Iterate through the results, extracting and storing relevant information (e.g., `result.entry_id`, `result.title`, `result.summary`) into a list. Include error handling for the arXiv query phase.

**(Paragraph 8)** **Processing Step 3: Loop and Call LLM for Summarization:** Iterate through the list of retrieved papers. For each paper's abstract:
    *   Construct a clear summarization prompt (similar to App 26.A or the example below), perhaps explicitly mentioning it's an abstract and asking for the core result. Example: `"Provide a 1-2 sentence summary highlighting the main result or conclusion of this astrophysical abstract: [Abstract Text]"`.
    *   Prepare the message list for the chat API.
    *   Call the LLM API (`client.chat.completions.create`) with the appropriate model, messages, `max_tokens` suitable for a short summary (e.g., 60-100), and a low `temperature` (e.g., 0.2-0.5) to encourage factual summarization.
    *   Wrap the API call in a `try...except` block to handle potential errors for individual summaries without stopping the whole process.
    *   Extract the generated summary text.
    *   Add a short `time.sleep()` (e.g., 0.5-1 second) to avoid hitting API rate limits if processing many abstracts.

**(Paragraph 9)** **Processing Step 4: Collate and Return Results:** Store the generated summary along with the paper's title and ID for each successfully processed abstract. Return the final list of results (e.g., list of dictionaries).

**(Paragraph 10)** **Processing Step 5: Execute and Display:** In the main part of the script, define an example arXiv query string (e.g., focusing on a specific topic like 'exoplanet atmospheres AND cat:astro-ph.EP'). Call the `summarize_recent_arxiv` function. Iterate through the returned list and print the paper title and its generated summary. Include the standard disclaimer about AI generation and verification.

**Output, Testing, and Extension:** The output is a formatted list showing titles and LLM-generated summaries for recent arXiv papers matching the query. **Testing:** Verify the arXiv query returns expected papers. Critically evaluate the quality, accuracy, and conciseness of several generated summaries against their original abstracts. Test with different query terms and different `max_papers`. Check that error handling works (e.g., if arXiv query fails or LLM API fails for one abstract). **Extensions:** (1) Add options to filter by date range explicitly. (2) Allow user to choose the summarization model via function argument. (3) Save results to a file (CSV, JSON) or email them. (4) Implement basic keyword highlighting in the summaries or original abstracts based on the input query terms. (5) Use a more advanced RAG approach where abstracts are first embedded and clustered, and then representative abstracts or cluster centroids are summarized.

```python
# --- Code Example: Application 29.B ---
# (Identical code to Application 29.B provided in the previous response, 
# implementing the arXiv summarizer function `summarize_arxiv_papers`)

# Note: Requires openai, arxiv libraries and API key environment variable.
# Performs actual network requests. Be mindful of API costs/rate limits.

import os
import time
import arxiv # Needs: pip install arxiv
try:
    from openai import OpenAI, OpenAIError
    openai_installed = True
except ImportError:
    openai_installed = False
    print("NOTE: 'openai' library not installed or outdated. Skipping application.")

# Initialize OpenAI client (requires API key in environment)
client = None
if openai_installed:
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        client = OpenAI() 
    else:
        print("Warning: OPENAI_API_KEY not set. Summarizer will not function.")

# Main function combining arXiv query and LLM summarization
def summarize_arxiv_papers(arxiv_query, max_papers=3, model_name="gpt-3.5-turbo"):
    """Queries arXiv for recent papers and summarizes their abstracts using an LLM."""
    
    # Check prerequisites
    if not client:
        print("Error: OpenAI client not initialized.")
        return []
    if arxiv is None: # Check if arxiv import succeeded (if conditional import used)
         print("Error: arxiv library not found or failed to import.")
         return []
        
    print(f"\nSearching arXiv for '{arxiv_query}' (max {max_papers} recent papers)...")
    papers = []
    try:
        # Step 2: Query arXiv
        search = arxiv.Search(
            query=arxiv_query,
            max_results=max_papers,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        # Increase page_size slightly just in case, limit later
        arxiv_client = arxiv.Client(page_size=max_papers + 5, delay_seconds=1.5, num_retries=2) 
        
        results_retrieved = 0
        for result in arxiv_client.results(search):
            # Sometimes abstracts can be None or empty, skip those
            if result.summary: 
                papers.append({
                    'id': result.entry_id.split('/')[-1],
                    'title': result.title.replace('\n',' ').strip(),
                    'abstract': result.summary.replace('\n',' ').strip(),
                    'published': result.published.date()
                })
                results_retrieved += 1
                if results_retrieved >= max_papers: # Strictly enforce max_papers
                     break 
            else:
                 print(f"  Skipping paper {result.entry_id} due to empty summary.")
        print(f"Retrieved {len(papers)} abstracts with content.")
        
    except Exception as e:
        print(f"Error querying arXiv: {e}")
        return [] # Return empty list on error

    # Step 3 & 4: Loop, Prompt, Summarize via LLM API
    results_list = []
    if papers:
        print(f"\nGenerating summaries using LLM ({model_name})...")
        for i, paper in enumerate(papers):
            print(f"  Summarizing paper {i+1}/{len(papers)} ({paper['id']})...")
            
            # Construct Prompt
            prompt = f"""Please provide a very concise (1-2 sentence) summary of the main 
            finding or conclusion described in the following astrophysical abstract. Focus 
            on the core result.

            Abstract: "{paper['abstract']}"

            Concise Summary:"""
            
            messages = [{"role": "user", "content": prompt}]

            # Call LLM API
            try:
                response = client.chat.completions.create(
                    model=model_name, 
                    messages=messages,
                    max_tokens=100, # Limit summary length
                    temperature=0.3, # Encourage factual summary
                    n=1
                )
                summary = response.choices[0].message.content.strip()
                # Basic cleaning of common LLM preamble/postamble if needed
                summary = summary.replace("Summary:", "").strip()
                results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': summary})
                
            except OpenAIError as e:
                print(f"    OpenAI API error for {paper['id']}: {e}")
                results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': f"(API Error)"})
            except Exception as e:
                 print(f"    Unexpected error during summarization for {paper['id']}: {e}")
                 results_list.append({'id': paper['id'], 'title': paper['title'], 'summary': "(Summarization Error)"})
            
            # Add delay between API calls
            time.sleep(1.0) # Adjust delay as needed based on rate limits

    # Step 5: Return results
    return results_list

# --- Example Usage ---
if openai_installed and client and arxiv is not None:
    # Example query: recent extragalactic papers mentioning JWST
    query = '(cat:astro-ph.GA OR cat:astro-ph.CO) AND (abs:JWST OR ti:JWST)'
    num_summaries = 3 # Limit number for example run
    
    summaries_list = summarize_arxiv_papers(query, max_papers=num_summaries)
    
    # Step 6: Display Results
    print("\n--- Recent arXiv Summaries ---")
    print("** Disclaimer: Summaries are AI-generated and require verification. **")
    if summaries_list:
        for item in summaries_list:
            print(f"\nID: {item['id']}")
            print(f"Title: {item['title']}")
            print(f"Summary: {item['summary']}")
    else:
        print("No summaries generated or retrieved.")
        
else:
    print("\nCannot run example usage as required libraries or API key are missing.")

print("-" * 20)
```

**Chapter 29 Summary**

This chapter transitioned from discussing LLM capabilities to demonstrating their practical implementation in building simple astrophysical tools using Python. The primary focus was on interacting with **Large Language Model (LLM) APIs**, particularly using client libraries like `openai`. The workflow involving API key management (emphasizing security), using the client library, structuring requests (often using message lists for chat models), making API calls (`client.chat.completions.create`), configuring generation parameters (`model`, `max_tokens`, `temperature`), parsing responses (extracting generated content, checking usage/tokens), and implementing robust error handling was detailed. Significant emphasis was placed on **prompt engineering** as a crucial skill for guiding LLMs, covering techniques like providing clear instructions, supplying relevant context, using few-shot examples to demonstrate desired output formats, structuring prompts logically, and iteratively refining prompts based on model outputs.

A key technique introduced for improving factual reliability was **Retrieval-Augmented Generation (RAG)**. The RAG workflow â€“ retrieving relevant information from a trusted external knowledge base (e.g., documents indexed in a vector store using embeddings) based on the user's query, and then feeding this retrieved context along with the query to the LLM for grounded answer generation â€“ was explained conceptually, often facilitated by frameworks like LangChain or LlamaIndex. Two practical application examples were developed: a **FITS keyword explainer** using an LLM API augmented by a predefined dictionary of keyword definitions (simple RAG), and an **arXiv abstract summarizer** combining arXiv API queries (via the `arxiv` library) with LLM API calls for automated summarization. Finally, the chapter addressed crucial practical considerations when using LLM APIs: managing **costs** based on token usage, respecting provider **rate limits** by implementing delays and retry logic, and accounting for network and model **latency** in application design.

---

**References for Further Reading (APA Format, 7th Edition):**

1.  **OpenAI. (n.d.).** *OpenAI API Documentation*. OpenAI. Retrieved January 16, 2024, from [https://platform.openai.com/docs](https://platform.openai.com/docs) (Includes Prompt Engineering Guide, API Reference, Cookbooks).
    *(Official documentation essential for using the OpenAI API (Sec 29.1), understanding prompt engineering best practices (Sec 29.2), and checking pricing/rate limits (Sec 29.6) and the applications.)*

2.  **Lewis, P., et al. (2020).** Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. *(See reference in Chapter 26)*.
    *(The foundational RAG paper, providing the theoretical basis for the technique discussed practically in Sec 29.3 and applied conceptually in Application 29.A.)*

3.  **LangChain Developers. (n.d.).** *LangChain Documentation*. LangChain. Retrieved January 16, 2024, from [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction) (See also LlamaIndex Documentation: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/))
    *(Documentation for popular frameworks designed to build LLM applications, particularly RAG systems, providing tools for indexing, retrieval, prompting, and chaining relevant to Sec 29.3.)*

4.  **White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., ... & Schmidt, D. C. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with Large Language Models*. arXiv preprint arXiv:2302.11382. [https://arxiv.org/abs/2302.11382](https://arxiv.org/abs/2302.11382)
    *(Provides examples and categorizes various prompt engineering patterns and techniques, offering practical strategies relevant to Sec 29.2.)*

5.  **arXiv. (n.d.).** *arXiv API User Manual*. arXiv.org. Retrieved January 16, 2024, from [https://info.arxiv.org/help/api/index.html](https://info.arxiv.org/help/api/index.html) (See also `arxiv` Python library: [https://github.com/lukasschwab/arxiv.py](https://github.com/lukasschwab/arxiv.py))
    *(Documentation for the arXiv API used for programmatic data retrieval in Application 29.B. The linked Python library simplifies interaction.)*
