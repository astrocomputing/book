**Chapter 7: Introduction to Astronomical Surveys and Archives**

Having established the fundamental ways astrophysical data are represented in files and memory, we now broaden our perspective to consider where this data originates and how it is organized on a global scale. This chapter serves as an introduction to the vast landscape of astronomical data collection and dissemination, setting the stage for programmatic data access explored in Part II. We begin by confronting the sheer scale and complexity of modern datasets – the "data tsunami" – driven by increasingly powerful telescopes and simulations. We then provide an overview of major ground-based and space-based observational facilities and surveys across the electromagnetic spectrum and beyond, highlighting the diversity of data produced. Subsequently, we introduce the concept of centralized data archives and the primary data centers (like MAST, IRSA, ESASky) responsible for curating, processing, and distributing this wealth of information. Essential aspects of data usage, including access policies and proper citation practices, are discussed. We then explore the web-based discovery portals that often serve as the initial entry point for finding relevant data before moving to programmatic queries. Finally, we clarify the crucial concept of data processing levels, distinguishing between raw instrument telemetry, calibrated datasets, and higher-level, science-ready data products.

**7.1 The Data Explosion in Astronomy**

*   **Objective:** Motivate the need for sophisticated data management, archives, and programmatic access techniques by illustrating the exponential growth in the volume, velocity, and complexity of astronomical data from both observations and simulations.
*   **Modules:** Conceptual; Python code examples less relevant here, focus is on understanding the scale.

The field of astrophysics is currently undergoing a profound transformation driven by an unprecedented explosion in data volume and complexity. Gone are the days when an astronomer might analyze a few photographic plates or a handful of spectra over their career. We are now firmly in the era of "Big Data" astronomy, where the sheer quantity of information generated by modern observational facilities and large-scale numerical simulations fundamentally changes how research is conducted. This "data tsunami" presents immense opportunities for discovery but also necessitates a paradigm shift towards advanced computational techniques for data management, processing, and analysis. Simply collecting the data is no longer the primary bottleneck; efficiently extracting scientific knowledge from it is the central challenge.

This data explosion is fueled by technological advancements on multiple fronts. Ground-based survey telescopes, like the Zwicky Transient Facility (ZTF), Pan-STARRS, the Dark Energy Survey (DES), and soon the Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST), scan vast swathes of the sky repeatedly, generating terabytes of imaging data per night and building up petabyte-scale archives over their lifetimes. Radio interferometers like the Atacama Large Millimeter/submillimeter Array (ALMA), the Very Large Array (VLA), and the upcoming Square Kilometre Array (SKA) produce complex visibility data that requires sophisticated processing to generate massive data cubes. Space-based observatories, such as the Hubble Space Telescope (HST), Chandra X-ray Observatory, XMM-Newton, Spitzer Space Telescope (now retired but archives remain crucial), Gaia, the Transiting Exoplanet Survey Satellite (TESS), and the James Webb Space Telescope (JWST), provide high-resolution, multi-wavelength data often inaccessible from the ground, contributing significantly to the growing archival volumes.

The complexity of the data is increasing alongside its volume. Modern surveys often provide not just static images but time-series photometry (light curves) for billions of objects, enabling studies of variable stars, supernovae, and transiting exoplanets. Integral Field Units (IFUs) on telescopes like MUSE or KCWI produce spectral data cubes, providing spectra for every pixel within a 2D field of view. High-resolution spectrographs generate detailed spectra requiring careful calibration and analysis. Furthermore, large-scale cosmological simulations (N-body, hydrodynamical, radiative transfer) modeling the formation and evolution of structure in the universe now routinely produce terabyte-to-petabyte scale outputs, tracking billions of particles or grid cells and encompassing a wide range of physical properties over cosmic time.

The sheer scale of these datasets renders traditional methods of data handling obsolete. It is often no longer feasible, or even desirable, for individual researchers to download entire survey datasets to their local machines. Transfer times become prohibitive, and local storage and processing capabilities are quickly overwhelmed. Analyzing LSST's full ten-year dataset, projected to be tens of petabytes, will require computational resources far beyond typical university clusters, necessitating analysis frameworks co-located with the data archives themselves. Even analyzing simulation snapshots, often hundreds of gigabytes or terabytes each, demands access to significant computational power and memory.

This necessitates a fundamental shift towards centralized **data archives** and **science platforms**. Major space agencies (NASA, ESA) and national observatories (NOIRLab, NRAO) invest heavily in creating robust infrastructure not only to store the data generated by their facilities but also to provide calibration pipelines, data curation services, sophisticated query interfaces, and often, computational resources located near the data. Examples include the Mikulski Archive for Space Telescopes (MAST) at STScI, the Infrared Science Archive (IRSA) at IPAC/Caltech, the ESA Science Archives (ESASky, ESA Hubble Science Archive, Gaia Archive, etc.), the Canadian Astronomy Data Centre (CADC), and various simulation data portals. These archives become the primary repositories where researchers access scientifically validated data products.

This shift also highlights the critical importance of **standardization**, as discussed in Part I. For archives to function effectively and for data from different sources to be combined, adherence to standards like FITS for data representation, WCS for coordinates, and defined data processing levels is paramount. Without these standards, integrating and analyzing petabyte-scale heterogeneous datasets would be virtually impossible. The development and adoption of these standards are crucial enablers of modern data-intensive astrophysics.

Furthermore, the data explosion necessitates a change in research methodologies. Discoveries are increasingly made not just through deep investigation of individual objects, but through statistical analysis of vast populations, searching for rare events in time-domain data streams, and applying machine learning algorithms to classify objects or identify complex patterns. This requires **automation** and **programmatic access** to data archives. Manually downloading and inspecting thousands or millions of files or catalog entries is simply not feasible. Researchers need tools and techniques to script queries, filter results, retrieve data efficiently, and process it systematically, often leveraging high-performance computing resources.

The principles of **FAIR data** (Findable, Accessible, Interoperable, Reusable) have become central guiding tenets for modern scientific data management, strongly promoted by funding agencies and research communities worldwide. Astronomical archives, through their adoption of standards like FITS and VO protocols (discussed in Chapter 8), strive to make their holdings FAIR. Ensuring data is findable via searchable catalogs, accessible through defined protocols, interoperable thanks to standard formats and metadata, and reusable through clear provenance and data policies is essential for maximizing the scientific return on investment in large facilities and enabling collaborative, reproducible research.

This immense scale and complexity directly motivate the need for the skills covered in this book. Effectively navigating the modern astrophysical data landscape requires proficiency in programming (particularly Python), understanding data formats and database querying, applying appropriate statistical and machine learning techniques, and potentially utilizing simulation tools and high-performance computing. The "data tsunami" is not just a challenge but a tremendous opportunity, providing the raw material for discoveries limited primarily by our ability to process and analyze it effectively.

In conclusion, the exponential growth in the volume, variety, and velocity of astronomical data from surveys and simulations marks a fundamental shift in the practice of astrophysics. This "data explosion" necessitates the development and use of large-scale archives, standardized data formats, and powerful computational tools. It underscores the critical importance of **astrocomputing** skills for modern researchers seeking to extract scientific knowledge from these rich datasets. The subsequent sections of this chapter, and indeed the rest of the book, explore the infrastructure and techniques developed to manage and exploit this data revolution.

**7.2 Overview of Major Ground-based Surveys**

*   **Objective:** Familiarize the reader with prominent ground-based astronomical surveys across different wavelengths (optical, infrared, radio), highlighting their scientific goals, types of data produced, and the scale of their operations.
*   **Modules:** Conceptual overview.

While space-based observatories offer unique advantages, ground-based telescopes remain cornerstones of astrophysical research, particularly for large-area surveys that monitor vast portions of the sky. Continuous improvements in detector technology (large CCD mosaics, sensitive infrared arrays, wide-band radio receivers) and telescope design have enabled a new generation of ground-based surveys producing datasets of unprecedented depth and breadth across the electromagnetic spectrum. Understanding the major players and the types of data they generate is crucial for identifying relevant resources for specific scientific questions.

In the **optical/near-infrared (NIR)** domain, **imaging surveys** have been particularly impactful. The **Sloan Digital Sky Survey (SDSS)**, starting in the late 1990s and continuing through multiple phases (SDSS-I to SDSS-V), revolutionized extragalactic astronomy and stellar population studies by systematically mapping a large fraction of the Northern sky in five optical bands (u, g, r, i, z) and obtaining spectra for millions of galaxies and quasars. Its publicly available data archive remains a fundamental resource. **Pan-STARRS (Panoramic Survey Telescope and Rapid Response System)** uses telescopes in Hawaii to repeatedly survey the sky primarily in the northern hemisphere, focusing on detecting moving objects (asteroids) and transient events, while building deep static sky maps. The **Dark Energy Survey (DES)**, using the Blanco telescope in Chile, imaged a large area of the Southern sky to high depth in multiple optical/NIR filters, primarily aimed at studying dark energy through techniques like weak lensing, galaxy clustering, and supernova searches. Looking forward, the **Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST)**, currently under construction in Chile, promises to dwarf previous surveys. It will image the entire accessible southern sky every few nights for ten years in six optical bands, generating an enormous time-domain dataset (~15 TB per night) ideal for transient science, cosmology, Galactic structure, and Solar System studies, resulting in a multi-petabyte archive.

Ground-based **spectroscopic surveys** complement imaging surveys by providing detailed information about the physical properties of celestial objects, such as their chemical composition, temperature, velocity, and redshift (distance). SDSS included a massive spectroscopic component targeting galaxies, quasars, and stars. More recent large spectroscopic surveys include the **APOGEE (Apache Point Observatory Galactic Evolution Experiment)** survey (part of SDSS), which uses high-resolution infrared spectroscopy to probe stellar populations across the Milky Way disk and bulge, penetrating dust obscuration. **LAMOST (Large Sky Area Multi-Object Fiber Spectroscopic Telescope)** in China is obtaining low-to-medium resolution optical spectra for millions of stars. Other specialized spectroscopic surveys target specific object classes or science goals, often using dedicated instruments on large telescopes.

Moving to longer wavelengths, **infrared surveys** from the ground are challenged by atmospheric absorption and emission but are crucial for studying cool objects, dust-obscured regions, and high-redshift galaxies. Surveys like the **UKIRT Infrared Deep Sky Survey (UKIDSS)** and the **VISTA Hemisphere Survey (VHS)** used dedicated telescopes in Hawaii and Chile, respectively, to map large areas in near-infrared bands (Y, J, H, K), providing deeper counterparts to optical surveys like SDSS. Upcoming facilities or instruments aim to push ground-based IR capabilities further.

The **radio** domain offers a unique window on the universe, probing non-thermal emission (synchrotron radiation from relativistic electrons), cold gas (via molecular lines or neutral hydrogen 21cm emission), and the cosmic microwave background. Major radio survey instruments are typically interferometers, combining signals from multiple antennas to achieve high angular resolution. The **Very Large Array (VLA)** in New Mexico has conducted seminal surveys like the **NRAO VLA Sky Survey (NVSS)** and the **Faint Images of the Radio Sky at Twenty-Centimeters (FIRST)** survey, mapping large areas at centimeter wavelengths. The **Atacama Large Millimeter/submillimeter Array (ALMA)** in Chile, operating at shorter wavelengths, provides unprecedented sensitivity and resolution for studying cold gas and dust in star-forming regions, protoplanetary disks, and distant galaxies, though its primary mode is pointed observations rather than blind surveys. The **Australian SKA Pathfinder (ASKAP)** and **MeerKAT** in South Africa are precursor telescopes for the **Square Kilometre Array (SKA)**, already conducting large surveys (e.g., ASKAP's EMU and WALLABY, MeerKAT's MIGHTEE and LADUMA) that are mapping vast areas of the sky with high sensitivity and resolution at centimeter wavelengths, probing neutral hydrogen, continuum emission, and polarization across cosmic time. The upcoming SKA itself promises to revolutionize radio astronomy, generating data volumes potentially exceeding even LSST's.

Other ground-based facilities contribute crucial data across the spectrum or through different messengers. Telescopes like the **Canada-France-Hawaii Telescope (CFHT)**, the **Subaru Telescope**, Gemini Observatory, the Very Large Telescope (VLT), and the Keck Observatory, while often used for pointed observations, also host instruments used for smaller-scale surveys or contribute significantly to large program data archives. Gamma-ray astronomy from the ground uses imaging atmospheric Cherenkov telescopes (IACTs) like **VERITAS**, **MAGIC**, and **H.E.S.S.**, and the upcoming **Cherenkov Telescope Array (CTA)**, to detect very high-energy photons. Neutrino detectors like **IceCube** and gravitational wave detectors like **LIGO**, **Virgo**, and **KAGRA** represent entirely different observational windows, also generating large datasets requiring specialized analysis.

The common thread among these diverse ground-based facilities and surveys is the production of large, often complex datasets that require archiving and distribution. While some older survey data might have been distributed via physical media or direct downloads, modern surveys almost exclusively rely on dedicated online archives, often accessible through web portals and, increasingly, through programmatic interfaces designed to handle the large data volumes involved.

Understanding the scientific goals, wavelength coverage, survey strategy (area, depth, cadence), and data products associated with these major ground-based surveys is the first step towards identifying the most relevant datasets for a given research question. Whether searching for nearby stars with Gaia parallaxes complemented by ground-based NIR photometry from VISTA, studying galaxy evolution using SDSS imaging and spectroscopy, or probing cosmic magnetism with ASKAP polarization data, knowing where the data comes from and what it represents is essential.

The data from these surveys are typically processed through sophisticated pipelines run by the survey teams or associated data centers. These pipelines perform tasks like instrument calibration, image coaddition, source detection, photometric and astrometric measurements, and spectral extraction. The resulting data products, ranging from raw instrument data to fully calibrated images, spectra, and comprehensive source catalogs, are then ingested into archives for public access, often after a proprietary period. The next sections will discuss these archives and data levels in more detail.

**7.3 Overview of Major Space-based Missions/Archives**

*   **Objective:** Introduce key space-based astronomical missions and their corresponding primary data archives (e.g., MAST, IRSA, ESASky), highlighting their multiwavelength coverage and unique observational capabilities compared to ground-based facilities.
*   **Modules:** Conceptual overview.

Complementing ground-based observatories, space-based missions provide access to wavelengths blocked by Earth's atmosphere (gamma-rays, X-rays, UV, significant portions of the infrared) and offer observations unhindered by atmospheric blurring (seeing) or weather, enabling extremely high angular resolution and sensitivity. Data from these missions are invaluable, often unique, resources curated and distributed through dedicated archives, primarily managed by major space agencies like NASA and ESA. Familiarity with these missions and their archives is crucial for multiwavelength astrophysics and accessing high-fidelity datasets.

NASA's primary archive for optical, ultraviolet, and near-infrared data from its flagship missions is the **Mikulski Archive for Space Telescopes (MAST)**, hosted at the Space Telescope Science Institute (STScI). MAST is the central repository for data from the **Hubble Space Telescope (HST)**, arguably the most productive telescope in history, providing decades of high-resolution imaging and spectroscopy. MAST also hosts data from the **James Webb Space Telescope (JWST)**, Hubble's successor, offering unprecedented sensitivity and resolution in the near- and mid-infrared. Other key missions archived at MAST include UV observatories like **GALEX** (Galaxy Evolution Explorer) and **IUE** (International Ultraviolet Explorer), the planet-finding missions **Kepler**, **K2**, and **TESS** (Transiting Exoplanet Survey Satellite), and numerous other smaller missions and surveys (including ground-based surveys like Pan-STARRS). MAST provides sophisticated web-based discovery tools, catalog access, and programmatic interfaces (explored in Part II).

For infrared astronomy, NASA's **Infrared Science Archive (IRSA)**, located at IPAC (Infrared Processing and Analysis Center) at Caltech, is a primary resource. IRSA hosts data from missions like the **Spitzer Space Telescope**, which provided crucial mid- and far-infrared observations, the **Wide-field Infrared Survey Explorer (WISE)** and its **NEOWISE** reactivation, which mapped the entire sky in the infrared multiple times (essential for asteroid detection and studies of stars, galaxies, and dust), the **Infrared Astronomical Satellite (IRAS)**, and the US data contributions from the ESA **Herschel Space Observatory** (far-infrared/submillimeter) and **Planck** (cosmic microwave background). IRSA offers powerful tools for querying image data, catalogs, and spectra across these missions.

High-energy astrophysics relies on space missions observing X-rays and gamma-rays. NASA's **Chandra X-ray Observatory** provides unparalleled high-resolution X-ray imaging and spectroscopy, with its data curated at the **Chandra Data Archive (CDA)** managed by the Chandra X-ray Center (CXC). ESA's **XMM-Newton** offers complementary capabilities with higher throughput for X-ray spectroscopy, and its data is accessible via the **XMM-Newton Science Archive (XSA)**. Other important X-ray missions with public archives include **NuSTAR** (focusing on hard X-rays), **Swift** (rapid response for gamma-ray bursts and multiwavelength follow-up), **RXTE** (Rossi X-ray Timing Explorer), and the **Neil Gehrels Swift Observatory**. Gamma-ray missions like the **Fermi Gamma-ray Space Telescope** and the **Compton Gamma Ray Observatory (CGRO)** also have dedicated archives, often hosted at NASA's High Energy Astrophysics Science Archive Research Center (HEASARC). HEASARC serves as a central portal for many NASA high-energy mission archives.

The European Space Agency (ESA) also operates major archives for its missions. The **ESA Science Data Centre (ESDC)** hosts several key archives accessible through the unified **ESASky** discovery portal or dedicated interfaces. These include the **Gaia Archive**, providing access to the transformative astrometric, photometric, and spectroscopic data from the Gaia mission; the **ESA Hubble Science Archive (eHST)**, mirroring Hubble data also found at MAST; the **XMM-Newton Science Archive (XSA)**; the **Herschel Science Archive (HSA)**; the **Planck Legacy Archive**; and archives for missions like **INTEGRAL** (gamma-ray), **ISO** (Infrared Space Observatory), and upcoming missions like **Euclid** (dark energy survey). ESA archives prioritize adherence to Virtual Observatory standards, often providing powerful TAP interfaces (Chapter 11).

Other international space agencies also contribute significant data archives. The **Japanese Aerospace Exploration Agency (JAXA)** manages archives for missions like **Suzaku** (X-ray), **Akari** (infrared survey), and **Hinode** (solar physics). National data centers, like the **Canadian Astronomy Data Centre (CADC)**, often mirror data from international facilities (like HST, CFHT, Gemini) and host data from Canadian-led projects. Solar physics data, beyond SDO (often accessed via JSOC), includes archives for missions like **SOHO** (Solar and Heliospheric Observatory), **STEREO**, and **Parker Solar Probe**, often accessible via NASA's Solar Data Analysis Center (SDAC) or dedicated mission websites.

These space-based archives typically provide data processed to various levels (see Section 7.6), ranging from raw telemetry to fully calibrated images, spectra, light curves, and source catalogs. They offer web portals for browsing and retrieval (Section 7.5) and, crucially for large-scale analysis, increasingly sophisticated **programmatic access** interfaces. These often include Virtual Observatory protocols (SIA, SSA, TAP – Chapter 8) or mission-specific Application Programming Interfaces (APIs) that can be queried using libraries like `astroquery` (Part II).

Understanding which archive hosts the data for a specific mission is key. MAST for NASA UV/Optical/NIR flagships (Hubble, Webb, Kepler/TESS), IRSA for NASA Infrared (Spitzer, WISE), HEASARC/CDA/XSA for High Energy, ESASky/ESA Archives for ESA missions (Gaia, Herschel, XMM, Euclid), and JSOC/SDAC for solar data are primary starting points. Many archives also host "contributed" high-level science products generated by research teams, further enriching their holdings.

The scientific potential unlocked by combining data across these different missions and wavelengths is immense. A multiwavelength approach is often essential for a complete understanding of astrophysical objects and phenomena. For example, studying an active galactic nucleus (AGN) might involve combining radio data (VLA/ALMA via NRAO archive) showing jets, infrared data (WISE/Spitzer via IRSA) probing the dusty torus, optical data (HST/SDSS via MAST/SDSS archive) revealing the host galaxy and accretion disk emission, UV data (GALEX/HST via MAST) for hot disk components, and X-ray data (Chandra/XMM via CDA/XSA) examining the corona and inner accretion flow. The existence of well-curated, accessible archives is what makes such multi-faceted studies feasible.

In conclusion, space-based missions offer unique observational capabilities, and their data are curated in major archives primarily operated by NASA and ESA, often specialized by wavelength regime (MAST, IRSA, HEASARC, ESA archives). These archives provide essential access to high-quality, often unique datasets crucial for a wide range of astrophysical research. Familiarity with the main missions and their corresponding archives is essential for navigating the landscape of available space-based data.

**7.4 Data Access Policies and Citation**

*   **Objective:** Emphasize the importance of understanding and respecting data access policies (proprietary periods, usage restrictions) and the ethical and practical necessity of properly citing the datasets, facilities, and software used in research.
*   **Modules:** Conceptual overview.

Accessing the wealth of data stored in astronomical archives comes with responsibilities. While the trend is overwhelmingly towards open data access, allowing researchers worldwide to benefit from major facilities, it's crucial to be aware of potential **data access policies** and the universal requirement for proper **citation**. Ignoring these aspects can lead to misunderstandings, ethical breaches, and hinder the reproducibility and recognition that underpin scientific progress.

Many large observational facilities, particularly new ones, often have a **proprietary period** associated with their data. During this period, typically ranging from several months to a year or more, the data is accessible only to the Principal Investigator (PI) team that proposed the observations and potentially their collaborators. The purpose of this period is to give the team who invested the effort in designing and obtaining the observations a reasonable timeframe to analyze the data and publish their primary results before it becomes public. It is essential to check the proprietary status of data before attempting to use it for publication, especially for recent observations obtained from major telescopes like HST, JWST, ALMA, VLT, Keck, etc. Archive interfaces usually clearly indicate the proprietary status and release date.

Beyond proprietary periods, some datasets might have specific **usage restrictions** or licensing terms, although this is less common for publicly funded astronomical data archives compared to commercial datasets. It's always prudent to check the archive's general data usage policy or any specific terms associated with a dataset, particularly if intending to use it for commercial purposes or redistribute it extensively. Most public archives, however, operate under the principle that data should be freely available for scientific research and educational use.

A cornerstone of ethical scientific practice is proper **attribution**. When you use data obtained from an observatory or archive, or results generated by a specific simulation project, it is absolutely essential to cite the source appropriately in your publications, presentations, and reports. This acknowledges the effort and resources invested in acquiring, processing, and curating the data, ensures reproducibility by allowing others to find the exact dataset used, and provides metrics that help justify continued funding for these vital facilities and projects. Failing to cite data sources appropriately is a serious oversight.

Archives and survey teams almost always provide specific instructions on **how to cite their data**. These instructions are usually found prominently on the archive website or within the documentation accompanying data releases. The required citation often includes:
1.  Reference(s) to the **instrument or survey paper(s)** describing the facility, observations, and data processing.
2.  Acknowledgement of the **funding agencies** and **institutions** involved.
3.  Sometimes, a specific **Digital Object Identifier (DOI)** assigned to the dataset itself or the data release. DOIs provide persistent, unique identifiers for datasets, making citation more precise and trackable.
4.  Acknowledgement text explicitly mentioning the **archive** itself (e.g., "This work made use of data from the Mikulski Archive for Space Telescopes (MAST)...").

Here is a hypothetical, but representative, example of the kind of acknowledgement text often required:

> *"This research has made use of data obtained from the Anytown Observatory Archive, which is funded by the National Science Foundation under grant XYZ-12345. We acknowledge the use of observations from the GreatScope Telescope under proposal ID GS-2023A-007 (PI: Stellar). We utilized data products generated by the SkyMapper Survey, described in Mapper et al. (2023, ApJ, 999, 123). This research made use of Astropy, a community-developed core Python package for Astronomy (Astropy Collaboration et al. 2013, 2018)."*

Note the inclusion of instrument papers, proposal IDs where relevant, survey papers, funding acknowledgements, and potentially archive acknowledgement. It's crucial to find and follow the *specific* instructions provided by the data source you are using. Many journals now also have explicit policies requiring data citation and sometimes data availability statements.

Beyond citing the data origin, it is equally important to cite the **software** used for analysis. Modern astrophysical research relies heavily on open-source software packages like Astropy, NumPy, SciPy, Matplotlib, Pandas, specific simulation analysis tools (`yt`), MCMC codes (`emcee`), etc. Citing these packages acknowledges the significant community effort involved in their development and maintenance, helps ensure reproducibility (by specifying software versions), and provides usage metrics that can support funding for software development. Most well-maintained packages provide clear instructions on how they should be cited, often including specific papers or DOIs (e.g., via Zenodo). Many packages, including Astropy, even provide built-in helpers or website tools to generate appropriate citation text.

Adhering to data policies and citation requirements is not just about following rules; it's about participating responsibly in the scientific community. It ensures fair credit is given, allows others to build upon previous work by accessing the same data and tools, and supports the long-term sustainability of the infrastructure that makes modern astrophysics possible. Always check the archive or project website for their specific policies and citation guidelines before using data in your work.

**7.5 Introduction to Data Discovery Portals**

*   **Objective:** Introduce the concept and common features of web-based data discovery portals provided by major astronomical archives, serving as the primary interface for manual data searching and exploration.
*   **Modules:** Conceptual overview; involves web browser usage, not specific Python modules.

With vast amounts of data stored in distributed archives worldwide, finding the specific datasets relevant to a particular scientific question can be a significant challenge. To address this, major astronomical data archives provide sophisticated **web-based discovery portals**. These portals act as interactive gateways, allowing users to search the archive's holdings based on various criteria, inspect metadata and previews of potential datasets, select desired data products, and initiate retrieval requests, all through a graphical user interface (GUI) accessible via a standard web browser. While programmatic access (Part II) is essential for large-scale or automated tasks, these web portals are invaluable tools for initial data exploration, familiarization with archive contents, and retrieving smaller data volumes.

Most major archives, such as MAST (Mikulski Archive for Space Telescopes), IRSA (Infrared Science Archive), ESASky (European Space Agency Sky), the NOIRLab Astro Data Archive, CADC (Canadian Astronomy Data Centre), and the archives for Chandra (CDA/ChaSeR) and XMM-Newton (XSA), feature powerful discovery portals as their main user interface. While the specific look-and-feel and features vary between archives, they generally share a common set of core functionalities designed to help users find and access data effectively.

A fundamental search capability is querying by **target name or sky coordinates**. Users can typically enter the name of a celestial object (e.g., "M31", "SN 2023a", "Sirius"), which the portal resolves to coordinates using services like SIMBAD or NED. Alternatively, users can directly input Right Ascension (RA) and Declination (Dec) coordinates, often specifying a search radius (e.g., "find all observations within 5 arcminutes of RA=X, Dec=Y"). This **cone search** functionality is ubiquitous and essential for finding data related to specific objects or sky regions.

Beyond target position, portals usually allow filtering based on various **observational constraints**. Users can often specify the desired mission or telescope (e.g., "Hubble", "Spitzer", "VLA"), instrument ("WFC3", "IRAC", "ACIS"), filter or wavelength ("F606W", "171 Angstrom", "20 cm"), observation date range, exposure time, proposal ID, or principal investigator. Advanced searches might allow filtering based on data processing level, data quality flags, or specific metadata keywords present in the FITS headers. These filters are crucial for narrowing down potentially millions of observations to the specific subset relevant to the user's science case.

Once a search is performed, the discovery portal typically presents the results in an interactive format. This often includes a **tabular list** of matching observations or data products, displaying key metadata like observation ID, target name, coordinates, instrument configuration, exposure time, observation date, data type, and proprietary status/release date. Many portals also feature an interactive **sky map** (often using tools like Aladin Lite) that visually displays the footprints (outlines of the observed area on the sky) of the matching observations overlaid on an all-sky image survey, helping users understand the spatial coverage and context of the data. **Preview images** or quick-look plots (e.g., light curves, spectra) are often available for quick assessment of data quality or content without needing to download the full files.

After inspecting the search results and identifying relevant datasets, users typically select the desired data products for retrieval. Portals often use a "shopping basket" metaphor, where users add files to a list. Once the selection is complete, the user initiates the **data retrieval** process. For small requests, data might be directly downloadable via HTTP links. For larger requests involving many files or significant data volumes, the archive often stages the data asynchronously. The user submits the request, and the archive system gathers the files (potentially retrieving them from tape storage) and notifies the user (e.g., via email) when the data is ready for download, often providing download scripts or access via tools like Globus Online for efficient transfer.

It is crucial to recognize that each archive portal has its own unique interface, specific search parameters, naming conventions for data products, and potentially different ways of organizing data from various missions or processing levels. While efforts towards standardization exist (especially through VO protocols), effectively using a specific portal often requires spending some time exploring its features and consulting the archive's documentation or tutorials. Understanding the terminology used by a particular archive (e.g., what constitutes a "dataset", an "observation", a "product") is essential for constructing meaningful queries and retrieving the correct files.

Discovery portals often provide additional valuable features beyond basic searching. These might include **cross-matching** capabilities (finding counterparts to a list of input coordinates in catalogs hosted by the archive), direct access to **source catalogs** generated from survey data, tools for **spectral or light curve visualization**, links to relevant **documentation** and **publication lists**, and sometimes even basic **online analysis tools**. Exploring these features can significantly enhance the data discovery and initial analysis process.

However, web portals have inherent limitations for certain research workflows. Performing searches for thousands of targets, applying complex filtering logic iteratively, or integrating data retrieval directly into an automated analysis pipeline is difficult or impossible through a GUI. Scripting queries and downloads becomes necessary for reproducibility and scalability. Furthermore, web interfaces can change over time, potentially breaking manual workflows reliant on specific button clicks or layouts.

In conclusion, data discovery portals are the essential human interfaces to major astronomical archives, providing powerful interactive tools for searching, exploring, visualizing, and retrieving data. They are excellent for familiarizing oneself with archive holdings, performing initial reconnaissance for a project, and downloading small to moderate datasets. While invaluable, their limitations in automation and scalability motivate the need for the programmatic access methods using Python libraries like `astroquery` and VO protocols, which will be the focus of the subsequent chapters in Part II. Understanding the capabilities and organization revealed through the web portal often provides crucial context for constructing effective programmatic queries later.

**7.6 Understanding Data Levels**

*   **Objective:** Explain the concept of data processing levels (e.g., Level 0, 1, 2, 3) commonly used by observatories and archives, clarifying the distinction between raw instrument data, calibrated data, and high-level science-ready products.
*   **Modules:** Conceptual overview.

When searching for and retrieving data from astronomical archives, you will frequently encounter references to different **data processing levels**, often denoted by numbers like Level 0, Level 1, Level 2, Level 3, or variations thereof (e.g., Level 1a, 1b, 2a). These levels represent distinct stages in the data processing pipeline, transforming the raw output from the telescope's detectors into scientifically usable information. Understanding the meaning of these levels is crucial for selecting the appropriate data products for your specific scientific goals, as different levels contain different information and have undergone different degrees of calibration and analysis. While the exact definitions can vary slightly between missions and observatories, a general hierarchy is widely adopted.

**Level 0** data typically represents the rawest form of telemetry received from the instrument, often consisting of unprocessed detector readouts, engineering data, spacecraft attitude information, and timing packets. This data is usually in a mission-specific, often binary, format that is not directly usable for scientific analysis without specialized software and detailed knowledge of the instrument. Level 0 data encodes the fundamental measurements but lacks calibration and often even basic structuring like image formation. Access to Level 0 data is generally restricted to instrument teams or processing centers, although it might be archived for long-term preservation or reprocessing needs.

**Level 1** data represents the first stage of significant processing, typically involving unpacking the raw telemetry, organizing it by observation or exposure, applying initial detector corrections, and formatting it into a more standardized structure, often FITS files. Level 1 data might be further subdivided:
*   **Level 1a:** Often represents minimally processed data, perhaps reformatted into FITS but still containing raw or near-raw detector counts (e.g., Data Numbers or DNs), with basic header information populated. Geometric distortion might still be present.
*   **Level 1b:** Typically involves applying standard instrumental calibrations to convert raw counts into physically meaningful units. For imaging data, this usually includes bias subtraction, dark current correction, flat-fielding, cosmic ray identification/masking, and potentially fixing known bad pixels. For spectroscopic data, it might involve wavelength calibration and flux calibration relative to a standard. The data values are often converted to units like electrons, counts per second, or physical flux units (though sometimes requiring further calibration steps). WCS information is usually added or refined at this stage. Level 1b data is often the starting point for detailed scientific analysis by researchers who want maximum control over the calibration process or need to perform non-standard reductions.

**Level 2** data represents calibrated, science-ready data products, typically expressed in standard physical units and mapped onto standard coordinate systems (e.g., sky coordinates via WCS). For imaging, Level 2 products are often fully calibrated images (e.g., in flux density units like Jy/pixel or surface brightness units), potentially corrected for geometric distortion and registered to a standard astrometric frame. For spectroscopy, Level 2 might be fully flux-calibrated 1D spectra or calibrated spectral cubes. For time-series data (like Kepler or TESS light curves), Level 2 often refers to calibrated flux time series with systematic effects removed or flagged. Level 2 data products are the most commonly used starting point for scientific analysis by the general astronomical community, as the fundamental instrument calibrations have already been applied by the observatory or survey team using well-validated pipelines.

**Level 3** data typically refers to **high-level science products** that combine or further analyze data from multiple Level 1 or Level 2 observations. Examples include deep mosaic images created by co-adding multiple exposures of the same sky region, comprehensive source catalogs extracted from survey data (listing positions, magnitudes, morphologies, etc.), combined spectra with higher signal-to-noise, or time-averaged data products. These products are often generated by the survey teams or specialized processing centers to facilitate specific science goals (e.g., creating a deep static sky map from a time-domain survey). Level 3 products can be extremely valuable as they represent significant processing effort and often provide readily usable summaries or enhanced views of the underlying data.

Sometimes, **Level 4** data products are also mentioned, typically referring to even more derived results, often from theoretical modeling or comparisons across entirely different datasets, or data published in Virtual Observatory compliant formats. The distinction between Level 3 and Level 4 can be blurry and less standardized than the lower levels.

When using data discovery portals or programmatic interfaces, archives usually allow users to filter or search specifically for data products at a certain processing level. For instance, a query might target "Level 2 calibrated images" or "Level 3 source catalogs". Understanding the processing steps involved in generating each level is crucial for interpreting the data correctly. For example, Level 1b data might still require manual correction for scattered light or background subtraction, while Level 2 data might already have these steps applied. Level 3 mosaics might involve specific choices about image resampling or background matching that could affect certain types of analysis.

It is therefore essential to consult the documentation provided by the specific mission, survey, or archive regarding their data processing pipelines and the precise definition of their data levels. This documentation explains which calibration steps have been applied, the units of the data values, the accuracy of the WCS and photometric calibration, known limitations or artifacts, and provides guidance on the appropriate use cases for each data level. Choosing the wrong data level (e.g., using uncalibrated Level 1a data assuming it's flux-calibrated Level 2) can lead to significant errors.

For most standard scientific analyses, researchers typically start with Level 2 data products, benefiting from the expert calibration performed by the instrument teams. However, for specialized analyses requiring non-standard calibration, detailed artifact investigation, or reprocessing with updated algorithms, access to Level 1b data might be necessary. Level 3 products are often ideal for large statistical studies or when a combined, deeper view of a region is required. Level 0 is rarely needed by end-users.

In conclusion, understanding the hierarchy of data processing levels (from raw Level 0 telemetry to calibrated Level 1/2 science data and high-level Level 3 products) is fundamental for navigating astronomical archives effectively. It allows researchers to select the data products most appropriate for their scientific goals and ensures they correctly interpret the information content, units, and calibration status of the data they retrieve, forming a crucial piece of context alongside format, WCS, and time information.

**Application 7.A: Identifying Infrared Data Sources for M31**

*   **Objective:** Reinforce the understanding of the diversity of astronomical archives and the types of data they hold (Sections 7.2, 7.3, 7.5), by requiring the researcher to map a specific scientific goal (studying infrared properties of the Andromeda galaxy, M31) to potentially relevant missions, instruments, and archives using web-based discovery tools and documentation.
*   **Astrophysical Context:** The Andromeda Galaxy (M31) is our nearest large spiral neighbor, providing a crucial laboratory for studying galaxy formation, stellar populations, dust properties, and star formation processes in detail. Infrared (IR) observations are particularly important as they penetrate dust obscuration, trace cool stellar populations, map thermal emission from dust grains heated by starlight (indicating star formation activity), and probe specific spectral features from molecules and dust components. A comprehensive study often requires combining data from multiple IR facilities covering different wavelength ranges (near-IR, mid-IR, far-IR).
*   **Data Source:** Information available on the websites and data discovery portals of major astronomical archives, particularly those specializing in infrared data (NASA/IPAC Infrared Science Archive - IRSA) and those hosting major space telescope data (Mikulski Archive for Space Telescopes - MAST, ESA Science Archives - ESASky). Knowledge of key infrared space missions (Spitzer, WISE, Herschel, JWST, IRAS, ISO, Akari) and relevant ground-based surveys (UKIDSS, VISTA/VHS) is needed.
*   **Modules Used:** Primarily requires a web browser and effective use of search engines and archive websites. No specific Python modules are directly used for this information gathering task, emphasizing the chapter's focus on understanding the data landscape before programmatic access.
*   **Technique Focus:** Information retrieval and synthesis. Researching the capabilities and data holdings of different archives. Identifying space missions and ground-based surveys operating at relevant infrared wavelengths. Using archive web portals (conceptually) to search for observations targeting M31. Understanding the types of data products typically offered for infrared missions (images, mosaics, source catalogs). This task focuses on Sections 7.2, 7.3, and 7.5.
*   **Processing (Web Research Workflow):**
    1.  Identify the science goal: Study dust and star formation in M31 using infrared data. This implies needing data spanning near-IR (tracing older stars, less obscured regions), mid-IR (tracing warm dust, PAHs), and far-IR (tracing cold dust, total star formation rate).
    2.  Recall or search for major IR space missions: Spitzer, Herschel, WISE, JWST, IRAS, ISO, Akari. Recall or search for major ground-based IR surveys: UKIDSS, VISTA/VHS.
    3.  Identify the primary archives for these missions: IRSA (Spitzer, WISE, IRAS, Herschel-US), MAST (JWST, Hubble-IR), ESA Archives/ESASky (Herschel, ISO, JWST-EU), JAXA archives (Akari), UKIDSS/WSA, VISTA/VSA.
    4.  Visit the websites or discovery portals of the most relevant archives (IRSA, MAST, ESASky).
    5.  Use the search interfaces on these portals. Enter "M31" or its coordinates (approx RA 00h 42m, Dec +41d 16m) as the target.
    6.  Filter the search results by mission/instrument:
        *   For Spitzer: Look for IRAC (near/mid-IR imaging) and MIPS (mid/far-IR imaging) observations.
        *   For Herschel: Look for PACS (far-IR imaging/spectroscopy) and SPIRE (far-IR/sub-mm imaging/spectroscopy) observations.
        *   For WISE: Look for all-sky survey data products covering the M31 region.
        *   For JWST: Look for MIRI (mid-IR imaging/spectroscopy) and NIRCam/NIRSpec (near-IR) observations.
        *   For ground-based: Check VISTA/UKIDSS archives for deep J, H, K band coverage.
    7.  Examine the search results to identify specific observation programs or surveys targeting M31 with these instruments.
    8.  Note the types of data products available for download (e.g., Level 2 calibrated images - often called BCD or PBCD for Spitzer, Level 2/3 maps for Herschel/JWST, source catalogs from WISE).
*   **Output:** A structured textual summary documenting the findings. This should list the relevant missions/instruments identified, the primary archive(s) hosting their data, the types of infrared observations suitable for the science goal (e.g., imaging, spectroscopy, wavelength range), and the typical data products available (e.g., calibrated single frames, mosaics, catalogs). Example snippets:
    *   "**Spitzer Space Telescope (via IRSA):** Provides crucial mid-infrared imaging via IRAC (3.6, 4.5, 5.8, 8.0 µm) and MIPS (24, 70, 160 µm). Data products include Level 1b (BCD) calibrated frames and Level 2 enhanced mosaics and source lists from legacy programs targeting M31."
    *   "**Herschel Space Observatory (via ESA Archives/IRSA):** Offers far-infrared imaging and spectroscopy with PACS (70-160 µm) and SPIRE (250-500 µm), essential for tracing cold dust emission related to star formation. Level 2/2.5/3 science-ready maps are typically available."
    *   "**Wide-field Infrared Survey Explorer (WISE/NEOWISE, via IRSA):** Provides all-sky maps and source catalogs at 3.4, 4.6, 12, and 22 µm, useful for studying resolved stellar populations and dust across M31, though potentially confusion-limited in crowded regions."
    *   "**James Webb Space Telescope (JWST, via MAST):** Offers high-sensitivity, high-resolution near- and mid-infrared imaging and spectroscopy with NIRCam, NIRSpec, and MIRI, enabling detailed studies of star-forming regions, stellar populations, and AGN activity within M31. Level 2 (calibrated) and Level 3 (mosaicked/combined) products are available."
    *   "**Ground-based (e.g., VISTA via VSA/ESO Archive):** Deep near-infrared surveys like VHS provide J, H, Ks band imaging, complementing optical surveys and probing less obscured stellar populations."
*   **Test:** Verify the existence of major M31 observing programs using the identified telescopes/instruments by searching proposal databases or relevant literature. Check if the wavelength coverage mentioned aligns with the scientific goal (dust/star formation). Confirm that the listed archives are indeed the primary hosts for the mission data.
*   **Extension:** Choose one specific instrument (e.g., Spitzer/IRAC). Use the IRSA web portal to find a specific observation ID or dataset name for an IRAC mosaic of M31's central region. Examine the metadata available online for this dataset (e.g., FITS header preview, observation date, exposure time). Find the documentation page describing the processing steps used to create the Level 2 IRAC mosaics.

**Application 7.B: Understanding Gaia Data Products and Levels**

*   **Objective:** Reinforce the understanding of data processing levels (Sec 7.6) and the importance of consulting documentation (Sec 7.4, 7.5) by investigating the specific data products and processing stages associated with a major survey, using the ESA Gaia mission as an example.
*   **Astrophysical Context:** The Gaia mission provides fundamental data for countless astrophysical studies. However, using the data effectively requires understanding what specific quantities are measured (astrometry, photometry, spectroscopy), how they are processed, what calibration levels are available, and what the typical uncertainties or limitations are. This knowledge is crucial for selecting the right data columns and interpreting results correctly.
*   **Data Source:** The official Gaia mission documentation website hosted by ESA (e.g., at cosmos.esa.int/web/gaia/dr3), particularly sections describing data processing, data model, and content of the data releases (e.g., Gaia DR3).
*   **Modules Used:** Web browser for accessing documentation. No specific Python modules are needed for this information-gathering task.
*   **Technique Focus:** Navigating and interpreting official mission documentation. Identifying different data processing stages or levels as defined by the mission. Listing key data products (measurements) available in a specific data release (e.g., DR3). Understanding the typical contents and quality of science-ready archive data. Relates primarily to Sec 7.3, 7.6.
*   **Processing (Web Documentation Research):**
    1.  Navigate to the official ESA Gaia Archive / Gaia Cosmos website.
    2.  Locate the documentation section, specifically for the latest major data release (e.g., Gaia DR3).
    3.  Search for information on the data processing pipeline or data processing levels. While Gaia might not use strict "Level 0/1/2/3" terminology, identify the stages from raw telemetry to calibrated measurements available in the archive. Note key processing steps like attitude reconstruction, source detection, cross-matching, astrometric solution, photometric calibration, spectral processing, and astrophysical parameter estimation.
    4.  Find the "Data Model" or "Archive Contents" documentation describing the tables available in the archive (e.g., `gaia_source` as the main table).
    5.  For the main source table (`gaia_source`), list the primary types of information provided for each star. This involves identifying key column names and understanding what they represent. Examples:
        *   Astrometry: `ra`, `dec`, `parallax`, `pmra`, `pmdec`, and associated errors and quality flags.
        *   Photometry: Mean fluxes/magnitudes in G, BP, and RP bands (`phot_g_mean_flux`, `phot_bp_mean_flux`, `phot_rp_mean_flux`, and corresponding `_mag` columns), along with errors.
        *   Radial Velocity: `radial_velocity` and `radial_velocity_error` (for a subset of stars).
        *   Astrophysical Parameters: `teff_gspphot`, `logg_gspphot`, `mh_gspphot` (effective temperature, surface gravity, metallicity derived from photometry), potentially others from spectroscopy (`teff_gspspec`, etc.).
        *   Other information: Source classification, variability flags, binary star parameters, potentially spectral or light curve data accessible via other tables linked by `source_id`.
    6.  Note the typical units for key quantities (e.g., degrees for RA/Dec, milliarcseconds for parallax, mas/yr for proper motion, magnitudes for photometry, km/s for radial velocity).
    7.  Look for information on data quality recommendations or known issues associated with the data release, often provided in release notes or dedicated documentation sections.
*   **Output:** A structured summary describing the Gaia DR3 data products and processing concept:
    *   A brief overview of the Gaia processing flow (mentioning key steps like astrometric solution, photometric/spectroscopic processing).
    *   A list of the main types of measurements available in the primary `gaia_source` table, including:
        *   Astrometry (5 or 6 parameters)
        *   Photometry (G, BP, RP bands)
        *   Radial Velocity (for subset)
        *   Astrophysical Parameters (photometrically derived Teff, logg, [M/H])
    *   Mention of other potential data products linked via `source_id` (e.g., light curves, spectra, specific object catalogs).
    *   Typical units for key parameters (mas, mas/yr, mag, km/s).
    *   Emphasis that the archive generally provides science-ready (Level 2 equivalent) data, but understanding quality flags and limitations described in documentation is crucial.
*   **Test:** Verify the listed data products against the official Gaia DR3 data model documentation available online. Check the units mentioned against the documentation. Find a research paper using Gaia DR3 data and see if the types of data columns they utilize match the list compiled.
*   **Extension:** Investigate the difference between the photometrically derived astrophysical parameters (`_gspphot`) and the spectroscopically derived ones (`_gspspec`). Find out which specific Gaia table contains epoch photometry (light curves) for variable sources. Explore the recommended quality cuts or filters described in the Gaia documentation for obtaining a "clean" sample for astrometric or photometric analysis (e.g., cuts on parallax error, RUWE parameter, photometric flags).

**Chapter 7 Summary**

This chapter provided a crucial overview of the landscape of astronomical data generation and curation, setting the context for programmatic data access and analysis. It began by highlighting the "data explosion" in modern astrophysics, driven by powerful new surveys and simulations, emphasizing the resulting challenges and the necessity for large-scale archives, standardized formats, and computational analysis techniques. An overview of major ground-based facilities followed, covering optical/NIR imaging (SDSS, Pan-STARRS, DES, LSST) and spectroscopic surveys (APOGEE, LAMOST), infrared surveys (UKIDSS, VISTA), and radio interferometers (VLA, ALMA, ASKAP, MeerKAT, SKA), illustrating the breadth of data being collected across wavelengths. Complementing this, major space-based missions and their primary archives were introduced, including NASA's MAST (Hubble, JWST, Kepler/TESS), IRSA (Spitzer, WISE), HEASARC (Chandra, XMM, high-energy), and ESA's archives accessed via ESASky (Gaia, Herschel, XMM, Euclid), emphasizing their unique capabilities and multiwavelength coverage.

Crucially, the chapter stressed the importance of responsible data usage, covering data access policies like proprietary periods and the ethical and practical necessity of properly citing data sources (facilities, surveys, archives), specific datasets (using DOIs where available), and the software tools used for analysis, referencing the guidelines provided by archives and projects. It then introduced the web-based data discovery portals provided by major archives (MAST, IRSA, ESASky, etc.) as essential tools for initial data exploration, searching by target or parameters, inspecting metadata and previews, and retrieving smaller datasets manually, while also noting their limitations for large-scale, automated, or reproducible workflows. Finally, the concept of data processing levels (Level 0 raw telemetry, Level 1 calibrated instrument data, Level 2 science-ready products, Level 3 high-level combined products) was explained, highlighting the importance of understanding the processing stage and calibration status of archived data before using it for scientific analysis, emphasizing the need to consult mission and archive documentation.

---

**References for Further Reading (APA Format, 7th Edition):**

1.  **Gaia Collaboration, Brown, A. G. A., Vallenari, A., Prusti, T., de Bruijne, J. H. J., Babusiaux, C., & Biermann, M. (2021).** Gaia Early Data Release 3: Summary of the contents and survey properties. *Astronomy & Astrophysics*, *649*, A1. [https://doi.org/10.1051/0004-6361/202039657](https://doi.org/10.1051/0004-6361/202039657)
    *(Provides a comprehensive overview of a major space-based survey and its data products, exemplifying the type of information discussed in Sec 7.3 and 7.6.)*

2.  **Ivezić, Ž., Kahn, S. M., Tyson, J. A., Abel, B., Acosta, E., Allsman, R., ... & LSST Collaboration. (2019).** LSST: From Science Drivers to Reference Design and Anticipated Data Products. *The Astrophysical Journal*, *873*(2), 111. [https://doi.org/10.3847/1538-4357/ab042c](https://doi.org/10.3847/1538-4357/ab042c)
    *(Describes a major upcoming ground-based survey, illustrating the scale and data product complexity motivating archives and advanced computation, relevant to Sec 7.1 and 7.2.)*

3.  **NASA/IPAC Infrared Science Archive (IRSA). (n.d.).** *IRSA Home*. IRSA. Retrieved January 16, 2024, from [https://irsa.ipac.caltech.edu/](https://irsa.ipac.caltech.edu/)
    *(Primary web portal for a major multi-mission archive discussed in Sec 7.3 and 7.5. Exploring this site provides practical insight into data discovery and holdings.)*

4.  **Mikulski Archive for Space Telescopes (MAST). (n.d.).** *MAST Home*. Space Telescope Science Institute. Retrieved January 16, 2024, from [https://mast.stsci.edu/](https://mast.stsci.edu/)
    *(Primary web portal for NASA's optical/UV/NIR flagship missions, discussed in Sec 7.3 and 7.5. Essential resource for Hubble, JWST, Kepler, TESS data.)*

5.  **Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., ... & Mons, B. (2016).** The FAIR Guiding Principles for scientific data management and stewardship. *Scientific Data*, *3*, 160018. [https://doi.org/10.1038/sdata.2016.18](https://doi.org/10.1038/sdata.2016.18)
    *(Articulates the widely adopted FAIR principles (Findable, Accessible, Interoperable, Reusable) that motivate modern archive design and data management practices discussed conceptually in Sec 7.1.)*
