**Chapter 22: Supervised Learning: Classification**

Following our exploration of regression for predicting continuous values, this chapter focuses on the other major category of supervised learning: **classification**. The goal of classification is to assign discrete **category labels** to input data points based on their features. This is one of the most common applications of machine learning in astrophysics, used for tasks like distinguishing stars from galaxies and quasars in survey images, identifying different morphological types of galaxies, classifying transient events found in time-domain data (e.g., supernova types), or identifying pulsar candidates in radio survey outputs. We will begin by defining the classification task (binary vs. multi-class) and then introduce several fundamental and widely used classification algorithms. We start with **Logistic Regression**, a linear model adapted for probabilistic binary classification. We then cover **Support Vector Machines (SVMs)**, powerful models that find optimal separating hyperplanes, often using kernels to handle non-linear boundaries. Subsequently, we revisit **Decision Trees** and **Random Forests**, demonstrating their application to classification problems by making predictions based on majority votes or probabilities in leaf nodes. Crucially, this chapter details the essential **evaluation metrics** specific to classification tasks, moving beyond simple accuracy to include the confusion matrix, precision, recall, F1-score, and the Receiver Operating Characteristic (ROC) curve with its associated Area Under the Curve (AUC), which are vital for assessing performance, especially on imbalanced datasets. Implementations using `scikit-learn` will be demonstrated throughout.

**22.1 Assigning Data to Categories**

Classification, within the supervised machine learning paradigm, deals with the problem of assigning predefined category labels to input data instances based on their measured features. Unlike regression (Chapter 21), which predicts a continuous numerical value, classification predicts a discrete output from a finite set of possible classes. The algorithm learns a decision boundary or a mapping function from labeled training data (where each instance has both input features `X` and a known class label `y`) that allows it to assign labels to new, unseen data points based solely on their features.

Astrophysics presents numerous classification challenges well-suited for ML. A classic example is **star-galaxy-quasar separation** in large imaging surveys like SDSS or LSST. Based on features derived from images (magnitudes, colors, concentration, morphology parameters), a classifier can be trained to label billions of detected sources automatically, a task impossible to perform manually at scale. Similarly, classifying galaxy morphologies into categories like **spiral, elliptical, lenticular, or irregular/merger** based on image features is another common application, often trained using labels from citizen science projects (like Galaxy Zoo) or expert classifications.

Time-domain astronomy heavily relies on classification. Identifying transient events detected in difference imaging requires classifying light curve alerts into categories like **supernova (with potential sub-types Ia, II, Ibc), variable star (pulsating, eclipsing, cataclysmic), AGN variability, or artifacts**. Searching for pulsars involves sifting through millions of signal candidates generated by processing radio data; classifiers are trained to distinguish promising **pulsar candidates** from ubiquitous **radio frequency interference (RFI)** based on features derived from the candidate signal properties (period, DM, pulse profile shape, signal-to-noise statistics). Classifying exoplanet transit candidates found by missions like Kepler or TESS as **'Planet Candidate' vs. 'False Positive'** (e.g., eclipsing binaries, background sources, instrumental effects) based on light curve shape and vetting metrics is another crucial application.

Classification problems are typically categorized as:
*   **Binary Classification:** There are only two possible output classes (e.g., 'Flare' vs 'No Flare', 'Pulsar' vs 'RFI', 'Planet' vs 'False Positive'). Often, one class is designated as "positive" (e.g., the rarer or more interesting class) and the other as "negative."
*   **Multi-class Classification:** There are three or more possible output classes, and each instance belongs to exactly one class (e.g., 'Star' vs 'Galaxy' vs 'Quasar'; 'Spiral' vs 'Elliptical' vs 'Irregular'; Spectral Type 'O', 'B', 'A', 'F', 'G', 'K', 'M').
*   **Multi-label Classification:** Each instance can be assigned *multiple* labels simultaneously (e.g., an image might contain both a 'galaxy' and a 'foreground star'). This is less common in basic astrophysical classification tasks compared to binary or multi-class problems and requires specialized algorithms or approaches. This chapter focuses primarily on binary and multi-class classification.

The input features `X` must generally be numerical, requiring preprocessing steps like scaling (Sec 20.2) and encoding of any categorical input features (Sec 20.3) before feeding them to most classification algorithms. The target labels `y`, although representing categories, are usually numerically encoded for use with libraries like `scikit-learn` (e.g., 0 and 1 for binary classification; 0, 1, 2, ... for multi-class). `sklearn.preprocessing.LabelEncoder` can be useful for encoding the target labels `y`.

Many classification algorithms do not directly output a hard class label but rather estimate the **probability** that an instance belongs to each class. For binary classification, the model might output P(y=1 | X), the probability of belonging to the positive class. A decision threshold (often 0.5 by default) is then applied to this probability to assign the final label (e.g., predict class 1 if P(y=1 | X) ≥ 0.5, otherwise predict class 0). Adjusting this threshold allows trading off between different types of errors (e.g., increasing recall at the cost of precision, see Sec 22.5). `scikit-learn` classifiers typically provide a `.predict_proba(X)` method to access these class probabilities alongside the `.predict(X)` method for hard labels based on the default threshold.

The choice of classification algorithm depends on factors like the linearity or non-linearity of the decision boundary between classes, the dimensionality of the feature space, the size of the dataset, the need for probabilistic outputs, and the desired interpretability. The following sections will introduce several foundational and widely used classification algorithms: Logistic Regression (linear), Support Vector Machines (linear or non-linear via kernels), and Decision Trees/Random Forests (non-linear, ensemble-based).

**22.2 Logistic Regression**

Despite its name containing "Regression," **Logistic Regression** is actually a fundamental and widely used algorithm for **binary classification**. It's a linear model, similar in form to linear regression, but adapted for predicting a categorical outcome (typically coded as 0 or 1) by modeling the *probability* of belonging to the positive class (class 1). It serves as an excellent baseline classifier due to its simplicity, interpretability, and computational efficiency.

Logistic Regression models the relationship between the input features `X = (x₁, x₂, ..., x<0xE1><0xB5><0x96>)` and the probability of the positive class P(y=1 | X) using the **logistic function** (also called the sigmoid function):

σ(z) = 1 / (1 + exp(-z))

The input to the sigmoid function, `z`, is a linear combination of the features, just like in linear regression:
z = θ₀ + θ₁x₁ + θ₂x₂ + ... + θ<0xE1><0xB5><0x96>x<0xE1><0xB5><0x96>

So, the model predicts the probability as:
P(y=1 | X, θ) = σ(z) = 1 / (1 + exp(-(θ₀ + θ₁x₁ + ... + θ<0xE1><0xB5><0x96>x<0xE1><0xB5><0x96>)))

The sigmoid function σ(z) conveniently squashes any real-valued input `z` into the range (0, 1), making its output suitable for interpretation as a probability. If `z` is large and positive, σ(z) approaches 1. If `z` is large and negative, σ(z) approaches 0. If `z` is 0, σ(z) is 0.5.

The parameters `θ = (θ₀, θ₁, ..., θ<0xE1><0xB5><0x96>)` are learned (fitted) from the training data. Unlike linear regression which minimizes the sum of squared residuals, Logistic Regression is typically trained by maximizing the **log-likelihood** based on the Bernoulli distribution (since the outcome is binary 0 or 1). This leads to minimizing a loss function called the **log loss** or **binary cross-entropy**. Although there's no closed-form analytical solution like OLS, the log-likelihood function for logistic regression is convex, meaning numerical optimization algorithms (like gradient descent variants or Newton's method) can reliably find the unique global maximum likelihood estimate for the parameters `θ`.

`scikit-learn` implements Logistic Regression in the `sklearn.linear_model.LogisticRegression` class. Key aspects of its usage include:
*   **Fitting:** `model = LogisticRegression(...).fit(X_train, y_train)`. `y_train` should contain binary labels (e.g., 0 and 1).
*   **Prediction:** `y_pred = model.predict(X_test)` returns the predicted class labels based on a default probability threshold of 0.5.
*   **Probability Estimation:** `y_proba = model.predict_proba(X_test)` returns an array of shape `[n_samples, 2]`, where `y_proba[:, 0]` is the estimated probability of class 0 and `y_proba[:, 1]` is the estimated probability of class 1. This is often more useful than the hard prediction, as the threshold can be adjusted based on the application's needs regarding precision vs. recall.
*   **Regularization:** `LogisticRegression` includes regularization by default to prevent overfitting, similar to Ridge/Lasso. The `penalty` argument ('l2' - Ridge default, 'l1' - Lasso, 'elasticnet') and the inverse regularization strength `C` (smaller C means stronger regularization) control this. Tuning `C` (often via cross-validation) is usually necessary. Since regularization is used, **feature scaling** (e.g., using `StandardScaler`, Sec 20.2) is highly recommended before training `LogisticRegression`.
*   **Multi-class:** While fundamentally binary, `scikit-learn`'s `LogisticRegression` can handle multi-class problems using strategies like One-vs-Rest (`ovr`, default) or Multinomial (setting `multi_class='multinomial'`).

Logistic Regression assumes a **linear decision boundary** between the classes in the feature space (or the transformed feature space if interactions/polynomials are added manually). The equation `z = θ₀ + θ₁x₁ + ... = 0` defines the hyperplane where the predicted probability is exactly 0.5. Data points on one side of this hyperplane are classified as class 1, and those on the other side as class 0. If the true boundary between classes is highly non-linear, Logistic Regression might perform poorly compared to more flexible models.

Despite its linearity assumption, Logistic Regression is very popular. It's computationally inexpensive to train, even on large datasets. The learned coefficients `θᵢ` provide some **interpretability**: the sign of `θᵢ` indicates whether increasing feature `xᵢ` increases or decreases the log-odds (logit) of belonging to the positive class, and the magnitude reflects the strength of that association, although interpretation requires care due to the non-linear sigmoid transformation to probability. It provides well-calibrated probability estimates if the assumptions hold reasonably well. It serves as an excellent baseline model against which more complex classifiers can be compared.

```python
# --- Code Example 1: Applying Logistic Regression ---
# Note: Requires scikit-learn installation.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Applying Logistic Regression for Binary Classification:")

# --- Simulate Data (Two roughly separable classes) ---
np.random.seed(1)
n_per_class = 100
# Class 0
X0 = np.random.multivariate_normal([1, 1], [[1, 0.5],[0.5, 1]], n_per_class)
y0 = np.zeros(n_per_class, dtype=int)
# Class 1
X1 = np.random.multivariate_normal([3, 3], [[1, -0.5],[-0.5, 1]], n_per_class)
y1 = np.ones(n_per_class, dtype=int)
# Combine
X = np.vstack((X0, X1))
y = np.concatenate((y0, y1))
print(f"\nGenerated 2D data for {len(np.unique(y))} classes.")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print(f"Split data into Train ({len(y_train)}) and Test ({len(y_test)}) sets.")

# --- Create and Fit Pipeline (Scaler + Logistic Regression) ---
# Scaling is important due to regularization in LogisticRegression
print("\nCreating and fitting pipeline (Scaler + LogisticRegression)...")
# Use default L2 penalty, default C=1.0
pipeline_logreg = make_pipeline(
    StandardScaler(),
    LogisticRegression(random_state=42)
)
pipeline_logreg.fit(X_train, y_train)
print("Pipeline fitted.")

# Access coefficients (after scaling)
# logreg_model = pipeline_logreg.named_steps['logisticregression']
# print(f"  Fitted Coefficients (theta1, theta2): {logreg_model.coef_}")
# print(f"  Fitted Intercept (theta0): {logreg_model.intercept_}")

# --- Predict and Evaluate ---
print("\nPredicting on test set and evaluating...")
y_pred = pipeline_logreg.predict(X_test)
y_proba = pipeline_logreg.predict_proba(X_test)[:, 1] # Probability of class 1

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"  Test Accuracy: {accuracy:.4f}")
print("  Confusion Matrix:\n", conf_matrix)
print("  Classification Report:\n", class_report)
# print(f"  Sample Probabilities (Class 1): {y_proba[:5].round(3)}")

# --- Visualize Decision Boundary (for 2D data) ---
print("\nGenerating decision boundary plot...")
fig, ax = plt.subplots(figsize=(7, 6))
# Plot data points
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', alpha=0.7, edgecolors='k')
ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
ax.set_title("Logistic Regression Decision Boundary")
legend1 = ax.legend(*scatter.legend_elements(), title="Classes")
ax.add_artist(legend1)

# Create a mesh grid to plot decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                     np.linspace(y_min, y_max, 200))
# Predict probabilities on the grid (need to apply scaler from pipeline)
grid_points = np.c_[xx.ravel(), yy.ravel()]
scaler = pipeline_logreg.named_steps['standardscaler']
grid_points_scaled = scaler.transform(grid_points)
Z_proba = pipeline_logreg.named_steps['logisticregression'].predict_proba(grid_points_scaled)[:, 1]
Z_proba = Z_proba.reshape(xx.shape)
# Plot contour where probability = 0.5 (decision boundary)
ax.contour(xx, yy, Z_proba, levels=[0.5], colors='black', linestyles='--')
# Optional: plot probability contours
# contourf = ax.contourf(xx, yy, Z_proba, levels=np.linspace(0, 1, 11), cmap="RdBu_r", alpha=0.4)
# fig.colorbar(contourf, label='Probability of Class 1')

fig.tight_layout()
# plt.show()
print("Plot generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This code trains a Logistic Regression classifier on simulated 2D data.
# 1. It generates two classes of points roughly separable by a line.
# 2. It uses a `Pipeline` to combine `StandardScaler` and `LogisticRegression`.
# 3. The pipeline is fitted on the training data.
# 4. Predictions (`.predict`) and probabilities (`.predict_proba`) are made on the test set.
# 5. Performance is evaluated using accuracy, confusion matrix, and classification report.
# 6. A plot is generated showing the test data points colored by true class. Crucially, 
#    it also calculates the model's predicted probability across a grid of points 
#    (applying the *same* scaling fitted on the training data) and draws the contour 
#    line where the predicted probability is 0.5. This contour represents the linear 
#    decision boundary learned by the Logistic Regression model.
```

In summary, Logistic Regression provides a simple, interpretable, and computationally efficient baseline for binary (and multi-class) classification problems. It models the probability of class membership using a linear combination of features passed through a sigmoid function and includes regularization to prevent overfitting. While limited by its assumption of a linear decision boundary, it's often a valuable first model to try and performs well when the classes are reasonably separable by a hyperplane in the feature space.

**22.3 Support Vector Machines (SVM)**

**Support Vector Machines (SVMs)** are a powerful and versatile class of supervised learning algorithms used for both classification and regression (SVR, Sec 21.3). For classification, SVMs aim to find the "best" boundary or **hyperplane** that separates different classes in the feature space. What makes SVMs powerful is their ability to find non-linear boundaries using the **kernel trick** and their focus on maximizing the **margin** between classes, which often leads to good generalization performance.

Consider a binary classification problem with two classes. A linear SVM seeks to find the hyperplane (a line in 2D, a plane in 3D, or a hyperplane in higher dimensions) that best separates the data points belonging to the two classes. The "best" hyperplane is defined as the one that has the **maximum margin**, where the margin is the distance between the hyperplane and the closest data points from *either* class. These closest points, which lie on the edge of the margin, are called the **support vectors**. The decision boundary is determined solely by these support vectors; other data points further away from the boundary do not influence it. This focus on the boundary points makes SVMs relatively robust to outliers that are far from the decision boundary.

Mathematically, finding the maximum margin hyperplane involves solving a constrained quadratic optimization problem. The objective is to maximize the margin width (which is equivalent to minimizing the norm of the weight vector `w` defining the hyperplane `w·x - b = 0`) subject to the constraint that all data points `xᵢ` with label `yᵢ` (+1 or -1) lie on the correct side of the margin: `yᵢ(w·xᵢ - b) ≥ 1`.

In cases where the data is not perfectly linearly separable, SVMs introduce **slack variables** (often denoted ξ<0xE1><0xB5><0xA2>) that allow some data points to fall within the margin or even on the wrong side of the hyperplane (misclassifications). The optimization objective then includes a penalty term for these violations, controlled by a regularization parameter `C`. A large `C` imposes a high penalty for margin violations, leading to a "hard margin" SVM that tries to classify all training points correctly (potentially overfitting). A smaller `C` allows more margin violations, leading to a "soft margin" SVM with a wider margin and potentially better generalization (less overfitting). `C` acts as an inverse regularization strength (similar to `1/α` in Ridge/Lasso).

The real power of SVMs comes from the **kernel trick**. Instead of finding a linear boundary in the original feature space, SVMs can use kernel functions to implicitly map the data into a much higher (potentially infinite) dimensional feature space where a linear separation might be possible, even if the classes were non-linearly separable in the original space. This mapping occurs implicitly through the calculation of dot products in the high-dimensional space via the kernel function, avoiding the computationally prohibitive task of explicitly transforming the data. Common kernel functions include:
*   `'linear'`: `K(xᵢ, xⱼ) = xᵢ · xⱼ`. Recovers the linear SVM.
*   `'poly'`: `K(xᵢ, xⱼ) = (γ * xᵢ · xⱼ + r)ᵈ`. Polynomial kernel of degree `d`. `gamma` (γ) and `coef0` (r) are hyperparameters.
*   `'rbf'` (Radial Basis Function / Gaussian Kernel): `K(xᵢ, xⱼ) = exp(-γ * ||xᵢ - xⱼ||²)`. A very powerful and popular default choice, capable of creating complex, non-linear decision boundaries. The `gamma` (γ > 0) hyperparameter controls the influence of a single training example; small gamma means far influence (smoother boundary), large gamma means close influence (more complex, potentially wiggly boundary).
*   `'sigmoid'`: `K(xᵢ, xⱼ) = tanh(γ * xᵢ · xⱼ + r)`. Sigmoid kernel.

`scikit-learn` implements SVM classification in the `sklearn.svm.SVC` class (Support Vector Classifier). Key hyperparameters include:
*   `C`: The regularization parameter (positive float, default 1.0). Controls the penalty for margin violations.
*   `kernel`: Type of kernel ('linear', 'poly', 'rbf', 'sigmoid', default 'rbf').
*   `gamma` (for 'rbf', 'poly', 'sigmoid'): Kernel coefficient (default 'scale').
*   `degree` (for 'poly'): Degree of the polynomial kernel (default 3).
*   `probability=True` (optional, default False): If set to True, enables calculation of class probabilities using Platt scaling after fitting, accessible via `.predict_proba()`. This incurs extra computational cost during fitting.
*   `class_weight` (optional): Can be set to `'balanced'` or a dictionary to handle imbalanced datasets (Sec 20.5).

Feature scaling (e.g., using `StandardScaler`, Sec 20.2) is **highly recommended** and often crucial for SVMs, especially when using kernels like 'rbf' or 'poly', as they rely on calculating distances or dot products between feature vectors. Features with larger ranges could otherwise dominate these calculations.

```python
# --- Code Example 1: Applying Support Vector Classifier (SVC) ---
# Note: Requires scikit-learn installation.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report, confusion_matrix

print("Applying Support Vector Classifier (SVC):")

# --- Simulate Data (Two classes, potentially non-linearly separable) ---
np.random.seed(10)
n_per_class = 100
# Class 0: Inner circle
radius0 = np.random.uniform(0, 2, n_per_class)
angle0 = np.random.uniform(0, 2*np.pi, n_per_class)
X0 = np.vstack((radius0 * np.cos(angle0), radius0 * np.sin(angle0))).T
y0 = np.zeros(n_per_class, dtype=int)
# Class 1: Outer ring
radius1 = np.random.uniform(3, 5, n_per_class)
angle1 = np.random.uniform(0, 2*np.pi, n_per_class)
X1 = np.vstack((radius1 * np.cos(angle1), radius1 * np.sin(angle1))).T
y1 = np.ones(n_per_class, dtype=int)
# Combine
X = np.vstack((X0, X1))
y = np.concatenate((y0, y1))
print(f"\nGenerated 2D data (two concentric rings).")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# --- Create and Fit Pipeline (Scaler + SVC with RBF kernel) ---
print("\nCreating and fitting pipeline (Scaler + SVC(kernel='rbf'))...")
# Use default C=1.0, gamma='scale'
pipeline_svc = make_pipeline(
    StandardScaler(), # Scaling is crucial for RBF kernel
    SVC(kernel='rbf', probability=True, random_state=42) # Enable probability estimates
)
pipeline_svc.fit(X_train, y_train)
print("Pipeline fitted.")

# --- Predict and Evaluate ---
print("\nPredicting on test set and evaluating...")
y_pred_svc = pipeline_svc.predict(X_test)
accuracy_svc = pipeline_svc.score(X_test, y_test) # .score gives accuracy for classifiers

print(f"  Test Accuracy: {accuracy_svc:.4f}")
print("  Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svc))
print("  Classification Report:\n", classification_report(y_test, y_pred_svc))

# --- Visualize Decision Boundary ---
print("\nGenerating decision boundary plot...")
fig, ax = plt.subplots(figsize=(7, 6))
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', alpha=0.7, edgecolors='k')
# Create mesh grid
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
# Predict class on the grid using the pipeline
Z = pipeline_svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# Plot decision boundary contours
ax.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.3)
# Highlight support vectors (optional, access via fitted SVC model)
# svc_model = pipeline_svc.named_steps['svc']
# ax.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1],
#            s=100, facecolors='none', edgecolors='k', label='Support Vectors')

ax.set_xlabel("Feature 1"); ax.set_ylabel("Feature 2")
ax.set_title(f"SVC Decision Boundary (RBF Kernel)")
# ax.legend() # Legend for support vectors if plotted
fig.tight_layout()
# plt.show()
print("Plot generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This code demonstrates SVC for a non-linearly separable problem (concentric rings).
# 1. It generates 2D data for two classes forming rings.
# 2. It uses a `Pipeline` with `StandardScaler` (essential for RBF) and `SVC`. 
#    The `kernel='rbf'` allows the SVM to find a non-linear boundary. `probability=True` 
#    is enabled, though not used in this example's evaluation.
# 3. The pipeline is fitted on the training data.
# 4. Predictions are made on the test set and evaluated using accuracy, confusion matrix, 
#    and classification report. SVC with RBF is expected to perform well here.
# 5. The plot shows the test data and the decision boundary learned by the SVC. 
#    The `contourf` plot fills regions based on the predicted class across the grid, 
#    visually showing the non-linear separation found by the RBF kernel. Support 
#    vectors (points near the boundary) could optionally be highlighted.
```

SVMs, particularly with the RBF kernel, are powerful "out-of-the-box" classifiers capable of handling complex, non-linear decision boundaries. They often achieve high accuracy. Their reliance on support vectors provides some robustness to outliers far from the boundary. However, like SVR, they can be sensitive to hyperparameter choices (`C`, `gamma` for RBF), requiring careful tuning via cross-validation. They can also become computationally expensive to train on very large datasets (scaling worse than linear). Furthermore, interpreting SVM models with non-linear kernels is challenging – it's difficult to understand the contribution of individual features in the implicitly high-dimensional space. Despite these drawbacks, SVMs remain a valuable tool for many astrophysical classification problems where high accuracy on complex datasets is desired.

**22.4 Decision Trees and Random Forests for Classification**

Decision Trees and Random Forests, introduced in Section 21.4 for regression, are also highly effective and widely used algorithms for **classification** tasks. They work by partitioning the feature space recursively based on simple tests on feature values, ultimately assigning a class label based on the majority vote (or probability) within the final partitions (leaves). Their ability to handle non-linear relationships and feature interactions without requiring feature scaling makes them very popular.

A **Decision Tree Classifier** builds a tree structure where each internal node splits the data based on a threshold applied to a single feature (e.g., `color < 0.5`, `concentration > 2.5`). The goal at each split is to choose the feature and threshold that best separate the classes in the resulting child nodes, typically by maximizing **information gain** or minimizing an **impurity measure** like the **Gini impurity** or **entropy**. Gini impurity measures the probability of misclassifying a randomly chosen element from a node if it were randomly labeled according to the class distribution within that node (lower is better, 0 for a pure node). Entropy measures the disorder or uncertainty in the class labels within a node (lower is better).

Data points filter down the tree until they reach a **leaf node**. For classification, each leaf node contains training samples predominantly belonging to one class. The prediction for a new data point reaching that leaf is typically the **majority class** among the training samples in that leaf. Alternatively, the tree can predict the *probability* of belonging to each class based on the fraction of training samples of each class within the leaf. `sklearn.tree.DecisionTreeClassifier` implements this algorithm.

As with regression trees, single classification trees are easy to interpret visually but are highly prone to **overfitting**. They can grow very deep, creating complex decision boundaries that perfectly classify the training data (including noise) but fail to generalize to unseen data. Pruning the tree (e.g., by limiting `max_depth`, setting `min_samples_split`, or `min_samples_leaf`) is necessary to control complexity and improve generalization, but finding the optimal pruning level can be tricky.

**Random Forests** mitigate the overfitting problem of single trees through ensemble averaging. A `RandomForestClassifier` builds an ensemble (a "forest") of many individual decision trees (`n_estimators`). Each tree is trained on a different bootstrap sample (random sample with replacement) of the training data. Additionally, at each split within each tree, only a random subset of features (`max_features`) is considered. This combined randomization (bagging + feature subspace sampling) decorrelates the individual trees in the forest.

To classify a new data point, the Random Forest passes it down *all* the trees in the ensemble. Each tree independently predicts a class label (or class probabilities). The final prediction of the Random Forest is typically the **majority vote** among the predictions of all individual trees (for hard classification) or the **average of the predicted probabilities** across all trees (if using `.predict_proba()`). This averaging process dramatically reduces the variance of the prediction, leading to a model that is much more robust, less prone to overfitting, and generally achieves higher accuracy than a single optimized decision tree.

`sklearn.ensemble.RandomForestClassifier` implements this. Key hyperparameters are similar to the regressor: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`. It also includes `class_weight='balanced'` or `'balanced_subsample'` options to handle imbalanced datasets effectively during the training of individual trees. Random Forests naturally handle multi-class classification problems.

Like their regression counterparts, Random Forest Classifiers offer several advantages: high accuracy, robustness to outliers and noise, ability to capture complex non-linearities and feature interactions, insensitivity to feature scaling, and providing useful **feature importance** scores (`.feature_importances_`) based on how much each feature contributes to reducing impurity across the forest. They are often a strong "go-to" algorithm for many classification tasks.

Drawbacks include reduced interpretability compared to a single tree or linear model (it's hard to trace exactly why the forest made a specific prediction), and potentially higher computational cost and memory usage compared to simpler models, especially for very large numbers of trees or deep trees.

```python
# --- Code Example 1: Applying Decision Tree and Random Forest Classifiers ---
# Note: Requires scikit-learn installation.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import make_moons # For non-linearly separable data

print("Comparing Decision Tree and Random Forest for Classification:")

# --- Simulate Non-linear Data (Two Moons) ---
X, y = make_moons(n_samples=300, noise=0.3, random_state=42)
print(f"\nGenerated 2D 'moons' data for binary classification.")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# --- Fit Decision Tree Classifier ---
# Allow deep tree initially
print("\nFitting Decision Tree Classifier (max_depth=None)...")
tree_clf = DecisionTreeClassifier(max_depth=None, random_state=42) 
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)
acc_tree = accuracy_score(y_test, y_pred_tree)
print(f"  Decision Tree Test Accuracy: {acc_tree:.4f}")
# print("  Decision Tree Report:\n", classification_report(y_test, y_pred_tree))

# --- Fit Random Forest Classifier ---
print("\nFitting Random Forest Classifier (n_estimators=100)...")
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) 
rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"  Random Forest Test Accuracy: {acc_rf:.4f}")
# print("  Random Forest Report:\n", classification_report(y_test, y_pred_rf))
print("  (Random Forest typically has higher accuracy/better generalization)")

# --- Visualize Decision Boundaries ---
print("\nGenerating decision boundary plots...")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
# Function to plot boundary
def plot_decision_boundary(clf, X, y, ax, title):
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.6, edgecolors='k')
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    ax.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.3)
    ax.set_xlabel("Feature 1"); ax.set_ylabel("Feature 2"); ax.set_title(title)

plot_decision_boundary(tree_clf, X_test, y_test, ax1, f"Decision Tree (Acc={acc_tree:.3f})")
plot_decision_boundary(rf_clf, X_test, y_test, ax2, f"Random Forest (Acc={acc_rf:.3f})")
fig.tight_layout()
# plt.show()
print("Plots generated.")
plt.close(fig)
print("-" * 20)

# Explanation: This code compares Decision Tree and Random Forest classifiers on the 
# non-linearly separable 'make_moons' dataset.
# 1. It fits a single `DecisionTreeClassifier` allowing unlimited depth.
# 2. It fits a `RandomForestClassifier` with 100 trees.
# 3. It calculates and prints the accuracy on the test set for both. The Random Forest 
#    is expected to perform better due to reduced overfitting.
# 4. It defines a helper function `plot_decision_boundary` to visualize the separation 
#    learned by each classifier.
# 5. It generates plots showing the test data points and the decision regions (shaded areas) 
#    for both classifiers. The Decision Tree boundary often looks jagged and complex 
#    (overfitting), while the Random Forest boundary is typically smoother and generalizes better.
```

Random Forests are often a very strong choice for classification problems in astrophysics, providing high accuracy with relatively little hyperparameter tuning compared to SVMs or neural networks, handling various feature types well (though `scikit-learn` requires numerical inputs), and offering robustness and feature importance insights. Their ensemble nature makes them less prone to overfitting than single decision trees.

**22.5 Evaluating Classification Models**

Evaluating the performance of a classification model is critical for understanding its effectiveness and comparing different approaches. Unlike regression where metrics focus on the magnitude of prediction errors, classification metrics assess how well the model assigns the correct category labels. Simply using **accuracy** – the overall fraction of correctly classified instances ( (TP + TN) / Total ) – can be highly misleading, especially for **imbalanced datasets** (Sec 20.5), where a model might achieve high accuracy by just predicting the majority class. Therefore, a suite of metrics, often derived from the **confusion matrix**, is essential for a comprehensive evaluation.

The **Confusion Matrix** is a table that summarizes the performance of a classifier on the test set. For a binary classification problem (with classes Positive/1 and Negative/0), it has four entries:
*   **True Positives (TP):** Number of actual Positive instances correctly predicted as Positive.
*   **True Negatives (TN):** Number of actual Negative instances correctly predicted as Negative.
*   **False Positives (FP) (Type I Error):** Number of actual Negative instances incorrectly predicted as Positive (a "false alarm").
*   **False Negatives (FN) (Type II Error):** Number of actual Positive instances incorrectly predicted as Negative (a "miss").

```
                 Predicted Negative   Predicted Positive
Actual Negative          TN                 FP
Actual Positive          FN                 TP 
```
`scikit-learn.metrics.confusion_matrix(y_true, y_pred)` calculates this matrix.

From the confusion matrix, several key performance metrics are derived:
*   **Accuracy:** `(TP + TN) / (TP + TN + FP + FN)`. Overall fraction correct. Misleading for imbalanced data.
*   **Precision (or Positive Predictive Value):** `TP / (TP + FP)`. Of all instances predicted as Positive, what fraction actually *are* Positive? Measures the reliability of positive predictions. High precision is crucial when the cost of a False Positive is high (e.g., triggering expensive follow-up observations based on a false candidate).
*   **Recall (or Sensitivity, True Positive Rate, Completeness):** `TP / (TP + FN)`. Of all actual Positive instances, what fraction did the model correctly identify? Measures the model's ability to find all positive instances. High recall is crucial when the cost of a False Negative is high (e.g., missing a rare supernova or a real gravitational wave signal).
*   **Specificity (or True Negative Rate):** `TN / (TN + FP)`. Of all actual Negative instances, what fraction did the model correctly identify?
*   **F1-score:** `2 * (Precision * Recall) / (Precision + Recall)`. The harmonic mean of Precision and Recall. Provides a single balanced measure, useful when both Precision and Recall are important. It penalizes models that achieve high performance in one metric at the expense of the other.

`sklearn.metrics.classification_report(y_true, y_pred)` conveniently calculates Precision, Recall, F1-score, and support (number of true instances) for *each class*, along with overall accuracy and averaged metrics (macro avg, weighted avg). This report is essential for understanding class-specific performance, especially with imbalanced data.

Many classifiers (like Logistic Regression, SVM with `probability=True`, Random Forests) output class **probabilities** rather than just hard labels. By default, `predict` uses a threshold of 0.5 on the probability of the positive class to assign labels. However, this threshold can be adjusted to trade off Precision versus Recall. Lowering the threshold increases Recall (finds more positives) but typically decreases Precision (more false positives). Increasing the threshold increases Precision but decreases Recall.

The **Receiver Operating Characteristic (ROC) Curve** visualizes this trade-off. It plots the **True Positive Rate (Recall)** against the **False Positive Rate (FPR = FP / (TN + FP) = 1 - Specificity)** at various classification thresholds. A good classifier will have an ROC curve that bows towards the top-left corner (high TPR, low FPR). A random classifier corresponds to the diagonal line (TPR = FPR). The **Area Under the ROC Curve (AUC)** provides a single scalar measure summarizing the classifier's performance across all thresholds. AUC = 1 represents a perfect classifier, while AUC = 0.5 represents a random classifier. AUC is a very useful metric, especially for imbalanced datasets, as it measures the overall ability of the model to discriminate between the positive and negative classes, independent of a specific chosen threshold. `sklearn.metrics.roc_curve(y_true, y_pred_proba)` calculates the TPR and FPR values for plotting, and `sklearn.metrics.roc_auc_score(y_true, y_pred_proba)` calculates the AUC value directly from the true labels and predicted probabilities for the positive class.

```python
# --- Code Example 1: Calculating Classification Metrics ---
# Note: Requires scikit-learn installation.
# Assume y_test (true labels) and y_pred (predicted labels) exist from a classifier
# Also assume y_proba (predicted probabilities for class 1) exist if classifier supports it

from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, 
                             roc_curve, roc_auc_score, ConfusionMatrixDisplay)
import matplotlib.pyplot as plt
import numpy as np # For simulating

print("Calculating and Interpreting Classification Metrics:")

# --- Simulate some true labels and predictions/probabilities ---
np.random.seed(42)
y_test = np.random.randint(0, 2, 100) # True binary labels
# Simulate probabilities, maybe slightly better than random
y_proba = y_test * 0.6 + 0.2 + np.random.rand(100)*0.4 
y_proba = np.clip(y_proba, 0.01, 0.99) # Ensure probs in [0,1] range
# Get hard predictions using 0.5 threshold
y_pred = (y_proba >= 0.5).astype(int)
print("\nGenerated simulated y_test, y_pred, y_proba.")

# --- Calculate Core Metrics ---
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred, target_names=['Class 0 (Neg)', 'Class 1 (Pos)'])

print(f"\nAccuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
# Can visualize confusion matrix
# disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Neg', 'Pos'])
# disp.plot()
# plt.show()

print("\nClassification Report:")
print(class_report) 
# Note Precision, Recall, F1-score per class

# --- Calculate ROC Curve and AUC ---
# Requires predicted probabilities for the positive class (class 1)
try:
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    auc_score = roc_auc_score(y_test, y_proba)
    print(f"\nArea Under ROC Curve (AUC): {auc_score:.4f}")

    # Plot ROC Curve
    print("Generating ROC Curve plot...")
    plt.figure(figsize=(6, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (1 - Specificity)')
    plt.ylabel('True Positive Rate (Recall / Sensitivity)')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.4)
    # plt.show()
    print("ROC Curve plot generated.")
    plt.close()

except ValueError as e_roc:
     print(f"\nCould not calculate ROC/AUC (requires probabilities or continuous scores): {e_roc}")
except Exception as e:
     print(f"\nError calculating ROC/AUC: {e}")

print("-" * 20)

# Explanation: This code demonstrates calculating various classification metrics.
# 1. It simulates true labels `y_test`, predicted probabilities `y_proba`, and hard 
#    predictions `y_pred` based on a 0.5 threshold.
# 2. It calculates overall `accuracy_score`.
# 3. It calculates the `confusion_matrix` (TN, FP, FN, TP).
# 4. It generates the `classification_report` which provides precision, recall, F1-score, 
#    and support for each class, along with averages. This is crucial for imbalanced data.
# 5. It calculates the False Positive Rates (`fpr`), True Positive Rates (`tpr`), and 
#    thresholds needed to plot the ROC curve using `roc_curve` (requires `y_proba`).
# 6. It calculates the Area Under the ROC Curve (`roc_auc_score`), a single metric 
#    summarizing performance across thresholds (also requires `y_proba`).
# 7. It generates the ROC curve plot, showing TPR vs FPR, comparing the classifier's 
#    performance to a random guess (diagonal line).
```

For **multi-class classification**, the concepts extend. The confusion matrix becomes NxN (where N is the number of classes), showing correct and incorrect classifications between all pairs of classes. Precision, Recall, and F1-score can be calculated *per class* (treating each class as "positive" versus all others) or averaged across classes (e.g., "macro average" treats all classes equally, "weighted average" weights by the number of true instances per class). ROC curves and AUC can also be extended to multi-class problems (e.g., using One-vs-Rest or One-vs-One strategies), though interpretation becomes more complex. `scikit-learn`'s metrics functions generally support multi-class evaluation.

Choosing the most relevant metric depends on the scientific goal. If detecting *all* instances of a rare event is paramount (even at the cost of some false alarms), **Recall** is key. If ensuring that predicted candidates are highly likely to be real is most important, **Precision** is prioritized. **F1-score** provides a balance. **AUC** gives a good overall measure of discriminative ability across different operating thresholds, particularly useful for comparing models independent of a specific threshold choice and for imbalanced data. Analyzing the full **classification report** and **confusion matrix** provides the most comprehensive picture of a classifier's strengths and weaknesses across all classes.

**22.6 Implementation (`train_test_split`, `.fit()`, `.predict()`, `.predict_proba()`, evaluation)**

This section summarizes the typical end-to-end implementation workflow for a supervised classification problem using `scikit-learn`, consolidating the steps discussed throughout Part IV so far, mirroring the regression workflow outlined in Section 21.6 but highlighting classification-specific aspects like probability prediction and evaluation metrics.

**Step 1: Load and Prepare Data:** Load the dataset (e.g., into a Pandas DataFrame). Identify the feature columns (X) and the target class label column (y). Perform necessary cleaning, feature engineering (Sec 20.4), handle missing values (Sec 20.1), and encode categorical features (Sec 20.3). Ensure X is a numerical matrix and y is a vector of encoded class labels (e.g., 0/1 for binary, 0/1/2... for multi-class).

**Step 2: Split Data into Training and Test Sets:** Use `sklearn.model_selection.train_test_split` to partition X and y into `X_train`, `X_test`, `y_train`, `y_test`. Crucially, use the `stratify=y` option if dealing with potentially imbalanced classes; this ensures that the proportion of each class in the training and test sets mimics the proportion in the original dataset, preventing biased evaluation due to skewed splits. Set `random_state` for reproducibility.

**Step 3: Preprocessing (Fit on Train, Transform Both):** Apply feature scaling (`StandardScaler` or `MinMaxScaler`, Sec 20.2) if required by the chosen classifier (e.g., Logistic Regression, SVM). Fit the scaler *only* on `X_train` and transform *both* `X_train` and `X_test`. Using a `Pipeline` (Sec 20.6) is the recommended way to bundle scaling and other preprocessing (like imputation or encoding if done via sklearn) with the classifier to ensure correct application during training, prediction, and cross-validation.

**Step 4: Choose and Instantiate Classifier:** Select the classification algorithm(s) (e.g., `LogisticRegression`, `SVC`, `DecisionTreeClassifier`, `RandomForestClassifier`). Instantiate the chosen model class(es), potentially setting hyperparameters. If using a pipeline, the classifier is the final step. For imbalanced datasets, consider using `class_weight='balanced'` during instantiation if the algorithm supports it (Sec 20.5).

**Step 5: Train (Fit) the Classifier/Pipeline:** Train the model or pipeline using *only* the training data: `model.fit(X_train, y_train)` or `pipeline.fit(X_train, y_train)`. (Note: If resampling techniques like SMOTE are used from `imblearn`, they are applied *before* this step to generate a modified `X_train_resampled`, `y_train_resampled` which is then passed to `.fit()`).

**Step 6: Make Predictions on Test Set:** Use the fitted model/pipeline to generate predictions for the held-out test set. There are typically two types of predictions:
*   **Hard Labels:** `y_pred = model.predict(X_test)` (or `pipeline.predict(X_test)`). This assigns the most likely class label based on the model's internal logic or a default threshold (usually 0.5 for binary probabilities).
*   **Probabilities:** `y_proba = model.predict_proba(X_test)` (or `pipeline.predict_proba(X_test)`). This returns an array of shape `[n_test_samples, n_classes]` containing the estimated probability for each class for each test sample. This is available for most classifiers (though might require `probability=True` during `SVC` instantiation) and is needed for calculating ROC/AUC metrics or for applications where probabilistic outputs are desired. For binary classification, `y_proba[:, 1]` gives the probability of the positive class (class 1).

**Step 7: Evaluate Classifier Performance:** Compare the predictions (`y_pred`) and/or probabilities (`y_proba`) with the true test labels (`y_test`) using appropriate classification metrics (Sec 22.5). Key metrics include:
*   `accuracy_score(y_test, y_pred)`
*   `confusion_matrix(y_test, y_pred)` (visualize with `ConfusionMatrixDisplay`)
*   `classification_report(y_test, y_pred)` (shows precision, recall, F1 per class)
*   `roc_auc_score(y_test, y_proba[:, 1])` (for binary classification)
*   Plot the ROC curve using `roc_curve(y_test, y_proba[:, 1])`.
Choose metrics appropriate for the specific problem goals and any class imbalance.

**Step 8 (Optional): Hyperparameter Tuning and Model Selection:** If needed, use cross-validation (`cross_val_score` with appropriate scoring like 'accuracy', 'f1', 'roc_auc') or grid search (`GridSearchCV`) on the *training set* to optimize hyperparameters or compare different models based on their average validation performance, before final evaluation on the test set.

```python
# --- Code Example: Consolidated Classification Workflow ---
# Note: Requires scikit-learn, pandas, matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier # Using RF as example
from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay

print("Consolidated Classification Workflow Example:")

# Step 1: Load/Prepare Data (Using previous simulation)
np.random.seed(1)
n_per_class=100; X0=np.random.multivariate_normal([1,1],[[1,0.5],[0.5,1]],n_per_class); y0=np.zeros(n_per_class,dtype=int)
X1=np.random.multivariate_normal([3,3],[[1,-0.5],[-0.5,1]],n_per_class); y1=np.ones(n_per_class,dtype=int)
X=np.vstack((X0,X1)); y=np.concatenate((y0,y1))
print(f"\nGenerated data: X shape={X.shape}, y shape={y.shape}")

# Step 2: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print(f"Split data: Train={len(y_train)}, Test={len(y_test)}")

# Step 3 & 4: Define Pipeline with Preprocessing and Classifier
# Pipeline: Scale features -> Random Forest Classifier
pipeline = make_pipeline(
    StandardScaler(),
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)
print(f"\nPipeline defined:\n{pipeline}")

# Step 5: Train Pipeline
print("\nFitting pipeline on training data...")
pipeline.fit(X_train, y_train)
print("Fitting complete.")

# Step 6: Predict on Test Set
print("\nPredicting on test set...")
y_pred = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)[:, 1] # Probabilities for class 1

# Step 7: Evaluate Classifier
print("\nEvaluating performance on test set:")
print("Classification Report:")
print(classification_report(y_test, y_pred))
try:
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC Score: {auc:.4f}")

    # Plot ROC Curve using scikit-learn's display function
    fig, ax = plt.subplots(figsize=(6, 6))
    roc_display = RocCurveDisplay.from_predictions(y_test, y_proba, name='Random Forest', ax=ax)
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    ax.set_title('ROC Curve')
    ax.grid(True, alpha=0.4)
    # plt.show()
    print("ROC curve plot generated.")
    plt.close(fig)
    
except ValueError as e_roc:
     print(f"\nCould not calculate ROC/AUC: {e_roc}")
except Exception as e:
     print(f"\nError during ROC/AUC calculation or plot: {e}")

# Step 8 (Conceptual): Cross-validation on training set for tuning/comparison
# print("\nConceptual cross-validation on training set:")
# cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')
# print(f"  Mean CV AUC on training data: {np.mean(cv_scores):.3f}")

print("-" * 20)

# Explanation: This code demonstrates the full classification workflow using a Pipeline.
# 1. Simulates 2-class data.
# 2. Splits data using `train_test_split` with stratification.
# 3. Creates a `Pipeline` using `make_pipeline` combining `StandardScaler` and 
#    `RandomForestClassifier`.
# 4. Fits the pipeline on the training data (`X_train`, `y_train`).
# 5. Makes both hard predictions (`.predict`) and probability predictions 
#    (`.predict_proba`) on the test set (`X_test`).
# 6. Evaluates performance using `classification_report` (precision, recall, f1 per class) 
#    and `roc_auc_score` (overall discrimination).
# 7. Plots the ROC curve using `RocCurveDisplay.from_predictions` for easy visualization.
# 8. Conceptually mentions how cross-validation would be applied to the pipeline 
#    on the training set for model selection or tuning purposes.
```

This structured implementation process, facilitated by `scikit-learn`'s consistent API and tools like `Pipeline` and `train_test_split`, enables robust development and evaluation of classification models. Following these steps helps ensure that preprocessing is applied correctly, evaluation is performed on unseen data, and results provide a reliable estimate of the model's ability to classify new instances accurately.

**Application 22.A: Star-Galaxy-QSO Classification using Colors**

**Objective:** This application demonstrates a practical multi-class classification task: classifying astronomical sources detected in a survey as either Stars, Galaxies, or Quasars (QSOs) based on their observed photometric colors. We will use a Random Forest Classifier (Sec 22.4) within a `scikit-learn` workflow, including data splitting and evaluation using appropriate multi-class metrics like the classification report and confusion matrix (Sec 22.5).

**Astrophysical Context:** Large imaging surveys like the Sloan Digital Sky Survey (SDSS) detect billions of sources. A fundamental step is classifying these sources. Stars, being point-like and having specific colors related to their temperature, generally occupy distinct regions in color-color or color-magnitude space compared to extended galaxies (whose colors depend on stellar population and redshift) or distant, highly luminous quasars (which often exhibit unique color signatures due to their non-stellar spectra and high redshifts). While morphology helps for resolved objects, color information is available even for point-like sources and is a powerful discriminant. Training a classifier on a subset of objects with known spectroscopic classifications allows for rapid, automated classification of the entire photometric catalog.

**Data Source:** A catalog derived from a survey like SDSS (`sdss_phot_class.csv`), containing photometric measurements (e.g., ugriz magnitudes) and a reliable spectroscopic classification ('STAR', 'GALAXY', 'QSO') for a representative subset of objects. This serves as our labeled training and testing data.

**Modules Used:** `pandas` (for data loading/handling), `numpy`, `sklearn.model_selection.train_test_split`, `sklearn.ensemble.RandomForestClassifier`, `sklearn.preprocessing.LabelEncoder` (to encode target labels), `sklearn.metrics` (`classification_report`, `confusion_matrix`, `accuracy_score`, `ConfusionMatrixDisplay`), `matplotlib.pyplot` (for confusion matrix visualization). Feature scaling (`StandardScaler`) might be considered, although Random Forests are less sensitive to it than linear models or SVMs.

**Technique Focus:** Implementing a multi-class classification workflow (Sec 22.1, 22.6). Engineering color indices as features (Sec 20.4 concept). Encoding the categorical target labels ('STAR', 'GALAXY', 'QSO') into numerical values (0, 1, 2) using `LabelEncoder`. Training a `RandomForestClassifier` (which naturally handles multi-class problems). Evaluating performance using multi-class metrics: overall accuracy, per-class precision/recall/F1-score via `classification_report`, and visualizing misclassifications with `confusion_matrix` (or `ConfusionMatrixDisplay`).

**Processing Step 1: Load Data and Feature Engineering:** Load the SDSS data into a Pandas DataFrame. Create standard color indices from the magnitudes: `u_g = df['u'] - df['g']`, `g_r = df['g'] - df['r']`, `r_i = df['r'] - df['i']`, `i_z = df['i'] - df['z']`. Define the feature matrix `X` using these colors (and potentially magnitudes or other features like concentration if available). Define the target vector `y_str` using the 'class' or 'spectype' column containing 'STAR', 'GALAXY', 'QSO' strings.

**Processing Step 2: Encode Labels and Split Data:** Use `LabelEncoder` to convert the string labels `y_str` into numerical labels `y` (e.g., Elliptical=0, Galaxy=1, Star=2 - check `encoder.classes_` for mapping). Split `X` and `y` into training and testing sets using `train_test_split`, ensuring `stratify=y` is used to maintain the proportions of stars, galaxies, and QSOs in both sets, as their frequencies might be different.

**Processing Step 3: Choose Model and Train:** Instantiate `RandomForestClassifier` (e.g., `n_estimators=100`, `random_state=42`, potentially `class_weight='balanced'` if classes are very imbalanced). Train the model using `model.fit(X_train, y_train)`. (Scaling `X_train` first with `StandardScaler` is optional but generally harmless for Random Forests).

**Processing Step 4: Predict and Evaluate:** Make predictions on the test set: `y_pred = model.predict(X_test)`. Calculate overall accuracy using `accuracy_score(y_test, y_pred)`. Generate the detailed `classification_report(y_test, y_pred, target_names=label_encoder.classes_)` to see precision, recall, and F1-score for each of the 'STAR', 'GALAXY', 'QSO' classes. Calculate the `confusion_matrix(y_test, y_pred)` to see exactly how many objects of each true class were classified into each predicted class. Visualize the confusion matrix using `ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=...)`.

**Processing Step 5: Interpret Results:** Analyze the classification report and confusion matrix. Which classes are classified most accurately? Are certain classes frequently confused with each other (e.g., high-redshift quasars sometimes misclassified as stars based on color)? How does performance differ between common classes (stars, galaxies) and rarer classes (QSOs)? This detailed evaluation provides insights beyond the overall accuracy.

**Output, Testing, and Extension:** Output includes the overall accuracy, the detailed classification report, and the confusion matrix (both numerical and visual). **Testing:** Verify the label encoding mapping. Check if performance metrics seem reasonable (e.g., accuracy > 90-95% might be expected for SDSS color-based separation). Examine the confusion matrix for significant off-diagonal elements indicating common misclassifications. **Extensions:** (1) Include morphological features (like concentration index, Petrosian radius) in addition to colors and see if classification improves. (2) Try different classification algorithms (e.g., `SVC` with RBF kernel, `GradientBoostingClassifier`, a simple Neural Network) and compare their performance using the same metrics. (3) Perform hyperparameter tuning for the Random Forest using `GridSearchCV`. (4) Investigate the feature importances (`model.feature_importances_`) to see which colors or features are most discriminative. (5) Apply the trained model to a larger photometric catalog without spectroscopic classifications to generate probabilistic classifications (`predict_proba`) for all sources.

```python
# --- Code Example: Application 22.A ---
# Note: Requires scikit-learn, pandas, matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay
# from sklearn.preprocessing import StandardScaler # Optional for RF
# from sklearn.pipeline import make_pipeline # Optional for RF

print("Star-Galaxy-QSO Classification using SDSS-like Colors:")

# Step 1: Simulate/Load Data and Engineer Features
np.random.seed(0)
n_obj = 1000
# Simulate features roughly based on SDSS distributions
classes = np.random.choice(['GALAXY', 'STAR', 'QSO'], size=n_obj, p=[0.55, 0.40, 0.05])
features = {}
# Generate colors based on class (very simplified model)
features['u-g'] = np.random.normal(1.5, 0.5, n_obj) # Base color
features['g-r'] = np.random.normal(0.6, 0.3, n_obj)
features['r-i'] = np.random.normal(0.3, 0.2, n_obj)
features['i-z'] = np.random.normal(0.2, 0.2, n_obj)
# Add class-dependent offsets/scatter
features['u-g'][classes=='STAR'] += np.random.normal(-0.3, 0.2, np.sum(classes=='STAR'))
features['g-r'][classes=='STAR'] += np.random.normal(-0.1, 0.1, np.sum(classes=='STAR'))
features['u-g'][classes=='QSO'] += np.random.normal(-1.0, 0.4, np.sum(classes=='QSO')) # QSOs are bluer
features['g-r'][classes=='QSO'] += np.random.normal(-0.2, 0.3, np.sum(classes=='QSO'))
features['r-i'][classes=='GALAXY'] += np.random.normal(0.1, 0.1, np.sum(classes=='GALAXY'))

df = pd.DataFrame(features)
df['class'] = classes # Target labels
print(f"\nGenerated {n_obj} simulated objects with colors and classes.")
print("Data head:\n", df.head())
print("\nClass distribution:\n", df['class'].value_counts())

# Define Features (X) and Target (y_str)
feature_cols = ['u-g', 'g-r', 'r-i', 'i-z']
target_col = 'class'
X = df[feature_cols]
y_str = df[target_col]

# Step 2: Encode Labels and Split Data
print("\nEncoding labels and splitting data...")
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_str)
class_names = label_encoder.classes_ # Get the mapping: ['GALAXY' 'QSO' 'STAR']
print(f"Labels encoded. Mapping: {list(zip(class_names, range(len(class_names))))}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print(f"Split data: Train={len(y_train)}, Test={len(y_test)}")

# Step 3: Choose Model and Train
print("\nTraining RandomForestClassifier...")
# Note: Scaling generally not needed for Random Forest
model = RandomForestClassifier(n_estimators=150, # More trees
                               random_state=42, 
                               n_jobs=-1, 
                               class_weight='balanced') # Handle imbalance
model.fit(X_train, y_train)
print("Training complete.")

# Step 4: Predict and Evaluate
print("\nEvaluating on test set...")
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nOverall Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
# Use labels=range(len(class_names)) and target_names=class_names for full report
print(classification_report(y_test, y_pred, labels=range(len(class_names)), target_names=class_names))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred, labels=range(len(class_names)))
print(cm)

# Step 5: Interpret / Visualize Confusion Matrix
print("\nVisualizing Confusion Matrix...")
fig, ax = plt.subplots(figsize=(6, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(ax=ax, cmap='Blues')
ax.set_title("Confusion Matrix")
# plt.show()
print("Plot generated.")
plt.close(fig)

# Feature Importances (Bonus)
print("\nFeature Importances:")
importances = model.feature_importances_
for name, imp in zip(feature_cols, importances):
    print(f"  {name}: {imp:.4f}")

print("-" * 20)
```

**Application 22.B: Classifying TESS Planet Candidates vs. False Positives**

**Objective:** This application demonstrates a binary classification task crucial for exoplanet surveys: distinguishing genuine transiting planet candidates from astrophysical or instrumental false positives using features derived from the light curve and vetting diagnostics. We will use a classifier like Support Vector Machine (SVC) (Sec 22.3) or Random Forest (Sec 22.4) and focus on evaluation using metrics suitable for potentially imbalanced data, such as the ROC curve and AUC score (Sec 22.5).

**Astrophysical Context:** Surveys like TESS generate millions of light curves, and automated pipelines detect numerous transit-like signals ("Threshold Crossing Events" or TCEs). However, only a small fraction of these signals correspond to genuine planets orbiting the target star. Many are astrophysical false positives (e.g., eclipsing binary stars in the background or foreground, or hierarchical triple systems involving the target star) or instrumental artifacts. Efficiently classifying TCEs or Objects of Interest (TOIs) into Planet Candidates (PCs) and False Positives (FPs) using machine learning, based on features extracted from the light curve shape and various diagnostic tests (e.g., centroid shifts, odd/even transit depth differences), is vital for prioritizing targets for expensive follow-up observations needed for confirmation.

**Data Source:** A curated catalog of TESS Objects of Interest (TOIs) available from MAST, specifically the "TOI Catalog" provided by the TESS Science Office (TSO) or derived tables from groups like the TESS Follow-up Observing Program (TFOP). This catalog typically includes: target information (TIC ID), fitted transit parameters (period, duration, depth), signal strength (SNR), and, crucially, various **vetting metrics** designed to identify false positives (e.g., centroid offset significance, secondary eclipse depth, odd/even transit depth ratio difference). Most importantly, it contains a disposition label assigned after human vetting and follow-up analysis, typically marking objects as 'PC' (Planet Candidate), 'FP' (False Positive), 'APC' (Ambiguous Planet Candidate), etc. We will focus on binary classification between PC and FP.

**Modules Used:** `pandas` or `astropy.table.Table` (for loading TOI catalog), `numpy`, `sklearn.model_selection.train_test_split`, `sklearn.preprocessing.StandardScaler` (likely needed, especially for SVM), `sklearn.svm.SVC` or `sklearn.ensemble.RandomForestClassifier`, `sklearn.pipeline.Pipeline`, `sklearn.metrics` (`classification_report`, `roc_auc_score`, `roc_curve`, `RocCurveDisplay`), `matplotlib.pyplot`. `sklearn.preprocessing.LabelEncoder` might be needed for the PC/FP labels.

**Technique Focus:** Binary classification workflow (Sec 22.1, 22.6). Selecting relevant features from the TOI catalog (transit parameters and vetting metrics). Handling potential missing values in features (Sec 20.1). Encoding the target labels ('PC', 'FP') into 0/1 (Sec 20.3). Applying feature scaling (`StandardScaler`, Sec 20.2). Training a classifier (e.g., SVC with RBF kernel or RandomForestClassifier, potentially with `class_weight='balanced'` due to likely imbalance where FPs might outnumber PCs). Evaluating performance using metrics robust to imbalance, particularly ROC AUC score, and visualizing the ROC curve (Sec 22.5). Using Pipelines to manage preprocessing and classification steps.

**Processing Step 1: Load and Prepare Data:** Load the TOI catalog into a Pandas DataFrame. Select relevant feature columns – typically quantitative vetting metrics like transit depth SNR, odd/even depth difference significance, centroid offset significance, secondary eclipse depth, potentially fitted transit shape parameters (duration, ingress time). Select the disposition column containing 'PC'/'FP' labels. Filter out ambiguous cases ('APC', 'Unknown') and rows with missing crucial features (or impute if appropriate).

**Processing Step 2: Encode Labels and Split Data:** Convert the 'PC'/'FP' labels into binary 0/1 using `LabelEncoder` (e.g., FP=0, PC=1). Split the features `X` and encoded labels `y` into training and testing sets using `train_test_split`, ensuring `stratify=y` is used to preserve the PC/FP ratio in both sets.

**Processing Step 3: Create and Train Pipeline:** Define a pipeline that includes `SimpleImputer` (if needed), `StandardScaler` (highly recommended for SVM, often beneficial for others), and the chosen classifier (`SVC` or `RandomForestClassifier`). Consider setting `class_weight='balanced'` in the classifier if the PC vs FP classes are significantly imbalanced. Fit the pipeline on the training data `pipeline.fit(X_train, y_train)`.

**Processing Step 4: Predict Probabilities and Evaluate:** Use the fitted pipeline to predict class *probabilities* on the test set: `y_proba = pipeline.predict_proba(X_test)[:, 1]` (probability of being class 1, i.e., PC). Calculate the ROC AUC score: `auc = roc_auc_score(y_test, y_proba)`. Generate the ROC curve data using `fpr, tpr, thresholds = roc_curve(y_test, y_proba)`. Plot the ROC curve using `RocCurveDisplay` or `plt.plot(fpr, tpr)`. Also generate a `classification_report` using hard predictions (`y_pred = pipeline.predict(X_test)`) based on the default 0.5 threshold to examine precision/recall.

**Processing Step 5: Interpret Results:** Evaluate the AUC score – values closer to 1 indicate better discrimination between PCs and FPs across all thresholds. Examine the shape of the ROC curve – a curve bowing strongly towards the top-left indicates good performance. Analyze the classification report: Does the model achieve high recall for Planet Candidates (correctly identifying most true planets)? Does it maintain reasonable precision (avoiding too many false alarms)? The balance between precision and recall might be adjusted by choosing a different probability threshold based on the ROC curve and the specific goals (e.g., prioritizing completeness vs. purity of the candidate list).

**Output, Testing, and Extension:** Output includes the AUC score, the ROC curve plot, and the classification report. **Testing:** Verify the data loading, feature selection, and label encoding steps. Check if the AUC score is significantly above 0.5. Examine the ROC curve shape. Compare performance with a dummy classifier predicting the majority class. **Extensions:** (1) Try different classification algorithms (SVC vs. Random Forest vs. Gradient Boosting vs. Logistic Regression). (2) Perform hyperparameter tuning for the chosen classifier using `GridSearchCV` with 'roc_auc' scoring. (3) Investigate feature importances (if using Random Forest) to understand which vetting metrics are most effective. (4) Explore different probability thresholds based on the ROC curve to optimize for either high recall (completeness) or high precision (purity) depending on the scientific goal for the candidate list. (5) Incorporate features derived directly from the light curve shape (e.g., using specialized feature extractors or even CNNs on target pixel files, Chapter 24).

```python
# --- Code Example: Application 22.B ---
# Note: Requires scikit-learn, pandas, matplotlib. Uses simulated data.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC # Using SVC as example classifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer # If needed for mixed types
from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, confusion_matrix

print("Classifying TESS Planet Candidates vs. False Positives:")

# Step 1: Simulate/Load TOI Catalog Data
np.random.seed(1)
n_pc = 200 # Planet Candidates
n_fp = 800 # False Positives (imbalanced)
# Simulate features (vetting metrics) - simplified distributions
def generate_features(n, is_pc):
    # PCs might have lower odd/even diff, lower centroid offset, deeper secondary?
    depth_snr = np.random.uniform(5, 50, n)
    oe_sigma = np.abs(np.random.normal(0, 1.5 if is_pc else 3.0, n)) # FP higher odd/even diff
    centroid_sigma = np.abs(np.random.normal(0, 2.0 if is_pc else 4.0, n)) # FP higher offset
    secondary_depth = np.random.uniform(0, 0.01 if is_pc else 0.5, n) # FP deeper secondaries
    # Add some NaNs
    for col in [oe_sigma, centroid_sigma, secondary_depth]:
         col[np.random.rand(n) < 0.1] = np.nan # 10% missing
    return pd.DataFrame({
        'DepthSNR': depth_snr, 
        'OddEvenSigma': oe_sigma, 
        'CentroidSigma': centroid_sigma,
        'SecondaryDepth': secondary_depth
    })

df_pc = generate_features(n_pc, is_pc=True); df_pc['Disposition'] = 'PC'
df_fp = generate_features(n_fp, is_pc=False); df_fp['Disposition'] = 'FP'
toi_data = pd.concat([df_pc, df_fp], ignore_index=True)
print(f"\nGenerated {len(toi_data)} simulated TOIs ({n_pc} PC, {n_fp} FP).")
print("Data head:\n", toi_data.head())
print("\nMissing values:\n", toi_data.isna().sum())

# Define features X and target y_str
feature_cols = ['DepthSNR', 'OddEvenSigma', 'CentroidSigma', 'SecondaryDepth']
target_col = 'Disposition'
X = toi_data[feature_cols]
y_str = toi_data[target_col]

# Step 2: Encode Labels and Split Data
print("\nEncoding labels and splitting data...")
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_str) # FP=0, PC=1
class_names = label_encoder.classes_
print(f"Labels encoded. Mapping: {list(zip(class_names, range(len(class_names))))}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print(f"Split data: Train={len(y_train)}, Test={len(y_test)}")

# Step 3: Create and Train Pipeline (Imputer -> Scaler -> SVC)
print("\nCreating and fitting pipeline (Imputer -> Scaler -> SVC)...")
# Use SVC with RBF kernel, enable probability, handle imbalance
pipeline_svc_toi = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), # Handle NaNs
    ('scaler', StandardScaler()),                 # Scale features for SVC
    ('classifier', SVC(kernel='rbf', C=1.0, gamma='scale', 
                       probability=True, # Needed for predict_proba and ROC AUC
                       class_weight='balanced', # Address class imbalance
                       random_state=42)) 
])
pipeline_svc_toi.fit(X_train, y_train)
print("Pipeline fitting complete.")

# Step 4: Predict Probabilities and Evaluate
print("\nPredicting on test set and evaluating...")
# Predict hard labels (using default 0.5 threshold)
y_pred = pipeline_svc_toi.predict(X_test) 
# Predict probabilities (for class 1 = PC)
y_proba = pipeline_svc_toi.predict_proba(X_test)[:, 1] 

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=class_names))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

try:
    auc = roc_auc_score(y_test, y_proba)
    print(f"\nAUC Score: {auc:.4f}")

    # Step 5: Visualize ROC Curve
    print("Generating ROC Curve plot...")
    fig, ax = plt.subplots(figsize=(6, 6))
    roc_display = RocCurveDisplay.from_predictions(y_test, y_proba, name='SVC (RBF)', ax=ax)
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    ax.set_title('ROC Curve: PC vs FP Classification')
    ax.grid(True, alpha=0.4)
    # plt.show()
    print("ROC curve plot generated.")
    plt.close(fig)
    
except Exception as e:
     print(f"\nError calculating/plotting ROC/AUC: {e}")

print("-" * 20)
```

**Summary**

This chapter focused on supervised machine learning for **classification**, the task of assigning discrete category labels to data based on input features. It contrasted classification (predicting categories like 'Star'/'Galaxy', 'Planet'/'False Positive') with regression (predicting continuous values) and introduced the concepts of binary, multi-class, and multi-label classification. Several key algorithms were explored, implemented using `scikit-learn`. **Logistic Regression** was presented as a simple, interpretable linear model adapted for binary (and multi-class) classification by modeling class probabilities via the sigmoid function, often serving as a useful baseline. **Support Vector Machines (SVMs)**, specifically `SVC`, were introduced as powerful classifiers aiming to find the maximum margin hyperplane separating classes, leveraging the kernel trick ('linear', 'poly', 'rbf') to handle non-linear boundaries and regularization (`C` parameter) to control complexity. **Decision Trees** (`DecisionTreeClassifier`) and especially **Random Forests** (`RandomForestClassifier`) were presented as non-linear, non-parametric methods that partition the feature space, with Random Forests using ensemble averaging (bagging and feature randomization) to drastically reduce the overfitting tendency of single trees, often achieving high accuracy and providing feature importance measures.

Crucially, the chapter detailed essential metrics for evaluating classifier performance, particularly in the common astrophysical context of imbalanced datasets where simple accuracy can be misleading. The **Confusion Matrix** (TP, TN, FP, FN) was introduced as the foundation for calculating **Precision** (reliability of positive predictions), **Recall** (sensitivity, completeness in finding true positives), **Specificity** (true negative rate), and the **F1-score** (harmonic mean of precision and recall). The `classification_report` in `scikit-learn` providing these metrics on a per-class basis was highlighted. For classifiers outputting probabilities, the trade-off between precision and recall via threshold adjustment was discussed, leading to the introduction of the **Receiver Operating Characteristic (ROC) Curve** (plotting True Positive Rate vs. False Positive Rate) and the **Area Under the ROC Curve (AUC)** as robust metrics for evaluating discrimination ability across all thresholds, particularly useful for imbalanced problems. Finally, the chapter consolidated the standard `scikit-learn` implementation workflow for classification, reiterating the importance of data splitting (`train_test_split` with `stratify=y`), fitting models/pipelines only on training data, making predictions (`.predict()`) and obtaining probabilities (`.predict_proba()`) on the test set, and performing thorough evaluation using appropriate classification metrics.

---

**References for Further Reading:**

1.  **Ivezić, Ž., Connolly, A. J., VanderPlas, J. T., & Gray, A. (2014).** *Statistics, Data Mining, and Machine Learning in Astronomy*. Princeton University Press. (Relevant chapters often available online, e.g., Chapter 9 covers classification algorithms: [http://press.princeton.edu/titles/10159.html](http://press.princeton.edu/titles/10159.html))
    *(Provides detailed coverage of classification concepts, algorithms like Logistic Regression, SVMs, Decision Trees, Random Forests, and evaluation metrics within an astronomical context.)*

2.  **James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013).** *An Introduction to Statistical Learning: with Applications in R*. Springer. (Python version resources often available online). [https://www.statlearning.com/](https://www.statlearning.com/)
    *(Excellent, accessible introduction to classification concepts (Ch 4), Logistic Regression (Ch 4), SVMs (Ch 9), Tree-based methods (Ch 8), and evaluation metrics.)*

3.  **Hastie, T., Tibshirani, R., & Friedman, J. (2009).** *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* (2nd ed.). Springer. [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)
    *(A more advanced reference covering Logistic Regression (Ch 4), SVMs (Ch 12), and Tree Ensembles/Random Forests (Ch 8, 15) in greater mathematical detail.)*

4.  **The Scikit-learn Developers. (n.d.).** *Scikit-learn Documentation: User Guide*. Scikit-learn. Retrieved January 16, 2024, from [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html) (Specific sections on relevant classifiers and Model evaluation: metrics and scoring)
    *(Essential resource for implementation details, API reference, and usage examples for `LogisticRegression`, `SVC`, `DecisionTreeClassifier`, `RandomForestClassifier`, and evaluation metrics like `classification_report`, `confusion_matrix`, `roc_auc_score`, `RocCurveDisplay` discussed in this chapter.)*

5.  **Fawcett, T. (2006).** An introduction to ROC analysis. *Pattern Recognition Letters*, *27*(8), 861–874. [https://doi.org/10.1016/j.patrec.2005.10.010](https://doi.org/10.1016/j.patrec.2005.10.010)
    *(A classic introductory paper clearly explaining ROC curves and the AUC metric, crucial evaluation tools for classification discussed in Sec 22.5.)*
