**Chapter 66: Workflow Management Systems (Snakemake, Nextflow, Parsl)**

While basic Python scripting can manage simple linear workflows (Chapter 65), handling complex analysis pipelines with intricate dependencies, numerous steps, large datasets, and the need for parallel execution across different computing environments often necessitates more powerful, specialized tools: **Workflow Management Systems (WMS)**. This chapter introduces several popular WMSs used in scientific computing, focusing on those commonly employed in bioinformatics and increasingly adopted in astrophysics due to their flexibility and scalability. We will revisit the core concepts of workflow management, emphasizing how WMSs automate dependency tracking and execution based on defined rules or processes, often visualized as a Directed Acyclic Graph (DAG). We then delve into three prominent systems: **Snakemake**, highlighting its Python-based rule definition syntax (inspired by Make), use of wildcards for pattern matching, configuration management, and seamless integration with cluster schedulers and containerization; **Nextflow**, showcasing its dataflow paradigm, Groovy-based Domain Specific Language (DSL), powerful support for process parallelization, and extensive features for reproducibility (including built-in container support); and **Parsl** (Parallel Scripting Library), focusing on its Python-native approach where parallel workflows are defined by decorating Python functions as "apps" and linking them via data dependencies, offering tight integration with the Python ecosystem and flexible execution backends. For each system, we discuss its core syntax, execution model, and typical use cases, providing conceptual examples. Finally, we offer guidance on **choosing the right WMS** based on project complexity, language preferences, execution environment, and community support.

**66.1 Revisiting WMS Concepts (DAGs, Rules/Processes, Execution)**

As scientific analyses become increasingly complex, involving multiple software tools, scripts, and processing stages with dependencies between them, managing the execution manually becomes highly inefficient and prone to errors. Workflow Management Systems (WMS) provide a structured solution by allowing researchers to formally define their analysis pipeline as a collection of tasks and their interdependencies, typically forming a Directed Acyclic Graph (DAG), as introduced conceptually in Chapter 64. The WMS then automates the execution of this DAG, ensuring tasks run in the correct order and potentially in parallel.

The core components specified in a workflow definition usually include:
*   **Tasks (or Rules, Processes):** The individual computational steps in the workflow (e.g., running a script, executing a command-line tool).
*   **Inputs:** The data files or parameters required for a task to run.
*   **Outputs:** The data files or results generated by a task upon successful completion.
*   **Dependencies:** The relationships between tasks, implicitly or explicitly defined by the fact that the output of one task serves as the input for another.
*   **Execution Logic:** The specific command or code (e.g., a shell command, a Python script call) associated with each task that transforms inputs into outputs.
*   **Parameters/Configuration:** Settings that control the behavior of individual tasks or the workflow as a whole (e.g., software paths, algorithm parameters, resource requests).

The WMS engine parses this workflow definition, constructs the corresponding DAG, and manages the execution. Its key responsibilities include:
*   **Dependency Resolution:** Determining the correct execution order based on the DAG structure.
*   **Task Scheduling:** Deciding which tasks are ready to run (i.e., all their dependencies are met).
*   **Parallel Execution:** Identifying independent tasks (branches in the DAG) that can be run concurrently and launching them in parallel across available resources (CPU cores or cluster nodes).
*   **Execution Environment Management:** Potentially handling software dependencies (e.g., via Conda environments or containers) required by each task.
*   **Job Submission (for HPC):** Interfacing with cluster schedulers (SLURM, PBS, etc.) to submit individual tasks or groups of tasks as batch jobs.
*   **Monitoring and Logging:** Tracking the status (waiting, running, completed, failed) of each task and recording detailed logs.
*   **Error Handling:** Detecting task failures and potentially stopping the workflow or allowing for partial reruns.
*   **Incremental Builds / Restart Capability:** Checking timestamps or checksums of input/output files to determine if a task needs to be re-executed when the workflow is rerun, saving time by reusing existing results (often called "smart restarts").

By explicitly defining the workflow and automating its execution, WMSs provide significant advantages over manual scripting: improved reproducibility (the workflow definition *is* the executable documentation), enhanced scalability (automatic parallelization and cluster integration), greater robustness (error handling, restarts), and increased efficiency (saving researcher time, reusing results). The choice of WMS often depends on the preferred syntax for defining workflows (e.g., Python-based, Make-like, custom DSL) and the specific features needed for execution management and platform compatibility. The following sections explore three popular WMS options: Snakemake, Nextflow, and Parsl.

**66.2 Snakemake: Rules, Wildcards, Configuration, Execution**

**Snakemake** (`pip install snakemake`) is a widely adopted WMS known for its readability and flexibility, particularly popular in bioinformatics but applicable to any field involving multi-step data processing pipelines. Its workflow definition syntax, written in a text file typically named `Snakefile`, is inspired by the classic `make` utility but uses Python code for expressing logic and configuration, making it powerful and highly extensible.

Workflows in Snakemake are defined as a series of **rules**. Each rule describes how to create specific **output files** from specific **input files** using a defined action (usually a shell command or Python script execution). Dependencies between rules are determined *implicitly* by Snakemake by matching the output filenames of one rule with the input filenames of another, automatically constructing the DAG.

A key feature of Snakemake rules is the use of **wildcards**. Placeholders enclosed in curly braces (e.g., `{sample}`, `{filter}`) can be used in input and output filenames. Snakemake automatically determines the wildcard values needed to produce a requested final output file and applies the rule accordingly. This allows writing generic rules that can operate on many files following a consistent naming pattern.

```python
# --- Code Example 1: Conceptual Snakemake Rule Structure ---
# Content for a 'Snakefile' (NOT a Python script)

snakefile_rule_example = """
# Assume input files like 'raw_data/sampleA.fastq.gz', 'raw_data/sampleB.fastq.gz'
SAMPLES = ["sampleA", "sampleB"] # Define sample names

# Rule 1: Align reads to reference genome
rule align_reads:
    input:
        fastq="raw_data/{sample}.fastq.gz", # Input depends on {sample} wildcard
        ref_genome="reference/genome.fasta"
    output:
        bam="aligned/{sample}.bam" # Output also uses {sample} wildcard
    log:
        "logs/align/{sample}.log"
    threads: 8 # Specify number of threads for this rule
    shell:
        # Shell command using wildcards and input/output placeholders
        "echo 'Aligning {input.fastq}...' > {log}; "
        "bwa mem -t {threads} {input.ref_genome} {input.fastq} | "
        "samtools view -Sb - > {output} 2>> {log}"

# Rule 2: Sort the aligned BAM file
rule sort_bam:
    input:
        "aligned/{sample}.bam" # Depends on output of 'align_reads'
    output:
        "sorted/{sample}.sorted.bam"
    log:
        "logs/sort/{sample}.log"
    shell:
        "echo 'Sorting {input}...' > {log}; "
        "samtools sort -@ {threads} -o {output} {input} 2>> {log}" # Assume {threads} defined globally or inherited

# Rule 3: Define final target (e.g., request all sorted BAMs)
rule all:
    input:
        # Use expand to generate all target output filenames based on SAMPLES list
        expand("sorted/{sample}.sorted.bam", sample=SAMPLES) 
"""

print("--- Conceptual Snakemake Rule Definition ---")
print(snakefile_rule_example)
# To Run (conceptual): snakemake --cores 4 all
print("\n--- Conceptual Execution Command ---")
print("# snakemake --cores 4 all")
print("-" * 20)

# Explanation: This shows a simplified Snakemake workflow.
# - `rule align_reads`: Defines how to create an output file like `aligned/sampleA.bam` 
#   from inputs `raw_data/sampleA.fastq.gz` and the reference genome. It uses the 
#   `{sample}` wildcard. The `shell:` section contains the command-line execution logic. 
#   It requests 8 threads.
# - `rule sort_bam`: Takes `aligned/{sample}.bam` (output of previous rule) as input 
#   and produces `sorted/{sample}.sorted.bam`. Snakemake infers this dependency.
# - `rule all`: Defines the final targets. `expand` generates the list of all desired 
#   sorted BAM files based on the `SAMPLES` list. When `snakemake all` is run, Snakemake 
#   works backward from these targets, determines that `sort_bam` is needed, which in 
#   turn needs `align_reads`, and executes the rules in the correct order for each sample, 
#   potentially in parallel if `--cores` allows.
```

Snakemake offers extensive features beyond basic rules:
*   **Configuration:** Workflows can be easily parameterized using external configuration files (JSON or YAML) accessed within the `Snakefile` via a global `config` dictionary, separating parameters from workflow logic.
*   **Python Integration:** Rule actions can be specified using `run:` blocks containing arbitrary Python code instead of just `shell:` commands, allowing complex logic or use of Python libraries directly within rules. Input/output objects within `run:` blocks have useful attributes (like filenames).
*   **Environment Management:** Integrates seamlessly with **Conda** for managing software dependencies per rule. You can specify a Conda environment file (`environment.yaml`) for a rule, and Snakemake automatically creates/activates the environment before running the rule's command, ensuring reproducibility. Support for **Singularity/Apptainer** containers is also built-in.
*   **Cluster/Cloud Execution:** Includes profiles and wrappers for submitting jobs to various HPC schedulers (SLURM, PBS, SGE, LSF) or cloud platforms (Google Cloud Storage, AWS S3, Kubernetes), allowing the same workflow definition to scale from local execution to large clusters.
*   **Reporting:** Can generate detailed HTML reports summarizing workflow execution, results, and code versions.

Snakemake's combination of a readable, Python-enhanced syntax, automatic dependency management based on filenames, powerful wildcard system, strong support for Conda/containers, and flexible execution backends makes it an excellent choice for managing complex bioinformatics and increasingly, astrophysical data analysis pipelines where reproducibility and scalability are key concerns. Its learning curve is generally considered moderate, especially for those familiar with Python and Make.

**66.3 Nextflow: Processes, Channels, Dataflow**

**Nextflow** (`nextflow run ...`) is another powerful and increasingly popular WMS, also widely adopted in bioinformatics but highly suitable for general scientific workflows. It takes a different approach based on the **dataflow paradigm**. Instead of defining rules based on input/output *files* like Snakemake, Nextflow defines independent **processes**, and data flows between these processes via asynchronous FIFO (First-In, First-Out) queues called **channels**.

Workflows are written in a **Domain Specific Language (DSL)** based on the Groovy programming language (which runs on the Java Virtual Machine, JVM, so Nextflow requires Java). A Nextflow script (`.nf` file) typically defines:
*   **Parameters (`params`):** Input parameters for the workflow, easily set via command line (`--param_name value`) or config files.
*   **Channels:** Represent streams of data values (e.g., tuples containing sample IDs and file paths) that trigger process executions. Channels can be created from value lists, file path patterns (`Channel.fromPath`), etc. Operators (`map`, `filter`, `groupTuple`, `combine`, `join`) manipulate data within channels.
*   **Processes:** Define the computational tasks. Each process has:
    *   `input:` Specifies the input channel(s) providing data items (e.g., `tuple val(sample_id), path(reads)` from channel `ch_reads`).
    *   `output:` Specifies the output channel(s) where results are emitted, often defining output file paths using input values (`path "${sample_id}.bam"`).
    *   `script:` Contains the shell command(s) or code block (potentially other languages like Python via heredocs) to be executed. Input values/files are accessed as variables within the script block (e.g., `${sample_id}`, `${reads}`).
*   **Workflow Block:** Defines how processes are connected by linking the output channels of one process to the input channels of others, implicitly defining the DAG.

```groovy
// --- Code Example 1: Conceptual Nextflow Script Structure ---
// File: pipeline.nf (NOT Python, uses Nextflow/Groovy DSL)

#!/usr/bin/env nextflow
nextflow.enable.dsl=2 // Use latest DSL version

// --- Parameters ---
params.reads = "$baseDir/raw_data/*_R{1,2}.fastq.gz" // Default input read pattern
params.ref_genome = "$baseDir/reference/genome.fasta"
params.outdir = "$baseDir/results"
params.cpus = 4

log.info """
         Reads Pattern : ${params.reads}
         Ref Genome    : ${params.ref_genome}
         Output Dir    : ${params.outdir}
         CPUs per task : ${params.cpus}
         """

// --- Channel Creation ---
// Create channel emitting tuples: [ sample_id, [read1_path, read2_path] ]
Channel
    .fromFilePairs( params.reads, size: 2 ) // Matches _R1/_R2 pairs
    .ifEmpty { exit 1, "Cannot find any reads matching: ${params.reads}" }
    .set { ch_raw_reads } 

// --- Process Definitions ---
process ALIGN_READS {
    tag "$sample_id" // Tag for monitoring
    publishDir "$params.outdir/aligned", mode: 'copy' // Copy output files here

    input:
    tuple val(sample_id), path(reads) // Takes [id, [r1, r2]] from channel
    path reference                 // Takes single reference file

    output:
    path "${sample_id}.bam"          // Emits BAM file path to output channel
    path "${sample_id}.log"          // Emits log file path

    script:
    """
    echo "Aligning ${sample_id}..." > ${sample_id}.log
    bwa mem -t ${params.cpus} ${reference} ${reads[0]} ${reads[1]} | \\
        samtools view -Sb - > ${sample_id}.bam 2>> ${sample_id}.log
    """
}

process SORT_BAM {
    tag "$sample_id"
    publishDir "$params.outdir/sorted", mode: 'copy'

    input:
    tuple val(sample_id), path(aligned_bam) // Takes [id, path] from ALIGN_READS output

    output:
    path "${sample_id}.sorted.bam"
    path "${sample_id}.sort.log"

    script:
    """
    echo "Sorting ${aligned_bam}..." > ${sample_id}.sort.log
    samtools sort -@ ${params.cpus} -o ${sample_id}.sorted.bam ${aligned_bam} 2>> ${sample_id}.sort.log
    """
}

// --- Workflow Definition ---
workflow {
    // Call ALIGN_READS process for each item in ch_raw_reads channel
    // Pass the static reference genome file as the second input
    ALIGN_READS(ch_raw_reads, params.ref_genome)

    // Call SORT_BAM process using the output channel from ALIGN_READS
    // Nextflow automatically matches output files to input paths by name pattern usually
    // Or explicitly use the output channel: ALIGN_READS.out[0] or ALIGN_READS.out.bam etc.
    SORT_BAM(ALIGN_READS.out.bam) // Assuming .bam identifies the correct output channel
}

```
```python
# --- Python Block to Display Conceptual Nextflow Script ---
print("--- Conceptual Nextflow Workflow Definition (pipeline.nf Content) ---")
print("#!/usr/bin/env nextflow")
print("nextflow.enable.dsl=2")
print("\n// --- Parameters ---")
print('params.reads = "$baseDir/raw_data/*_R{1,2}.fastq.gz"')
print('params.ref_genome = "$baseDir/reference/genome.fasta"')
# ... (rest of params and log.info) ...
print("\n// --- Channel Creation ---")
print('Channel.fromFilePairs( params.reads, size: 2 ).set { ch_raw_reads }')
print("\n// --- Process Definitions ---")
print("process ALIGN_READS {")
print("  tag \"$sample_id\"")
# ... (input, output, script block for ALIGN_READS) ...
print("}")
print("\nprocess SORT_BAM {")
print("  tag \"$sample_id\"")
# ... (input, output, script block for SORT_BAM) ...
print("}")
print("\n// --- Workflow Definition ---")
print("workflow {")
print("  ALIGN_READS(ch_raw_reads, params.ref_genome)")
print("  SORT_BAM(ALIGN_READS.out.bam)")
print("}")
# To Run (conceptual, requires nextflow, java, bwa, samtools): 
# nextflow run pipeline.nf -profile standard [--reads='path/to/reads/*.gz']
print("\n--- Conceptual Execution Command ---")
print("# nextflow run pipeline.nf -profile standard --reads 'input_dir/*.fastq.gz'")
print("-" * 20)

# Explanation: This shows a simplified Nextflow pipeline.
# - Parameters (`params.`) are defined with defaults, easily overridden.
# - `Channel.fromFilePairs` creates a channel `ch_raw_reads` emitting tuples 
#   containing a sample ID and paths to paired-end read files.
# - `process ALIGN_READS` defines a task that takes input from `ch_raw_reads` 
#   and the reference genome path. Its `script:` block uses `bwa mem` and `samtools`. 
#   It declares it outputs a BAM file and a log file.
# - `process SORT_BAM` takes input from the output channel of `ALIGN_READS` (implicitly 
#   or explicitly linked) and runs `samtools sort`.
# - The `workflow { ... }` block defines the connections: `ALIGN_READS` is called with 
#   the input read channel, and `SORT_BAM` is called with the output of `ALIGN_READS`.
# Nextflow automatically parallelizes by running the `ALIGN_READS` process instance 
# for each sample ID emitted by the channel. When `ALIGN_READS` finishes for a sample, 
# its output triggers the corresponding `SORT_BAM` instance.
```

Key strengths of Nextflow include:
*   **Implicit Parallelism:** Parallel execution happens automatically based on the channel data flow – processes run concurrently for each item arriving on their input channel(s).
*   **Scalability:** Excellent support for various execution environments (local, SLURM, PBS, cloud platforms like AWS Batch, Google Cloud Life Sciences) via configurable "executors" and "profiles".
*   **Reproducibility:** Built-in support for **Conda** environments and **Docker/Singularity/Apptainer** containers ensures that processes run with specific software versions, greatly enhancing reproducibility. It also caches intermediate results effectively.
*   **Resilience:** Workflows can often be resumed automatically from the point of failure.
*   **Extensive Operators:** Rich set of channel operators for complex data manipulation and workflow logic (splitting, merging, joining, grouping data streams).

The main potential hurdles for Python users are the need for Java and learning the Groovy-based DSL syntax, although the DSL is quite expressive once learned. Nextflow is a very powerful choice for complex, production-scale workflows requiring high levels of parallelization, portability across platforms, and robust reproducibility guarantees, especially when involving containerized tools.

**66.4 Parsl: Python Parallel Scripting Library**

For researchers who prefer to define and manage their workflows entirely within the Python language, **Parsl** (`pip install parsl`) offers a compelling solution. Parsl (Parallel Scripting Library) is a Python library designed to facilitate parallel and distributed computing by allowing users to easily annotate Python functions or external applications as parallel **tasks** (called "apps") and automatically manage their execution across diverse resources, from laptops to supercomputers.

Parsl's core concept revolves around decorating Python functions with `@python_app` or external commands with `@bash_app`. These decorators signal to Parsl that the function call represents a task to be potentially executed remotely or in parallel. Parsl handles the execution logic, data staging (transferring inputs/outputs if needed), and dependency management implicitly based on data flow between apps.

Key components of Parsl:
*   **Apps (`@python_app`, `@bash_app`):** Decorators that transform Python functions or wrap shell commands into Parsl "apps," the basic units of parallel execution.
*   **Futures:** When an app is called, it typically returns immediately with a **future** object (similar to Python's `concurrent.futures.Future` or Dask futures). This future acts as a placeholder for the eventual result. Actual execution might happen asynchronously in the background. You can check if a future is complete (`.done()`) or explicitly wait for and retrieve its result (`.result()`).
*   **Implicit Dependencies:** Parsl automatically infers dependencies between apps based on data flow. If the output (future) of App A is passed as an input argument to App B, Parsl ensures that App A completes successfully before App B starts executing. This builds the workflow DAG implicitly.
*   **Configuration:** Parsl requires a configuration object specifying the **execution provider** (how tasks are launched, e.g., `LocalThreads`, `LocalProcesses`, `HighThroughputExecutor` for clusters via SLURM/PBS/Condor) and potentially **data providers** (how files are transferred, e.g., Globus, HTTP). Configuration can be defined in the script or loaded from files.
*   **Executors:** The execution provider manages one or more **executors** (like `ThreadPoolExecutor`, `ProcessPoolExecutor`, `HighThroughputExecutor`) which are responsible for actually running the app tasks on the target resources (local cores, cluster nodes).

```python
# --- Code Example 1: Basic Parsl Workflow (Python Apps) ---
# Note: Requires parsl installation: pip install parsl
import parsl
from parsl import python_app, bash_app
import time
import os
import shutil

print("Defining and Running a Simple Parsl Workflow:")

# --- Configure Parsl ---
# Simplest config: Use local threads for execution
# More complex configs needed for HPC clusters (see Parsl docs)
try:
    # parsl.load() # Loads default config (often local threads)
    # Explicit config for local processes (similar to multiprocessing)
    from parsl.config import Config
    from parsl.executors import HighThroughputExecutor
    from parsl.providers import LocalProvider
    
    local_htex_config = Config(
        executors=[
            HighThroughputExecutor(
                label="htex_local", # Executor label
                max_workers=os.cpu_count(), # Use available cores
                provider=LocalProvider(), # Run processes locally
            )
        ],
        strategy=None, # Default strategy often suitable
    )
    parsl.load(local_htex_config)
    print("\nParsl configured with HighThroughputExecutor (local processes).")
    parsl_loaded = True
except ImportError:
    print("\nWarning: Parsl not installed. Skipping example.")
    parsl_loaded = False
except Exception as e_load:
    print(f"\nError loading Parsl config: {e_load}")
    parsl_loaded = False

# --- Define Parsl Apps ---
if parsl_loaded:
    @python_app # Decorator marks this as a parallel task
    def generate_data(size, filename_out):
        """Generates random data and saves it."""
        import numpy as np
        import os
        print(f"  App generate_data (PID {os.getpid()}) running...")
        data = np.random.rand(size)
        np.save(filename_out, data)
        print(f"  App generate_data saved {filename_out}")
        return filename_out # Return filename for dependency tracking

    @python_app
    def analyze_data(filename_in):
        """Loads data and calculates mean."""
        import numpy as np
        import os
        print(f"  App analyze_data (PID {os.getpid()}) running for {filename_in}...")
        data = np.load(filename_in)
        mean_val = np.mean(data)
        print(f"  App analyze_data finished {filename_in}")
        return mean_val

    # --- Define Workflow ---
    print("\nDefining workflow...")
    output_dir = "parsl_temp_output"
    os.makedirs(output_dir, exist_ok=True)
    
    data_futures = [] # Store futures for data generation tasks
    for i in range(4): # Generate 4 data files in parallel
        fname = os.path.join(output_dir, f"data_{i}.npy")
        # Calling the app returns a future immediately
        data_future = generate_data(size=10**6, filename_out=fname) 
        data_futures.append(data_future)
    print(f"Launched {len(data_futures)} generate_data apps.")

    analysis_futures = [] # Store futures for analysis tasks
    # Loop through data futures. Analysis depends on corresponding data future.
    for data_future in data_futures:
        # Pass the *future* object as input to the next app
        # Parsl waits for data_future to complete before running analyze_data
        analysis_future = analyze_data(filename_in=data_future) 
        analysis_futures.append(analysis_future)
    print(f"Launched {len(analysis_futures)} analyze_data apps (depend on generation).")

    # --- Wait for Results ---
    print("\nWaiting for analysis results...")
    # Accessing .result() blocks until the future is complete
    final_means = [fut.result() for fut in analysis_futures]
    print("\nWorkflow complete.")
    print(f"Calculated means: {final_means}")

    # --- Cleanup ---
    parsl.dfk().cleanup() # Cleanup Data Flow Kernel state
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    print(f"Cleaned up {output_dir}.")
    parsl.clear() # Clear loaded config

print("-" * 20)

# Explanation:
# 1. Imports Parsl components (`python_app`, `Config`, `HighThroughputExecutor`, etc.).
# 2. Configures Parsl to use the `HighThroughputExecutor` running processes locally 
#    (similar to `multiprocessing.Pool`). Parsl must be configured before apps are called.
# 3. Defines two functions `generate_data` and `analyze_data`, decorated with `@python_app`.
# 4. Defines the workflow:
#    - It launches multiple `generate_data` apps concurrently. Calling the app returns 
#      a future `data_future` immediately. These futures are stored.
#    - It then launches multiple `analyze_data` apps, crucially passing the *future* 
#      object `data_future` (which resolves to the filename) as the `filename_in` argument. 
#      Parsl understands this dependency: `analyze_data` for item `i` will only run after 
#      `generate_data` for item `i` completes and returns the filename.
# 5. It waits for the results by calling `.result()` on each `analysis_future`. This 
#    blocks until the specific task associated with that future is finished.
# 6. Finally, it cleans up Parsl resources. Parsl handles executing the apps in parallel 
#    based on dependencies and the configured executor.
```

**Advantages of Parsl:**
*   **Python Native:** Workflows are defined using standard Python syntax and decorators, making it very natural for Python programmers.
*   **Implicit Dependencies:** Dependencies are usually inferred automatically from the flow of data (futures) between Python apps, simplifying definition compared to explicit dependency declaration in some other systems.
*   **Flexibility:** Supports various execution backends (threads, processes, HPC schedulers via HTEX, cloud, custom executors) allowing the same script to run across different environments with configuration changes.
*   **Interactivity:** Well-suited for interactive parallel computing within Jupyter notebooks.

**Potential Challenges:**
*   **Configuration:** Setting up the `Config` object, especially for complex HPC environments (specifying providers, launchers, executors), can require careful attention to detail.
*   **Data Management:** While Parsl can stage data implicitly for apps, managing large data transfers efficiently between nodes in an HPC environment might still require careful planning or use of appropriate data provider configurations.
*   **Learning Curve:** While Pythonic, understanding the concepts of futures, asynchronous execution, and the configuration system takes some learning effort.

Parsl provides an excellent option for researchers who want to parallelize complex Python-based workflows, including those involving calls to external codes (via `@bash_app`), while staying entirely within the Python programming environment. Its implicit dependency tracking and flexible execution backends make it a powerful tool for scaling scientific Python scripts from laptops to supercomputers.

**66.5 Containerization Support (Docker/Singularity)**

A major challenge in ensuring the reproducibility and portability of complex scientific workflows is managing the **software environment**. Workflows often depend on specific versions of numerous libraries (Python packages, compilers, system tools), and running the same workflow on a different machine or even later on the same machine with updated software can lead to different results or outright failures. **Containerization** technologies like **Docker** and **Singularity/Apptainer** provide a powerful solution by packaging an application or workflow step along with *all* its dependencies (libraries, system tools, specific configurations) into a self-contained, portable unit called a **container image**.

**Docker** is the most popular containerization platform, primarily used in web development and cloud environments. It uses a `Dockerfile` (Sec A.VI.4) to define the steps for building a container image, starting from a base OS image, installing system packages, setting up Python environments (e.g., using `pip install -r requirements.txt` or Conda environments), copying application code, and specifying runtime configurations. Docker images can be shared via registries like Docker Hub and run consistently on any system with Docker installed (Linux, macOS, Windows via WSL2). However, Docker typically requires root privileges (or specific group membership) to run containers, which is often restricted on shared HPC systems due to security concerns.

**Singularity/Apptainer** (`apptainer` is the successor project to Singularity) was developed specifically to address the needs of HPC environments. It allows users to run containers **without requiring root privileges**. Singularity/Apptainer can import layers from existing Docker images or build images directly from definition files (`.def` files, similar in concept to Dockerfiles). These images are often stored as single `.sif` (Singularity Image Format) files, which are easy to share and manage on cluster file systems. Users can then execute commands *within* the container environment (e.g., `singularity exec my_container.sif python my_script.py`) using their standard user permissions. The container provides the necessary software environment isolated from the host system's environment.

Most modern Workflow Management Systems (Snakemake, Nextflow, Parsl) provide direct support for executing workflow tasks *inside* containers, greatly enhancing reproducibility and simplifying dependency management:

*   **Snakemake:** Rules can include a `container:` directive specifying a Docker image URI (e.g., `docker://python:3.9-slim`) or a Singularity image path. When the rule executes, Snakemake automatically pulls the image (if needed) and runs the rule's `shell` or `run` command *inside* that container using `singularity exec` (if Singularity/Apptainer is configured as the containerization backend).
*   **Nextflow:** Has excellent native support for containers. Processes can include a `container` directive specifying a Docker or Singularity image. Nextflow automatically handles pulling the image and running the process's script block within the container. It integrates smoothly with various executors (local, SLURM, cloud) using containers.
*   **Parsl:** Can be configured to run apps within containers. The `HighThroughputExecutor`, often used for HPC clusters, can be configured with options to wrap app execution within `singularity exec` or `docker run` commands, ensuring tasks run with their specified containerized environment on the compute nodes.

**Benefits of Using Containers in Workflows:**
*   **Reproducibility:** Captures the exact software environment (OS libraries, Python version, package versions) needed for each step, ensuring the workflow yields the same results regardless of where or when it's run (provided the container image is available).
*   **Dependency Management:** Avoids complex installation procedures or conflicts with the host system's libraries. All dependencies are self-contained within the container image.
*   **Portability:** Workflows become easily portable across different machines, clusters, or cloud platforms that support the container runtime (Docker or Singularity/Apptainer).
*   **Collaboration:** Simplifies sharing workflows with collaborators, as they only need the workflow definition file and the specified container image(s) to run it, without needing to manually replicate the complex software environment.

Building container images requires learning Dockerfile or Singularity definition file syntax. For Python workflows, this typically involves starting from a base Python or Conda image, copying `requirements.txt` or `environment.yml`, running `pip` or `conda` install inside the container build process, and copying the workflow scripts/code. Creating efficient, minimal container images is often desirable to reduce storage and startup time.

Integrating containerization with a WMS provides a powerful solution for achieving robust, reproducible, and portable execution of complex astrophysical workflows across diverse computational environments. It encapsulates both the workflow logic (via WMS definition) and the execution environment (via containers), significantly advancing the state of reproducible computational science.

**66.6 Choosing a WMS: Comparison and Use Cases**

With several capable Workflow Management Systems available, choosing the right one for a specific astrophysical project depends on various factors, including the nature of the workflow, the primary programming language used, the target execution environment, the need for specific features (like containerization or complex dependency handling), and the familiarity and preferences of the research team. Here's a brief comparison of Snakemake, Nextflow, and Parsl:

**Snakemake:**
*   **Syntax:** Python-based rules in a `Snakefile`, inspired by Make. Rules define outputs based on inputs using shell commands or Python code blocks. Dependencies are primarily implicit based on filename matching.
*   **Strengths:** Very readable syntax for those familiar with Python/Make. Excellent support for wildcards, making it easy to scale workflows across samples/parameters following naming conventions. Strong integration with Conda and Singularity/Docker for environment management. Good reporting features. Active development and community.
*   **Weaknesses:** Implicit dependency management based on filenames can sometimes become complex to debug for intricate workflows. Dataflow logic (complex branching/merging based on content) might be less natural to express than in Nextflow.
*   **Use Cases:** Excellent for file-based pipelines involving sequences of command-line tools or scripts, particularly where input/output filenames follow predictable patterns (common in genomics, image processing). Good choice for Python-centric teams comfortable with Make-like concepts.

**Nextflow:**
*   **Syntax:** Groovy-based Domain Specific Language (DSL). Defines independent `processes` and connects them using data `channels` (asynchronous queues). Dependencies follow the dataflow paradigm.
*   **Strengths:** Powerful dataflow model handles complex dependencies and dynamic parallelism naturally. Excellent, robust support for various execution environments (local, HPC schedulers, cloud) and container runtimes (Docker, Singularity). Implicit parallelization based on channel inputs. Good caching and resume capabilities. Large community (especially bioinformatics) and pre-built workflow repositories (nf-core).
*   **Weaknesses:** Requires learning Groovy DSL syntax, which might be a barrier for pure Python users. Defining very complex conditional logic can sometimes be verbose. Less focused on wildcards for filename patterns compared to Snakemake (though possible).
*   **Use Cases:** Ideal for complex, production-scale workflows requiring high levels of parallelization, portability across diverse platforms (especially cloud), robust container integration, and sophisticated dataflow logic. Often favored for large collaborative projects standardizing on complex pipelines.

**Parsl:**
*   **Syntax:** Pure Python. Workflows defined by decorating Python functions (`@python_app`) or wrapping shell commands (`@bash_app`) and connecting them via Python variables holding "futures." Dependencies are implicit based on data flow between app calls.
*   **Strengths:** Native Python interface, very low barrier to entry for Python programmers. Excellent for parallelizing existing Python scripts or functions with minimal refactoring. Implicit dependency tracking is convenient. Flexible execution backends (threads, processes, HTCondor, SLURM, PBS via HighThroughputExecutor). Good for interactive parallel computing in notebooks.
*   **Weaknesses:** Managing complex workflows with numerous steps and intricate dependencies might become less structured compared to explicit rule/process definitions in Snakemake/Nextflow. Data staging between remote nodes might require more explicit configuration depending on the executor/provider setup. Community and pre-built workflow availability might be less extensive than Snakemake/Nextflow currently.
*   **Use Cases:** Best suited for Python-centric teams wanting to parallelize computational scripts and functions across local cores or HPC clusters with minimal deviation from standard Python programming. Excellent for interactive parallel workflows and integrating parallel tasks seamlessly into larger Python applications.

**Other Considerations:**
*   **Community and Support:** Snakemake and Nextflow currently have very large, active communities, particularly driven by bioinformatics, offering extensive documentation, tutorials, forums, and pre-built workflows (like nf-core). Parsl's community is growing, particularly within scientific Python and HPC domains.
*   **Learning Curve:** Parsl likely has the lowest barrier for Python users. Snakemake is relatively easy for those familiar with Python and Make. Nextflow's Groovy DSL requires learning a new syntax.
*   **Maturity and Features:** All three are mature and feature-rich, but might excel in different areas (e.g., Nextflow's container/cloud integration, Snakemake's wildcard handling, Parsl's Python integration).

**Recommendation:**
*   For predominantly **Python-based workflows**, especially interactive ones or parallelizing existing scripts, **Parsl** is a strong contender due to its native feel.
*   For **file-based pipelines involving many command-line tools** (potentially mixed languages) where filename patterns drive the workflow, **Snakemake** offers a readable and powerful solution with excellent environment management.
*   For **complex, production-scale workflows needing maximum portability**, robust **container support**, advanced **dataflow logic**, and leveraging a large community/existing pipelines, **Nextflow** is often the preferred choice, despite the initial DSL learning curve.

Ultimately, the best choice depends on the specific project requirements and team expertise. Trying simple examples in each system can help determine the best fit. All three significantly enhance the ability to manage complex astrophysical computational workflows compared to manual scripting alone.

---
**Application 66.A: TESS Data Reduction Pipeline using Snakemake**

**(Paragraph 1)** **Objective:** This application provides a practical example of defining an astronomical data processing workflow using **Snakemake** (Sec 66.2). We will create a `Snakefile` that automates the process of downloading TESS Target Pixel Files (TPFs), extracting light curves using aperture photometry, performing basic cleaning/detrending, and saving the results for a list of target TIC IDs and sectors, leveraging helper Python scripts that use `lightkurve`.

**(Paragraph 2)** **Astrophysical Context:** As described in Application 64.A, analyzing TESS data to find transits or study variability requires processing raw pixel data (TPFs) into cleaned light curves. This involves multiple steps (download, photometry, detrending) that need to be applied consistently to potentially many targets across multiple TESS sectors. Snakemake is well-suited for managing this repetitive, file-based pipeline.

**(Paragraph 3)** **Data Source:** A list of TESS Input Catalog IDs (`tic_ids`) and potentially sector numbers (`sectors`). The actual TPF data will be downloaded from MAST via `lightkurve` within the workflow steps. We assume helper Python scripts (`download_tpf.py`, `extract_lc.py`, `detrend_lc.py`) exist in a `scripts/` directory.

**(Paragraph 4)** **Modules Used:** `snakemake` (to run the workflow), Python standard libraries (`os`, `glob`). The helper scripts would use `lightkurve`, `numpy`, `astropy.io.fits`. The `Snakefile` itself uses Snakemake's Python-like syntax.

**(Paragraph 5)** **Technique Focus:** Defining a Snakemake workflow. Using Python lists (`TIC_IDS`, `SECTORS`) to define targets. Using `expand()` function with wildcards (`{tic}`, `{sector}`) to generate target output filenames for the final rule. Defining intermediate rules (`download_tpf`, `extract_raw_lc`, `detrend_cleaned_lc`) where output filenames use wildcards. Specifying dependencies implicitly through filename matching between rule inputs and outputs. Using the `shell:` directive to call the underlying Python helper scripts, passing input/output filenames and wildcards via f-strings or `{input}`, `{output}`, `{wildcards}` placeholders. Defining a `rule all` to trigger the generation of all desired final light curve files.

**(Paragraph 6)** **Processing Step 1: Prepare Helper Scripts (Conceptual):** Create Python scripts in a `scripts/` subdirectory:
    *   `download_tpf.py`: Takes TIC ID, sector, output filename. Uses `lightkurve.search_targetpixelfile().download()` and saves the TPF FITS file.
    *   `extract_lc.py`: Takes input TPF filename, output LC filename. Uses `lightkurve.TessTargetPixelFile()`, defines an aperture, performs photometry (`.to_lightcurve()`), saves the raw SAP light curve (e.g., as FITS table).
    *   `detrend_lc.py`: Takes input raw LC filename, output flattened LC filename. Loads the light curve, performs cleaning (`.remove_outliers()`) and detrending (`.flatten()`), saves the flattened light curve.

**(Paragraph 7)** **Processing Step 2: Create `Snakefile`:** Define the workflow logic.
```python
# --- Content for Snakefile ---
import glob
# Assume helper scripts are in ./scripts/

# --- Configuration ---
WORKDIR = "tess_workflow_output"
TIC_IDS = ["261136679", "142178937"] # Example TICs
SECTORS = ["1", "27"] # Example Sectors to try for each TIC

# --- Generate target filenames ---
# Final output: flattened light curves
FINAL_LCS = expand(WORKDIR + "/detrended/tic{tic}_s{sector}_flat.fits", 
                   tic=TIC_IDS, sector=SECTORS)

# --- Rule: Target 'all' ---
rule all:
    input: FINAL_LCS

# --- Rule: Detrend Light Curve ---
rule detrend_cleaned_lc:
    input:
        raw_lc = WORKDIR + "/raw_lc/tic{tic}_s{sector}_rawlc.fits"
    output:
        flat_lc = WORKDIR + "/detrended/tic{tic}_s{sector}_flat.fits"
    log:
        WORKDIR + "/logs/detrend/tic{tic}_s{sector}.log"
    shell:
        "echo 'Detrending {input.raw_lc}...' > {log}; "
        "python scripts/detrend_lc.py --input {input.raw_lc} --output {output.flat_lc} &>> {log}"

# --- Rule: Extract Raw Light Curve ---
rule extract_raw_lc:
    input:
        tpf = WORKDIR + "/tpfs/tic{tic}_s{sector}_tp.fits"
    output:
        raw_lc = WORKDIR + "/raw_lc/tic{tic}_s{sector}_rawlc.fits"
    log:
        WORKDIR + "/logs/extract/tic{tic}_s{sector}.log"
    shell:
        "echo 'Extracting LC from {input.tpf}...' > {log}; "
        "python scripts/extract_lc.py --input {input.tpf} --output {output.raw_lc} &>> {log}"

# --- Rule: Download TPF ---
rule download_tpf:
    output:
        # Wildcards are defined by the output needed by subsequent rules
        tpf = WORKDIR + "/tpfs/tic{tic}_s{sector}_tp.fits"
    log:
        WORKDIR + "/logs/download/tic{tic}_s{sector}.log"
    params: # Pass wildcards as parameters to the script
        tic = "{tic}",
        sector = "{sector}"
    shell:
        "echo 'Downloading TPF for TIC {params.tic} Sector {params.sector}...' > {log}; "
        "python scripts/download_tpf.py --tic {params.tic} --sector {params.sector} --output {output.tpf} &>> {log}"

```

**(Paragraph 8)** **Processing Step 3: Setup Directories and Run:** Create the base directory (`tess_workflow_output`) and subdirectories (`tpfs`, `raw_lc`, `detrended`, `logs`). Run Snakemake from the directory containing the `Snakefile`: `snakemake --cores N all` (where N is number of cores).

**(Paragraph 9)** **Processing Step 4: Monitor Execution:** Snakemake will print which rules are being executed and their progress. It determines the DAG: to get the final detrended LCs (`rule all` targets), it needs the raw LCs (`detrend_cleaned_lc` input), which need the TPFs (`extract_raw_lc` input), which triggers `download_tpf`. It runs these steps, potentially in parallel for different TIC/Sector combinations if `--cores` > 1.

**(Paragraph 10)** **Processing Step 5: Inspect Outputs:** Check the `tess_workflow_output/detrended/` directory for the final flattened light curve FITS files. Examine log files in `tess_workflow_output/logs/` for details or errors.

**Output, Testing, and Extension:** The output consists of intermediate TPF and raw LC files, final detrended LC files, and log files, organized in the specified output directory structure. **Testing:** Use `snakemake --dryrun -p all` first to check the execution plan. Verify output files are created for all requested TIC/Sector combinations. Inspect intermediate and final files for correctness. Delete an intermediate file and rerun to check if Snakemake correctly resumes/reruns necessary steps. **Extensions:** (1) Add rules for plotting the light curves at different stages. (2) Add a rule for running a BLS transit search on the detrended light curves. (3) Use a configuration file (`config.yaml`) to specify TIC IDs, sectors, and parameters for helper scripts instead of hardcoding in the `Snakefile`. (4) Add Conda environment specifications per rule for better reproducibility. (5) Configure Snakemake to submit jobs to an HPC cluster for processing a much larger list of targets.

```python
# --- Code Example: Application 66.A ---
# This block displays the conceptual Snakefile content and run command.
# Actual execution requires creating the Snakefile, helper scripts, 
# and directories, plus installing snakemake.

print("--- Conceptual Snakemake Workflow for TESS LC Processing ---")

snakefile_content = """
# Content for Snakefile (as shown in Processing Step 2 above)
import glob

WORKDIR = "tess_workflow_output"
TIC_IDS = ["261136679", "142178937"] # Example TICs
SECTORS = ["1", "27"] # Example Sectors

FINAL_LCS = expand(WORKDIR + "/detrended/tic{tic}_s{sector}_flat.fits", 
                   tic=TIC_IDS, sector=SECTORS)

rule all:
    input: FINAL_LCS

rule detrend_cleaned_lc:
    input:
        raw_lc = WORKDIR + "/raw_lc/tic{tic}_s{sector}_rawlc.fits"
    output:
        flat_lc = WORKDIR + "/detrended/tic{tic}_s{sector}_flat.fits"
    log:
        WORKDIR + "/logs/detrend/tic{tic}_s{sector}.log"
    shell:
        "echo 'Detrending {input.raw_lc}...' > {log}; "
        "python scripts/detrend_lc.py --input {input.raw_lc} --output {output.flat_lc} &>> {log}"

rule extract_raw_lc:
    input:
        tpf = WORKDIR + "/tpfs/tic{tic}_s{sector}_tp.fits"
    output:
        raw_lc = WORKDIR + "/raw_lc/tic{tic}_s{sector}_rawlc.fits"
    log:
        WORKDIR + "/logs/extract/tic{tic}_s{sector}.log"
    shell:
        "echo 'Extracting LC from {input.tpf}...' > {log}; "
        "python scripts/extract_lc.py --input {input.tpf} --output {output.raw_lc} &>> {log}"

rule download_tpf:
    output:
        tpf = WORKDIR + "/tpfs/tic{tic}_s{sector}_tp.fits"
    log:
        WORKDIR + "/logs/download/tic{tic}_s{sector}.log"
    params: 
        tic = "{tic}",
        sector = "{sector}"
    shell:
        "echo 'Downloading TPF for TIC {params.tic} Sector {params.sector}...' > {log}; "
        "python scripts/download_tpf.py --tic {params.tic} --sector {params.sector} --output {output.tpf} &>> {log}"

"""
print("--- Snakefile Content (Conceptual) ---")
print(snakefile_content)

print("\n--- Conceptual Execution Command (run in terminal) ---")
print("# (Create directories: tess_workflow_output/logs/..., scripts/)")
print("# (Create dummy scripts: scripts/download_tpf.py, etc.)")
print("# snakemake --cores 4 all") 
print("-" * 20)
```

**Application 66.B: Parallel Transit Search using Parsl**

**(Paragraph 1)** **Objective:** This application demonstrates using the **Parsl** parallel scripting library (Sec 66.4) to define and execute a workflow that performs a Box Least Squares (BLS) transit search in parallel on multiple pre-processed TESS light curve files across local CPU cores. It highlights Parsl's Python-native approach using decorated functions ("apps") and implicit dependency management via futures.

**(Paragraph 2)** **Astrophysical Context:** Searching for the periodic dimming caused by transiting exoplanets in large collections of stellar light curves is a computationally intensive task. The BLS algorithm is commonly used for detecting transit-like signals. Applying BLS efficiently to thousands or millions of light curves (e.g., from TESS) requires parallel processing. Parsl provides a way to easily parallelize the application of a Python-based BLS search function across many input files.

**(Paragraph 3)** **Data Source:** A list of filenames pointing to pre-processed (cleaned, detrended) light curve files (e.g., `detrended/tic*_flat.fits` generated by a workflow like App 66.A). Each file contains time and flux information, likely as a FITS table. We will simulate these input files or just use the filenames.

**(Paragraph 4)** **Modules Used:** `parsl` (for `@python_app`, `Config`, `HighThroughputExecutor`, etc.), `lightkurve` (inside the app function, for loading data and running BLS), `numpy`, `os`.

**(Paragraph 5)** **Technique Focus:** Defining Parsl workflows in Python. (1) Configuring Parsl with an appropriate executor (e.g., `HighThroughputExecutor` using local processes for CPU parallelism). (2) Defining a Python function `run_bls_search(lc_filename)` decorated with `@python_app` which loads a light curve, runs `lc.to_periodogram(method='bls')`, finds the best-fit period/duration/depth, and returns these results (e.g., in a dictionary). (3) Creating a list of input light curve filenames. (4) Launching the `run_bls_search` app concurrently for each filename, obtaining a list of future objects. (5) Waiting for the futures to complete and collecting the results using `.result()`.

**(Paragraph 6)** **Processing Step 1: Configure Parsl:** Import `parsl` and configuration elements. Define a `Config` using `HighThroughputExecutor` with `LocalProvider` to run tasks in parallel using local processes (similar to `multiprocessing`). Load the configuration using `parsl.load()`.

**(Paragraph 7)** **Processing Step 2: Define BLS App Function:** Define `run_bls_search(lc_filename)`:
    *   Add the `@python_app` decorator above the function definition.
    *   Inside, import `lightkurve`. Load the light curve from `lc_filename` (`lk.read()`). Handle potential file loading errors.
    *   Run BLS: `bls = lc.to_periodogram(method='bls', duration=...)`. Specify relevant period ranges or durations.
    *   Extract best result: `best_period = bls.period_at_max_power`, `best_duration = bls.duration_at_max_power`, `best_depth = bls.depth_at_max_power`, `max_power = bls.max_power`.
    *   Return a dictionary containing filename, period, duration, depth, power. Include error handling for BLS failures.

**(Paragraph 8)** **Processing Step 3: Prepare Inputs and Launch Apps:** Create a list `lightcurve_files` containing paths to the detrended light curve FITS files (use `glob` or generate dummy paths). Create an empty list `bls_futures`. Loop through `lightcurve_files`, calling `future = run_bls_search(fname)` for each file. Append the returned `future` object to `bls_futures`. Parsl starts scheduling and executing these app calls asynchronously.

**(Paragraph 9)** **Processing Step 4: Collect Results:** Wait for all tasks to finish and collect results. Use a list comprehension: `bls_results = [fut.result() for fut in bls_futures]`. The `.result()` call blocks until the specific future it's called on is complete. Include a `try...except` around `.result()` to handle potential exceptions raised within the app function.

**(Paragraph 10)** **Processing Step 5: Process Results and Cleanup:** Process the `bls_results` list (e.g., print results, filter for significant detections based on power, save to a table/file). Call `parsl.dfk().cleanup()` and `parsl.clear()` to shut down the Parsl Data Flow Kernel and clear the configuration.

**Output, Testing, and Extension:** Output includes status messages from Parsl during execution and the final list `bls_results` containing the BLS results dictionary for each processed light curve. **Testing:** Verify the number of results matches the number of input files. Check if the returned periods/durations/depths seem plausible (for simulated inputs). Test error handling by providing a path to a non-existent or corrupted file. Run on a small number of files serially (by configuring Parsl with a single worker or manually looping) and compare execution time to the parallel run to estimate speedup. **Extensions:** (1) Configure Parsl to use an HPC cluster executor (`parsl.providers.SlurmProvider`, etc.) to run the search on a much larger number of files distributed across cluster nodes. (2) Add more sophisticated vetting logic after the BLS search within the app or in a subsequent app (e.g., checking transit shape, odd/even transit depth). (3) Define the workflow with more steps using Parsl's implicit dependencies (e.g., an app to download, an app to detrend, then the BLS app). (4) Use Dask Bag (App 40.A) for the same task and compare the implementation style and performance with Parsl.

```python
# --- Code Example: Application 66.B ---
# Note: Requires parsl, lightkurve, numpy, astropy
# pip install parsl lightkurve numpy astropy matplotlib
import parsl
from parsl import python_app, bash_app, FitFailed # Import FitFailed for error handling
from parsl.config import Config
from parsl.executors import HighThroughputExecutor
from parsl.providers import LocalProvider
import lightkurve as lk
import numpy as np
import os
import glob
import shutil
import time
import pandas as pd # For results

print("Parallel BLS Transit Search using Parsl:")

# --- Step 1: Configure Parsl ---
print("\nConfiguring Parsl for local execution...")
try:
    # Configure HighThroughputExecutor with local provider
    parsl_config = Config(
        executors=[
            HighThroughputExecutor(
                label="htex_local", # Label for the executor
                max_workers=os.cpu_count(), # Use all available cores
                provider=LocalProvider( # Manage workers locally
                    init_blocks=1,
                    max_blocks=1,
                ),
            )
        ],
        # strategy='simple', # Use 'simple' strategy for potentially faster task distribution if tasks are very short
        retries=1 # Example: Allow 1 retry for failed apps
    )
    parsl.load(parsl_config)
    print("Parsl loaded with local HighThroughputExecutor.")
    parsl_loaded = True
except Exception as e_load:
    print(f"Error loading Parsl config: {e_load}")
    parsl_loaded = False

# --- Step 2: Define BLS App Function ---
@python_app(executors=['htex_local']) # Specify which executor to use
def run_bls_search_app(lc_filename, min_period=0.5, max_period=20.0):
    """Loads LC file, runs BLS, returns best result."""
    import lightkurve as lk # Imports needed within the app function
    import numpy as np
    import os
    
    basename = os.path.basename(lc_filename)
    print(f"  BLS App: Processing {basename}...")
    try:
        lc = lk.read(lc_filename)
        # Ensure flux is finite, handle potential issues
        lc = lc.remove_nans().normalize() 
        
        # Run BLS periodogram
        # Specify duration range based on period (example)
        durations = np.linspace(0.05, 0.2, 10) * u.day # Example durations
        bls = lc.to_periodogram(method='bls', 
                               period=np.arange(min_period, max_period, 0.01)*u.day, 
                               duration=durations)
                               
        # Extract results
        best_period = bls.period_at_max_power.value
        best_duration = bls.duration_at_max_power.value
        best_depth = bls.depth_at_max_power
        max_power = bls.max_power.value
        
        print(f"  BLS App: Finished {basename}. Period={best_period:.3f} d")
        return {'filename': basename, 'period': best_period, 
                'duration': best_duration, 'depth': best_depth, 'power': max_power, 'error': None}

    except Exception as e:
        print(f"  BLS App: Error processing {basename}: {e}")
        # Return error state
        return {'filename': basename, 'period': np.nan, 'duration': np.nan, 
                'depth': np.nan, 'power': np.nan, 'error': str(e)}

# --- Main Workflow Execution ---
if parsl_loaded:
    # --- Step 3: Prepare Inputs ---
    # Create dummy detrended files first
    input_dir = "parsl_detrended_lcs"
    os.makedirs(input_dir, exist_ok=True)
    n_lcs = 32 # Number of light curves
    print(f"\nCreating {n_lcs} dummy light curve files in '{input_dir}'...")
    for i in range(n_lcs):
         fname = os.path.join(input_dir, f"tic123_s{i}_flat.fits")
         # Create simple FITS table with time, flux
         time_arr = np.arange(0, 27, 0.002) # Example time
         flux_arr = np.random.normal(1.0, 0.001, len(time_arr)) # Noise
         # Add transit? Maybe to a few?
         if i % 5 == 0: # Add transit every 5th file
              period = np.random.uniform(1,10)
              t0 = np.random.uniform(0, period)
              phase = ((time_arr - t0 + period/2) % period) - period/2
              duration_phase = 0.05
              in_transit = np.abs(phase) < duration_phase / 2
              flux_arr[in_transit] -= 0.005 # Add 0.5% deep transit
         
         lc_tab = Table({'TIME': time_arr, 'FLUX': flux_arr})
         lc_tab.write(fname, format='fits', overwrite=True)
         
    lightcurve_files = glob.glob(os.path.join(input_dir, "*.fits"))
    print(f"Found {len(lightcurve_files)} input files.")

    # --- Step 4: Launch Apps ---
    print("\nLaunching BLS search apps...")
    bls_futures = []
    for lc_file in lightcurve_files:
        # Pass arguments to the app function
        future = run_bls_search_app(lc_file, min_period=1.0, max_period=10.0) 
        bls_futures.append(future)
    print(f"Launched {len(bls_futures)} apps.")

    # --- Step 5: Collect Results ---
    print("\nWaiting for results...")
    bls_results = []
    for i, fut in enumerate(bls_futures):
        try:
            result = fut.result() # Blocks until this specific future is done
            bls_results.append(result)
            # Optional: print progress
            # print(f"  Result {i+1}/{len(bls_futures)} received.") 
        except FitFailed as e_fit: # Parsl specific exception for app errors
             print(f"App for future {i} failed: {e_fit}")
             # Append error information if needed
             # bls_results.append({'filename': lightcurve_files[i], 'error': str(e_fit)})
        except Exception as e:
             print(f"Error getting result for future {i}: {e}")
             # bls_results.append({'filename': lightcurve_files[i], 'error': str(e)})
             
    print("\nWorkflow complete. All results received.")

    # Step 6: Process Results
    if bls_results:
        results_df = pd.DataFrame([r for r in bls_results if r]) # Filter potential None/errors
        print("\n--- Sample BLS Results ---")
        print(results_df.head())
        
        # Find candidates (example: high power)
        candidates = results_df[results_df['power'] > 10] # Example threshold
        print(f"\nFound {len(candidates)} potential candidates (Power > 10):")
        if not candidates.empty: print(candidates[['filename', 'period', 'power']])
        
    # --- Step 7: Cleanup ---
    finally: # Ensure cleanup happens
         print("\nCleaning up Parsl DFK and temporary files...")
         parsl.dfk().cleanup() # Cleanup Data Flow Kernel state
         if os.path.exists(input_dir): shutil.rmtree(input_dir)
         print("Cleanup complete.")
         parsl.clear() # Clear loaded config

else:
    print("Skipping Parsl execution.")

print("-" * 20)
```

**Chapter 66 Summary**

This chapter moved beyond basic Python scripting for workflow management to introduce dedicated **Workflow Management Systems (WMS)** designed to handle complex, multi-step computational pipelines with robustness, scalability, and reproducibility. Revisiting the core concepts, it emphasized how WMSs represent workflows as Directed Acyclic Graphs (DAGs) of tasks with defined inputs, outputs, and dependencies, automating execution order and enabling parallel processing. Three prominent WMS tools were explored in detail. **Snakemake** was presented, highlighting its readable Python-based syntax for defining **rules** (tasks) in a `Snakefile`, its powerful use of **wildcards** for pattern-matching filenames, implicit dependency management based on input/output matching, configuration via external files, strong integration with Conda/containers, and support for various execution backends (local, cluster, cloud). **Nextflow** was introduced, focusing on its **dataflow paradigm** where data streams flow through asynchronous **channels** triggering independent **processes** defined using a Groovy-based DSL in `.nf` scripts. Its strengths in implicit parallelization, robust support for diverse executors and container technologies (Docker/Singularity), effective caching, and resilience were noted.

Finally, **Parsl** (Parallel Scripting Library) was described as a Python-native library allowing workflows to be defined by decorating standard Python functions as parallel **apps** (`@python_app`, `@bash_app`). Dependencies are managed implicitly through the flow of **futures** (placeholders for results) between app calls. Parsl's flexibility in using different execution backends (local threads/processes, HPC schedulers via `HighThroughputExecutor`) directly from Python configuration was highlighted. A comparison guided the choice between these systems based on factors like workflow complexity, primary language (Python vs. mixed), execution environment needs, dataflow vs. rule-based logic preference, and community support. Two applications conceptually demonstrated defining a TESS data reduction pipeline using Snakemake's rule structure and implementing a parallel BLS transit search using Parsl's Python apps and futures.

---

**References for Further Reading (APA Format, 7th Edition):**

1.  **Köster, J., & Rahmann, S. (2012).** Snakemake—a scalable bioinformatics workflow engine. *Bioinformatics*, *28*(19), 2520–2522. [https://doi.org/10.1093/bioinformatics/bts480](https://doi.org/10.1093/bioinformatics/bts480) (See also documentation: [https://snakemake.readthedocs.io/en/stable/](https://snakemake.readthedocs.io/en/stable/))
    *(Introduces Snakemake. The documentation is the primary resource for learning its syntax, features, and configuration.)*

2.  **Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017).** Nextflow enables reproducible computational workflows. *Nature Biotechnology*, *35*(4), 316–319. [https://doi.org/10.1038/nbt.3820](https://doi.org/10.1038/nbt.3820) (See also documentation: [https://www.nextflow.io/docs/latest/index.html](https://www.nextflow.io/docs/latest/index.html))
    *(Introduces Nextflow. The documentation explains its DSL, dataflow concepts, executors, and container integration.)*

3.  **Babuji, Y., et al. (2019).** Parsl: Enabling Scalable Interactive Computing in Python. In *Proceedings of the ACM International Conference on High Performance Computing, Networking, Storage and Analysis (SC19)* (Article 49). [https://doi.org/10.1145/3295500.3356186](https://doi.org/10.1145/3295500.3356186) (See also documentation: [https://parsl-project.org/](https://parsl-project.org/))
    *(Describes Parsl, focusing on its Python integration, app decorators, futures, and configuration system.)*

4.  **Leipzig, J. (2017).** A review of workflow systems for genomics. *Briefings in Bioinformatics*, *18*(3), 530-536. [https://doi.org/10.1093/bib/bbw020](https://doi.org/10.1093/bib/bbw020)
    *(Provides a comparative overview of various workflow systems (including some discussed here) from the perspective of bioinformatics, highlighting common features and challenges.)*

5.  **Grüning, B., et al. (2018).** Practical Computational Reproducibility in the Life Sciences. *Cell Systems*, *6*(6), 631-635. [https://doi.org/10.1016/j.cels.2018.03.014](https://doi.org/10.1016/j.cels.2018.03.014)
    *(Discusses the importance of reproducibility and the role of tools like workflow managers, Conda, and containers (Docker/Singularity) in achieving it, relevant context for Sec 66.5 and Chapter 69.)*
